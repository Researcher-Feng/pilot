# agent_IO.py
import os
import sys
sys.path.append(r'../..')
from utils.api_config import *
from prompt.system import *
from function.contex_fun import *
from function.format_fun import *
from function.memory_fun import *
selected_model = 'deepseek-official'
os.environ["OPENAI_API_KEY"] = key_config.get(selected_model, "")
os.environ["OPENAI_BASE_URL"] = key_url
os.environ["DEEPSEEK_API_KEY"] = key_config.get(selected_model, "")
os.environ["DEEPSEEK_BASE_URL"] = ds_key_url
os.environ["LANGSMITH_TRACING"] = "false"
os.environ["LANGSMITH_API_KEY"] = smith_key
os.environ.pop('HTTP_PROXY', None)
os.environ.pop('HTTPS_PROXY', None)
os.environ.pop('http_proxy', None)
os.environ.pop('https_proxy', None)
os.environ.pop('ALL_PROXY', None)
os.environ.pop('all_proxy', None)

from langchain_core.messages import HumanMessage, ToolMessage
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, wrap_tool_call, dynamic_prompt, ModelRequest, ModelResponse
from langchain.chat_models import init_chat_model
from langchain_ollama import ChatOllama
from langchain.agents import AgentState
from langchain.agents.middleware import AgentMiddleware
from typing import Callable, List, Optional, Dict, Any


class ModelConfig:
    """ç»Ÿä¸€çš„æ¨¡åž‹é…ç½®ç±»"""

    def __init__(self, model_type: str = "api", **kwargs):
        self.model_type = model_type  # "api" æˆ– "local"
        self.model_name = kwargs.get("model_name", "")
        self.base_url = kwargs.get("base_url", "http://localhost:11434")
        self.temperature = kwargs.get("temperature", 0.7)
        self.max_tokens = kwargs.get("max_tokens", 2000)
        self.timeout = kwargs.get("timeout", 30)
        self.extra_params = kwargs.get("extra_params", {})


class SimpleAgent(object):
    """å¢žå¼ºçš„Agentç±»ï¼Œæ”¯æŒå¤šæ¨¡å¼å’Œæœ¬åœ°è°ƒç”¨"""

    def __init__(self, agent_type: str = "student"):
        """åˆå§‹åŒ–agent

        Args:
            agent_type: "student" æˆ– "teacher"
        """
        self.agent_type = agent_type
        self.prompt_sys = None
        self.tools = []
        self.context_schema = None
        self.response_format = None
        self.checkpointer = None
        self.middleware_list = []

        self.agent_config = None
        self.response = None
        self.context = None

        self.middleware_er = None
        self.custom_middleware_er = CustomMiddleware
        self.model = None
        self.agent = None

        # å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé…ç½®
        self.dialogue_history = []
        self.max_turns = 5
        self.current_turn = 0

        # åŠŸèƒ½å¼€å…³
        self.parallel_thinking_enabled = False
        self.socratic_teaching_enabled = False
        self.math_background_level = "intermediate"

    def model_init(self, model_config: ModelConfig):
        """åˆå§‹åŒ–æ¨¡åž‹ï¼Œæ”¯æŒAPIå’Œæœ¬åœ°è°ƒç”¨"""
        if model_config.model_type == "local":
            # ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡åž‹
            self.model = ChatOllama(
                model=model_config.model_name,
                base_url=model_config.base_url,
                temperature=model_config.temperature,
                num_predict=model_config.max_tokens,
                **model_config.extra_params
            )
            print(f"âœ… åˆå§‹åŒ–æœ¬åœ°æ¨¡åž‹: {model_config.model_name}")
        else:
            # APIæ¨¡åž‹é…ç½®
            api_kwargs = {
                "temperature": model_config.temperature,
                "timeout": model_config.timeout,
                "max_tokens": model_config.max_tokens,
            }
            self.model = init_chat_model(model_config.model_name, **api_kwargs)
            print(f"âœ… åˆå§‹åŒ–APIæ¨¡åž‹: {model_config.model_name}")

        self.middleware_er = MiddlewareFunc(self.model, self.model)

    def agent_init(self, model_config, prompt_sys_name=None, tools_list=None, context_schema=Context,
                   response_format=ResponseFormat, checkpointer=simple_checkpointer,
                   middleware=None, **kwargs):
        """åˆå§‹åŒ–agenté…ç½®"""
        if prompt_sys_name:
            self.prompt_sys = prompt_sys_name
        if tools_list:
            self.tools = tools_list
        self.context_schema = context_schema
        self.response_format = response_format
        self.checkpointer = checkpointer

        # è®¾ç½®åŠŸèƒ½å¼€å…³
        self.max_turns = kwargs.get("max_turns", 5)
        self.parallel_thinking_enabled = kwargs.get("parallel_thinking", False)
        self.socratic_teaching_enabled = kwargs.get("socratic_teaching", False)
        self.math_background_level = kwargs.get("math_background", "intermediate")

        # æ ¹æ®åŠŸèƒ½å¼€å…³è°ƒæ•´ç³»ç»Ÿæç¤ºè¯
        self._adjust_prompt_based_on_settings()

        if middleware:
            for m in middleware:
                if m == 'dynamic':
                    self.middleware_list.append(self.middleware_er.middleware_dynamic_model_selection())
                elif m == 'handle_tool_errors':
                    self.middleware_list.append(self.middleware_er.middleware_handle_tool_errors())
                elif m == 'user_role_prompt':
                    self.middleware_list.append(self.middleware_er.middleware_user_role_prompt())
                elif m == 'CustomMiddleware':
                    self.middleware_list.append(self.custom_middleware_er())

        if model_config.model_type == "local":
            self.agent = self._create_simple_agent()
        else:
            self.agent = create_agent(
                model=self.model,
                system_prompt=self.prompt_sys,
                tools=self.tools,
                context_schema=self.context_schema,
                response_format=self.response_format,
                checkpointer=self.checkpointer,
                middleware=self.middleware_list,
            )

    def _create_simple_agent(self):
        """åˆ›å»ºä¸åŒ…å«å·¥å…·çš„ç®€å•å¯¹è¯agent"""
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.messages import AIMessage, HumanMessage

        # åˆ›å»ºç®€å•çš„å¯¹è¯é“¾
        prompt = ChatPromptTemplate.from_messages([
            ("system", self.prompt_sys),
            ("user", "{input}")
        ])

        # ç®€å•çš„å¯¹è¯é“¾ï¼Œä¸åŒ…å«ä»»ä½•å·¥å…·
        chain = prompt | self.model

        # åŒ…è£…æˆç±»ä¼¼agentçš„æŽ¥å£
        class SimpleAgentWrapper:
            def __init__(self, chain):
                self.chain = chain

            def invoke(self, input_dict, config=None, context=None):
                user_input = input_dict["messages"][-1]['content']
                # è°ƒç”¨æ¨¡åž‹
                response_content = self.chain.invoke({"input": user_input})

                # ç¡®ä¿response_contentæ˜¯å­—ç¬¦ä¸²
                if hasattr(response_content, 'content'):
                    response_text = response_content.content
                else:
                    response_text = str(response_content)

                # è¿”å›žä¸ŽLangGraphå…¼å®¹çš„æ ¼å¼
                return {
                    "messages": [AIMessage(content=response_text)]
                }

            def stream(self, input_dict, config=None, context=None, stream_mode="values"):
                # ç®€å•çš„æµå¼å“åº”å®žçŽ°
                result = self.invoke(input_dict, config, context)
                yield result

        return SimpleAgentWrapper(chain)

    def _adjust_prompt_based_on_settings(self):
        """æ ¹æ®åŠŸèƒ½å¼€å…³è°ƒæ•´ç³»ç»Ÿæç¤ºè¯"""
        if self.agent_type == "student":
            # å­¦ç”Ÿæç¤ºè¯è°ƒæ•´
            base_prompt = STUDENT_PROMPT

            if self.parallel_thinking_enabled:
                base_prompt += "\n\n" + PARALLEL_THINKING_PROMPT

            # æ ¹æ®æ•°å­¦èƒŒæ™¯è°ƒæ•´
            if self.math_background_level == "beginner":
                base_prompt += "\n\n" + MATH_BACKGROUND_BEGINNER
            elif self.math_background_level == "advanced":
                base_prompt += "\n\n" + MATH_BACKGROUND_ADVANCED
            else:
                base_prompt += "\n\n" + MATH_BACKGROUND_INTERMEDIATE

            self.prompt_sys = base_prompt

        elif self.agent_type == "teacher":
            # æ•™å¸ˆæç¤ºè¯è°ƒæ•´
            base_prompt = TEACHER_PROMPT

            if self.socratic_teaching_enabled:
                base_prompt += "\n\n" + SOCRATIC_TEACHING_PROMPT

            self.prompt_sys = base_prompt

    def config_create(self, key_i, value_i):
        self.agent_config = {"configurable": {key_i: value_i}, "recursion_limit": 100}

    def context_set(self, **kwargs):
        self.context = self.context_schema(**kwargs)

    def chat_once(self, user_input, response_type='invoke', silence=False, **kwargs):
        if response_type == 'invoke':
            self.agent_response_invoke(user_input, **kwargs)
            if not silence:
                self.agent_output()
        else:
            self.agent_response_stream(user_input, **kwargs)

    def multi_agent_chat_explicit(self, teacher_agent, problem, **kwargs):
        """æ¨¡å¼1: æ˜¾å¼äº¤äº’ - æ•™å¸ˆå’Œå­¦ç”Ÿç›´æŽ¥å¯¹è¯"""
        print(f"\nðŸŽ¯ å¼€å§‹è§£é¢˜: {problem}")
        print("=" * 50)

        # é‡ç½®å¯¹è¯åŽ†å²
        self.dialogue_history = []
        self.current_turn = 0

        # å­¦ç”Ÿé¦–æ¬¡å°è¯•
        student_response = self._invoke_agent(problem)
        self.dialogue_history.append(("student", student_response))
        print(f"ðŸ‘¨â€ðŸŽ“ å­¦ç”Ÿ: {student_response}")

        for turn in range(self.max_turns):
            print(f"\nðŸ”„ ç¬¬ {turn + 1} è½®å¯¹è¯:")
            print("-" * 30)

            # æ•™å¸ˆå›žåº”
            teacher_input = self._format_teacher_input(problem, self.dialogue_history)
            print(f"ðŸ‘¨â€ðŸ« teacher_input: {teacher_input}")
            teacher_response = teacher_agent._invoke_agent(teacher_input)
            self.dialogue_history.append(("teacher", teacher_response))

            print(f"ðŸ‘¨â€ðŸ« æ•™å¸ˆ: {teacher_response}")

            # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»“æŸå¯¹è¯
            if self._should_end_dialogue(teacher_response):
                print("âœ… æ•™å¸ˆè®¤ä¸ºè§£é¢˜å®Œæˆ")
                break

            # å­¦ç”Ÿå›žåº”
            student_input = self._format_student_input(problem, self.dialogue_history)
            print(f"ðŸ‘¨â€ðŸŽ“ student_input: {student_input}")
            student_response = self._invoke_agent(student_input)
            self.dialogue_history.append(("student", student_response))

            print(f"ðŸ‘¨â€ðŸŽ“ å­¦ç”Ÿ: {student_response}")

            # æ£€æŸ¥å­¦ç”Ÿæ˜¯å¦å¾—å‡ºæ­£ç¡®ç­”æ¡ˆ
            if self._has_correct_answer(student_response, problem):
                print("ðŸŽ‰ å­¦ç”Ÿå¾—å‡ºæ­£ç¡®ç­”æ¡ˆ!")
                break

            self.current_turn = turn + 1

        return self._get_final_answer()

    def multi_agent_chat_tool_based(self, problem, **kwargs):
        """æ¨¡å¼2: å·¥å…·è°ƒç”¨ - å­¦ç”Ÿä½œä¸ºcontrollerè°ƒç”¨æ•™å¸ˆå·¥å…·"""
        print(f"\nðŸŽ¯ å¼€å§‹å·¥å…·è°ƒç”¨æ¨¡å¼è§£é¢˜: {problem}")
        print("=" * 50)

        # é…ç½®å­¦ç”Ÿagentä»¥åŒ…å«æ•™å¸ˆå·¥å…·
        teacher_tool = self._create_teacher_tool()
        self.tools.append(teacher_tool)

        # é‡æ–°åˆå§‹åŒ–agentä»¥åŒ…å«æ–°å·¥å…·
        self.agent = create_agent(
            model=self.model,
            system_prompt=self.prompt_sys + "\n\nYou can use the ask_teacher tool when you need guidance.",
            tools=self.tools,
            context_schema=self.context_schema,
            response_format=self.response_format,
            checkpointer=self.checkpointer,
            middleware=self.middleware_list,
        )

        # å­¦ç”Ÿè‡ªä¸»è§£é¢˜ï¼Œå¯åœ¨éœ€è¦æ—¶è°ƒç”¨æ•™å¸ˆå·¥å…·
        final_response = self._invoke_agent(problem)
        print(f"ðŸ‘¨â€ðŸŽ“ å­¦ç”Ÿæœ€ç»ˆå›žç­”: {final_response}")

        return final_response

    def _create_teacher_tool(self):
        """åˆ›å»ºæ•™å¸ˆå·¥å…·ä¾›å­¦ç”Ÿè°ƒç”¨"""
        from langchain.tools import tool

        @tool
        def ask_teacher(question: str) -> str:
            """Ask the teacher for guidance on a specific question or problem.

            Use this tool when:
            - You're stuck on a math problem
            - You need clarification on concepts
            - You want to check your approach
            - You need step-by-step guidance
            """
            # è¿™é‡Œå¯ä»¥é›†æˆçœŸå®žçš„æ•™å¸ˆagentè°ƒç”¨
            # æš‚æ—¶è¿”å›žæ¨¡æ‹Ÿå“åº”
            teacher_responses = [
                "Let me guide you through this step by step. What part are you finding difficult?",
                "Good attempt! Let's break this down. What's your current approach?",
                "I see where you might be confused. Let me ask you a question to help you think differently...",
                "Remember the key concept here is to identify the known values and what you're solving for.",
                "Try breaking the problem into smaller parts. What's the first step you would take?"
            ]
            import random
            return random.choice(teacher_responses)

        return ask_teacher

    def _invoke_agent(self, input_text):
        """è°ƒç”¨agentå¹¶è¿”å›žå“åº”"""
        response = self.agent.invoke({
            "messages": [{"role": "user", "content": input_text}]
        }, config=self.agent_config, context=self.context)

        if response and 'messages' in response:
            return response['messages'][-1].content  # response['structured_response'].main_response
        return ""

    def _format_teacher_input(self, problem, history):
        """æ ¼å¼åŒ–æ•™å¸ˆè¾“å…¥"""
        context = f"Problem: {problem}\n\nDialogue History:\n"
        for role, content in history:
            context += f"{role}: {content}\n\n"

        # æ ¹æ®è‹æ ¼æ‹‰åº•æ•™å­¦å¼€å…³è°ƒæ•´æç¤º
        if self.socratic_teaching_enabled:
            context += "Please use Socratic questioning to guide the student. Ask thought-provoking questions rather than giving direct answers."
        else:
            context += "Please provide appropriate guidance to help the student solve the problem."

        return context

    def _format_student_input(self, problem, history):
        """æ ¼å¼åŒ–å­¦ç”Ÿè¾“å…¥"""
        context = f"Original Problem: {problem}\n\nDialogue History:\n"
        for role, content in history:
            context += f"{role}: {content}\n\n"

        # æ ¹æ®å¹¶è¡Œæ€è€ƒå¼€å…³è°ƒæ•´æç¤º
        if self.parallel_thinking_enabled:
            context += "Use parallel thinking to consider multiple approaches to this problem."

        context += "Please continue solving the problem or respond to the teacher's guidance:"
        return context

    def _should_end_dialogue(self, teacher_response):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»“æŸå¯¹è¯"""
        end_phrases = ["finished", "completed"]
        return any(phrase in teacher_response.lower() for phrase in end_phrases)

    def _has_correct_answer(self, student_response, problem):
        """ç®€å•åˆ¤æ–­å­¦ç”Ÿæ˜¯å¦å¾—å‡ºæ­£ç¡®ç­”æ¡ˆ"""
        # è¿™é‡Œå¯ä»¥é›†æˆæ›´å¤æ‚çš„ç­”æ¡ˆéªŒè¯é€»è¾‘
        math_indicators = ["answer is", "result is", "equals", "=", "solution is", "final answer"]
        return any(indicator in student_response.lower() for indicator in math_indicators)

    def _get_final_answer(self):
        """èŽ·å–æœ€ç»ˆç­”æ¡ˆ"""
        student_final_answer = ""
        if self.dialogue_history:
            for role, content in self.dialogue_history:
                if role == "student":
                    student_final_answer = content[1]
            return student_final_answer
        return ""

    def agent_response_invoke(self, user_input, **kwargs):
        self.response = self.agent.invoke({
            "messages": [{"role": "user", "content": user_input}]
        }, config=self.agent_config, context=self.context, **kwargs)

    def agent_response_stream(self, user_input, **kwargs):
        for chunk in self.agent.stream({
            "messages": [{"role": "user", "content": user_input}]
        }, config=self.agent_config, context=self.context, stream_mode="values", **kwargs):
            latest_message = chunk["messages"][-1]
            if latest_message.content:
                print(f"Agent: {latest_message.content}")
            elif latest_message.tool_calls:
                print(f"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}")

    def agent_output(self, all_messages=False):
        if all_messages:
            for i in self.response['messages']:
                print(f"{type(i)}: {i.content}")
        else:
            print(f"{type(self.response['messages'][-1])}: {self.response['structured_response'].main_response}")

    def get_dialogue_summary(self):
        """èŽ·å–å¯¹è¯æ‘˜è¦"""
        summary = f"Dialogue Summary ({self.current_turn} turns):\n"
        for i, (role, content) in enumerate(self.dialogue_history):
            summary += f"Turn {i + 1} ({role}): {content[:100]}...\n"
        return summary


# å…¶ä»–ç±»ä¿æŒä¸å˜...
class CustomState(AgentState):
    user_preferences: dict


class CustomMiddleware(AgentMiddleware):
    state_schema = CustomState

    def before_model(self, state: CustomState, runtime):
        if state.user_preferences.get("style") == "technical":
            return {"system_prompt": "Provide detailed technical explanations with specifications."}
        else:
            return {"system_prompt": "Explain concepts in simple, easy-to-understand terms."}

    def before_tool_call(self, state: CustomState, runtime, tool_name):
        if state.user_preferences.get("verbosity") == "detailed":
            return "technical_search"
        else:
            return "simple_search"


class MiddlewareFunc(object):
    def __init__(self, basic_model, advanced_model):
        self.basic_model = basic_model
        self.advanced_model = advanced_model

    def middleware_handle_tool_errors(self) -> Callable:
        @wrap_tool_call
        def handle_tool_errors(request, handler):
            try:
                return handler(request)
            except Exception as e:
                return ToolMessage(
                    content=f"Tool error: Please check your input and try again. ({str(e)})",
                    tool_call_id=request.tool_call["id"]
                )

        return handle_tool_errors

    def middleware_dynamic_model_selection(self) -> Callable:
        @wrap_model_call
        def dynamic_model_middleware(request: ModelRequest, handler) -> ModelResponse:
            messages = request.state.get("messages", [])
            message_count = len(messages)
            print(f"Debug: Current message count = {message_count}")

            if message_count >= 2:
                user_content = ""
                for msg in reversed(messages):
                    if isinstance(msg, HumanMessage):
                        user_content = msg.content
                        break
                complex_keywords = ['solution', 'problem', 'explain', 'analyze', 'how to', 'why', 'compare']
                is_complex = any(keyword in user_content.lower() for keyword in complex_keywords)
                if is_complex and self.advanced_model:
                    model = self.advanced_model
                    model_name = self.advanced_model.model_name
                    print(f'ðŸ”€ Selected ADVANCED model for complex query: {model_name}')
                else:
                    model = self.basic_model
                    model_name = self.basic_model.model_name
                    print(f'ðŸ”€ Selected BASIC model: {model_name}')
            else:
                model = self.basic_model
                model_name = self.basic_model.model_name
                print(f'ðŸ”€ Selected BASIC model (first message): {model_name}')
            request.model = model
            return handler(request)

        return dynamic_model_middleware

    def middleware_user_role_prompt(self) -> Callable:
        @dynamic_prompt
        def user_role_prompt(request: ModelRequest) -> str:
            user_role = request.runtime.context.user_role
            if user_role == "expert":
                return EXPECT_PROMPT
            elif user_role == "beginner":
                return BEGINNER_PROMPT
            return BASE_PROMPT

        return user_role_prompt