from function.agent_IO import ModelConfig, ExperimentRecorder, ExpertStudentAgent, SimpleAgent, SummaryConfig, StudentCognitiveState, DialogueRecord
from utils.evaluator import *
from utils.dataset.parallel_thinking_sft_dataset import RawDataset

from langchain.chat_models import init_chat_model
from tqdm import tqdm
import torch
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

device_name = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"Using device: {device_name}")


class MultiAgentSystem:
    """å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç®¡ç†å™¨"""

    def __init__(self, config):
        self.config = config
        self.student_agent = None
        self.teacher_agent = None
        self.expert_agent = None
        self.summary_llm = None
        self.current_solution_tree = None
        exp_name = f"math_tutoring_explicit_interaction" if config.agent.explicit_interaction else "math_tutoring_explicit"
        self.experiment_recorder = ExperimentRecorder(exp_name)

    def initialize_agents(self):
        """åˆå§‹åŒ–å­¦ç”Ÿå’Œæ•™å¸ˆæ™ºèƒ½ä½“"""
        debug_mode = self.config.agent.get("debug_mode", False)

        # å­¦ç”Ÿæ¨¡å‹é…ç½®
        student_model_config = ModelConfig(
            model_type=self.config.model.model_type_student,
            model_name=self.config.model.api_name_student,
            base_url=self.config.model.base_url_student if hasattr(self.config.model, 'base_url_student') else None,
            temperature=self.config.model.temperature_student,
            max_tokens=self.config.model.max_tokens_student
        )

        # æ•™å¸ˆæ¨¡å‹é…ç½®
        teacher_model_config = ModelConfig(
            model_type=self.config.model.model_type_teacher,
            model_name=self.config.model.api_name_teacher,
            base_url=self.config.model.base_url_teacher if hasattr(self.config.model, 'base_url_teacher') else None,
            temperature=self.config.model.temperature_teacher,
            max_tokens=self.config.model.max_tokens_teacher
        )

        # åˆå§‹åŒ–å­¦éœ¸Agentï¼ˆå¦‚æœå¯ç”¨è§£é¢˜æ ‘ï¼‰
        if self.config.agent.get("use_solution_tree", False):
            self.expert_agent = ExpertStudentAgent(
                debug_mode=self.config.agent.get("debug_mode", False)
            )
            expert_model_config = ModelConfig(
                model_type=self.config.model.model_type_expert,  # ä½¿ç”¨æ•™å¸ˆæ¨¡å‹é…ç½®
                model_name=self.config.model.api_name_expert,
                base_url=self.config.model.base_url_expert if hasattr(self.config.model, 'base_url_student') else None,
                temperature=self.config.model.temperature_expert,
                max_tokens=self.config.model.max_tokens_expert
            )
            self.expert_agent.model_init(expert_model_config, 'expert')
            self.expert_agent.config_create("thread_id", "expert_1")
            self.expert_agent.agent_init(
                expert_model_config,
                prompt_sys_name=self.config.agent.expert_sys_prompt,
                tools_list=[]
            )
            logger.info(f"âœ… Expert agent initialized: {self.config.model.api_name_student}")

        # åˆå§‹åŒ–å­¦ç”Ÿæ™ºèƒ½ä½“
        self.student_agent = SimpleAgent(agent_type="student", debug_mode=debug_mode)
        self.student_agent.model_init(student_model_config, 'student')
        self.student_agent.config_create("thread_id", "student_1")

        # åˆå§‹åŒ–æ•™å¸ˆæ™ºèƒ½ä½“
        self.teacher_agent = SimpleAgent(agent_type="teacher", debug_mode=debug_mode)
        self.teacher_agent.model_init(teacher_model_config, 'teacher')
        self.teacher_agent.config_create("thread_id", "teacher_1")

        # é…ç½®å­¦ç”Ÿè®¤çŸ¥çŠ¶æ€
        self._setup_cognitive_state()

        # é…ç½®å¯¹è¯æ‘˜è¦åŠŸèƒ½
        self._setup_conversation_summary()

        # é…ç½®æ™ºèƒ½ä½“å‚æ•°
        student_kwargs = {
            "max_turns": self.config.agent.get("max_turns", 5),
            "parallel_thinking": self.config.agent.get("parallel_thinking", False),
        }

        teacher_kwargs = {
            "socratic_teaching": self.config.agent.get("socratic_teaching", False),
        }

        # å·¥å…·é…ç½®
        student_tools = []
        teacher_tools = []

        # if self.config.agent.get("parallel_thinking", False):
        #     student_tools.append(parallel_thinking)
        #
        # if self.config.agent.get("socratic_teaching", True):
        #     teacher_tools.append(socratic_questioning)
        #     teacher_tools.append(math_concept_explainer)

        if self.config.agent.get("use_solution_tree", False):
            teacher_kwargs["prompt_solution_tree"] = self.config.agent.teacher_tree_sys_prompt
            student_kwargs["prompt_solution_tree"] = self.config.agent.student_tree_sys_prompt

        self.student_agent.agent_init(
            student_model_config,
            prompt_sys_name=self.config.agent.student_sys_prompt,
            tools_list=student_tools,
            **student_kwargs
        )
        self.teacher_agent.agent_init(
            teacher_model_config,
            prompt_sys_name=self.config.agent.teacher_sys_prompt,
            tools_list=teacher_tools,
            **teacher_kwargs
        )

        # å¯ç”¨æ‘˜è¦åŠŸèƒ½
        summary_config = SummaryConfig(
            enabled=self.config.agent.get("conversation_summary", False),
            max_turns=self.config.agent.get("summary_max_turns", 8),
            max_token_limit=self.config.agent.get("summary_max_tokens", 1500)
        )

        summary_enabled = self.config.agent.get("conversation_summary", False)
        if summary_enabled:
            self.student_agent.enable_conversation_summary(summary_config, self.summary_llm)
            self.teacher_agent.enable_conversation_summary(summary_config, self.summary_llm)
            logger.info(f"âœ… å¯¹è¯æ‘˜è¦åŠŸèƒ½å·²å¯ç”¨ (max_turns: {summary_config.max_turns})")
        else:
            logger.info("âœ… å¯¹è¯æ‘˜è¦åŠŸèƒ½å·²ç¦ç”¨")

        # è®¾ç½®ä¸Šä¸‹æ–‡
        self.student_agent.context_set(
            user_id="Jack",
            user_role="student",
            parallel_thinking=student_kwargs["parallel_thinking"],
            conversation_mode="explicit" if self.config.agent.get("explicit_interaction", True) else "tool_based"
        )
        self.teacher_agent.context_set(
            user_id="Professor Smith",
            user_role="teacher",
            socratic_teaching=teacher_kwargs["socratic_teaching"]
        )

        logger.info("âœ… Multi-agent system initialized successfully!")
        logger.info(f"   Student: {self.config.model.api_name_student}")
        logger.info(f"   Teacher: {self.config.model.api_name_teacher}")
        logger.info(f"   Expert: {'Enabled' if self.expert_agent else 'Disabled'}")
        logger.info(f"   Cognitive State: {'Enabled' if self.config.agent.get('use_cognitive_state', False) else 'Disabled'}")
        logger.info(f"   Solution Tree: {'Enabled' if self.config.agent.get('use_solution_tree', False) else 'Disabled'}")
        logger.info(
            f"   Mode: {'Explicit Interaction' if self.config.agent.get('explicit_interaction', True) else 'Tool-based'}")
        logger.info(f"   Parallel Thinking: {student_kwargs['parallel_thinking']}")
        logger.info(f"   Socratic Teaching: {teacher_kwargs['socratic_teaching']}")

        return self.student_agent, self.teacher_agent

    def _setup_cognitive_state(self):
        """é…ç½®å­¦ç”Ÿè®¤çŸ¥çŠ¶æ€"""
        if self.config.agent.get("use_cognitive_state", False):
            cognitive_state = StudentCognitiveState(
                carelessness_level=self.config.agent.get("carelessness_level", 5),
                math_background=self.config.agent.get("math_background", "intermediate"),
                response_style=self.config.agent.get("response_style", "thoughtful"),
                preferred_method=self.config.agent.get("preferred_method", "balanced"),
                learning_style=self.config.agent.get("learning_style", "reading-writing")
            )
            self.student_agent.set_cognitive_state(cognitive_state)

    def generate_solution_tree(self, problem):
        """ç”Ÿæˆè§£é¢˜æ ‘"""
        if self.expert_agent and hasattr(self.expert_agent, 'generate_solution_tree'):
            self.current_solution_tree = self.expert_agent.generate_solution_tree(problem)

            # è®¾ç½®è§£é¢˜æ ‘åˆ°å­¦ç”Ÿå’Œæ•™å¸ˆagent
            if self.current_solution_tree:
                self.student_agent.set_solution_tree(self.current_solution_tree)
                self.teacher_agent.set_solution_tree(self.current_solution_tree)

            return self.current_solution_tree
        return None

    def update_cognitive_state(self, problem, student_approach, errors, method_used, success):
        """æ›´æ–°å­¦ç”Ÿè®¤çŸ¥çŠ¶æ€"""
        if (self.config.agent.get("use_cognitive_state", False) and
                self.student_agent.cognitive_state):
            self.student_agent.cognitive_state.update_based_on_interaction(
                problem, student_approach, errors, method_used, success
            )

    def get_cognitive_state(self):
        """è·å–å½“å‰è®¤çŸ¥çŠ¶æ€"""
        if (self.config.agent.get("use_cognitive_state", False) and
                self.student_agent.cognitive_state):
            return self.student_agent.cognitive_state.to_dict()
        return None

    def update_cognitive_state_based_on_dialogue(self, problem, dialogue_record, success):
        """åŸºäºå¯¹è¯è®°å½•æ›´æ–°è®¤çŸ¥çŠ¶æ€"""
        if not self.config.agent.get("use_cognitive_state", False):
            return

        if not self.student_agent or not self.student_agent.cognitive_state:
            return

        # åˆ†æå­¦ç”Ÿçš„æ–¹æ³•å’Œé”™è¯¯
        student_approach = _analyze_student_approach(dialogue_record)
        errors = _extract_errors(dialogue_record)
        method_used = _detect_method_used(dialogue_record)
        response_characteristics = _analyze_student_response_characteristics(dialogue_record)

        # æ›´æ–°è®¤çŸ¥çŠ¶æ€
        self.student_agent.cognitive_state.update_based_on_interaction(
            problem, student_approach, errors, method_used, success
        )

        # æ›´æ–°å›å¤é£æ ¼ï¼ˆåŸºäºè§‚å¯Ÿï¼‰
        if response_characteristics != "neutral":
            self.student_agent.cognitive_state.response_style = response_characteristics

    def analyze_student_progress(self, dialogue_record):
        """åˆ†æå­¦ç”Ÿè¿›æ­¥æƒ…å†µ"""
        if not self.config.agent.get("use_cognitive_state", False):
            return None

        if not self.student_agent or not self.student_agent.cognitive_state:
            return None

        cognitive_state = self.student_agent.cognitive_state.to_dict()

        progress_info = {
            "carelessness_trend": "stable",
            "background_improvement": "none",
            "method_preference": cognitive_state["preferred_method"],
            "recent_success_rate": cognitive_state["recent_success_rate"]
        }

        # åˆ†æç²—å¿ƒç¨‹åº¦è¶‹åŠ¿
        if len(self.student_agent.cognitive_state.problem_solving_history) >= 3:
            recent_errors = sum(1 for record in self.student_agent.cognitive_state.problem_solving_history[-3:]
                                if record["errors"])
            earlier_errors = sum(1 for record in self.student_agent.cognitive_state.problem_solving_history[-6:-3]
                                 if record["errors"])

            if recent_errors < earlier_errors:
                progress_info["carelessness_trend"] = "improving"
            elif recent_errors > earlier_errors:
                progress_info["carelessness_trend"] = "worsening"

        return progress_info

    def _setup_conversation_summary(self):
        """é…ç½®å¯¹è¯æ‘˜è¦åŠŸèƒ½"""
        summary_enabled = self.config.agent.get("conversation_summary", False)
        if not summary_enabled:
            self.summary_llm = None
            return

        try:
            # ä½¿ç”¨ä¸“é—¨é…ç½®çš„æ‘˜è¦æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨è½»é‡çº§æ¨¡å‹
            summary_model_name = self.config.agent.get("summary_model_name", "qwen2.5:0.5b")
            summary_model_type = self.config.agent.get("summary_model_type", "local")
            summary_base_url = self.config.agent.get("summary_base_url", "http://localhost:11434")

            if summary_model_type == "local":
                from langchain_ollama import ChatOllama
                self.summary_llm = ChatOllama(
                    model=summary_model_name,
                    base_url=summary_base_url,
                    temperature=self.config.agent.get("temperature_summary_model", 0.2),
                    num_predict=self.config.agent.get("max_tokens_summary_model", 2000),
                )
                logger.info(f"âœ… Summary model: {summary_model_name} (local)")
            else:
                # APIæ‘˜è¦æ¨¡å‹
                api_kwargs = {
                    "temperature": 0.1,
                    "timeout": 20,
                    "max_tokens": 300,
                }
                self.summary_llm = init_chat_model(summary_model_name, **api_kwargs)
                logger.info(f"âœ… Summary model: {summary_model_name} (API)")

        except Exception as e:
            logger.warning(f"âŒ Failed to setup summary model: {e}")
            logger.info("âš ï¸  Summary feature will use main model if available")
            self.summary_llm = None

    def get_memory_statistics(self):
        """è·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        if self.student_agent:
            stats["student"] = self.student_agent.get_memory_stats()
        if self.teacher_agent:
            stats["teacher"] = self.teacher_agent.get_memory_stats()
        return stats

    def clear_all_memory(self):
        """æ¸…ç©ºæ‰€æœ‰å†…å­˜"""
        if self.student_agent:
            self.student_agent.clear_memory()
        if self.teacher_agent:
            self.teacher_agent.clear_memory()

    def visualize_results(self, output_dir: str = "results"):
        """å¯è§†åŒ–å®éªŒç»“æœ"""
        if not self.experiment_recorder.records:
            print("âŒ æ²¡æœ‰å®éªŒæ•°æ®å¯ä¾›å¯è§†åŒ–")
            return

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self.experiment_recorder.calculate_statistics()
        stats = self.experiment_recorder.summary_stats

        # è®¾ç½®ç»˜å›¾é£æ ¼
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Multi-Agent Math Tutoring Experiment Results', fontsize=16, fontweight='bold')

        # 1. å‡†ç¡®ç‡å’Œç­”æ¡ˆæ³„éœ²ç‡
        metrics = ['Accuracy', 'Answer Leakage Rate']
        values = [stats['accuracy'], stats['answer_leakage_rate']]
        colors = ['#2ecc71', '#e74c3c']

        bars1 = axes[0, 0].bar(metrics, values, color=colors, alpha=0.7)
        axes[0, 0].set_title('Accuracy vs Answer Leakage')
        axes[0, 0].set_ylabel('Rate')
        axes[0, 0].set_ylim(0, 1)

        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars1, values):
            axes[0, 0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                            f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

        # 2. å¹³å‡å¯¹è¯è½®æ•°åˆ†å¸ƒ
        turn_counts = [r.total_turns for r in self.experiment_recorder.records]
        axes[0, 1].hist(turn_counts, bins=range(1, max(turn_counts) + 2), alpha=0.7, color='#3498db', edgecolor='black')
        axes[0, 1].set_title('Distribution of Dialogue Turns')
        axes[0, 1].set_xlabel('Number of Turns')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].axvline(stats['avg_turns_per_problem'], color='red', linestyle='--',
                           label=f'Average: {stats["avg_turns_per_problem"]:.2f}')
        axes[0, 1].legend()

        # 3. å¹¶è¡Œæ€è€ƒå’Œæ€è€ƒè·¯å¾„
        thinking_data = ['Parallel Thinking', 'Thinking Paths']
        thinking_values = [stats['avg_parallel_thinking'], stats['avg_thinking_paths']]

        bars2 = axes[0, 2].bar(thinking_data, thinking_values, color=['#9b59b6', '#f39c12'], alpha=0.7)
        axes[0, 2].set_title('Average Thinking Metrics per Problem')
        axes[0, 2].set_ylabel('Average Count')

        for bar, value in zip(bars2, thinking_values):
            axes[0, 2].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,
                            f'{value:.2f}', ha='center', va='bottom', fontweight='bold')

        # 4. æ•™å¸ˆæ„å›¾åˆ†å¸ƒ
        teacher_intents = []
        for record in self.experiment_recorder.records:
            for turn in record.turns:
                if 'teacher_intent' in turn:
                    teacher_intents.append(turn['teacher_intent'])

        if teacher_intents:
            intent_counts = pd.Series(teacher_intents).value_counts()
            axes[1, 0].pie(intent_counts.values, labels=intent_counts.index, autopct='%1.1f%%',
                           startangle=90, colors=plt.cm.Set3(np.linspace(0, 1, len(intent_counts))))
            axes[1, 0].set_title('Teacher Response Intent Distribution')

        # 5. æ­£ç¡®ç­”æ¡ˆ vs é”™è¯¯ç­”æ¡ˆ
        correct_data = ['Correct', 'Incorrect']
        correct_values = [stats['correct_answers'], stats['total_problems'] - stats['correct_answers']]

        bars3 = axes[1, 1].bar(correct_data, correct_values, color=['#27ae60', '#c0392b'], alpha=0.7)
        axes[1, 1].set_title('Correct vs Incorrect Answers')
        axes[1, 1].set_ylabel('Count')

        for bar, value in zip(bars3, correct_values):
            axes[1, 1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,
                            f'{value}', ha='center', va='bottom', fontweight='bold')

        # 6. è½®æ¬¡ä¸å‡†ç¡®ç‡çš„å…³ç³»
        turns_vs_correct = []
        for record in self.experiment_recorder.records:
            turns_vs_correct.append((record.total_turns, 1 if record.correct else 0))

        if turns_vs_correct:
            df = pd.DataFrame(turns_vs_correct, columns=['turns', 'correct'])
            accuracy_by_turns = df.groupby('turns')['correct'].mean().reset_index()
            axes[1, 2].plot(accuracy_by_turns['turns'], accuracy_by_turns['correct'],
                            marker='o', linewidth=2, markersize=8, color='#e67e22')
            axes[1, 2].set_title('Accuracy by Number of Turns')
            axes[1, 2].set_xlabel('Number of Turns')
            axes[1, 2].set_ylabel('Accuracy Rate')
            axes[1, 2].grid(True, alpha=0.3)

        plt.tight_layout()

        # ä¿å­˜å›¾ç‰‡
        import os
        os.makedirs(output_dir, exist_ok=True)
        plot_file = os.path.join(output_dir,
                                 f"{self.experiment_recorder.experiment_name}_{self.experiment_recorder.timestamp}_plots.png")
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.show()

        print(f"âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {plot_file}")
        return plot_file

    def visualize_cognitive_progress(self, output_dir: str = "results"):
        """å¯è§†åŒ–è®¤çŸ¥çŠ¶æ€è¿›æ­¥"""
        if not self.config.agent.get("use_cognitive_state", False):
            return None

        if not self.student_agent or not self.student_agent.cognitive_state:
            return None

        # æ”¶é›†å†å²æ•°æ®
        history = self.student_agent.cognitive_state.problem_solving_history
        if len(history) < 2:
            return None

        # åˆ›å»ºè®¤çŸ¥çŠ¶æ€å˜åŒ–å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Student Cognitive State Progress', fontsize=16, fontweight='bold')

        # 1. ç²—å¿ƒç¨‹åº¦å˜åŒ–
        carelessness_levels = []
        success_rates = []

        # è®¡ç®—æ»‘åŠ¨çª—å£çš„æˆåŠŸç‡
        window_size = min(3, len(history))
        for i in range(len(history)):
            window_start = max(0, i - window_size + 1)
            window = history[window_start:i + 1]
            success_rate = sum(1 for record in window if record["success"]) / len(window)
            success_rates.append(success_rate)

            # æ¨¡æ‹Ÿç²—å¿ƒç¨‹åº¦å˜åŒ–ï¼ˆåŸºäºé”™è¯¯é¢‘ç‡ï¼‰
            recent_errors = sum(1 for record in window if record["errors"])
            carelessness = max(1, min(10, 5 + recent_errors * 2))  # ç®€å•æ¨¡æ‹Ÿ
            carelessness_levels.append(carelessness)

        # ç²—å¿ƒç¨‹åº¦å›¾
        axes[0, 0].plot(range(len(carelessness_levels)), carelessness_levels,
                        marker='o', linewidth=2, color='#e74c3c')
        axes[0, 0].set_title('Carelessness Level Trend')
        axes[0, 0].set_xlabel('Problem Sequence')
        axes[0, 0].set_ylabel('Carelessness Level (1-10)')
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_ylim(1, 10)

        # 2. æˆåŠŸç‡å˜åŒ–
        axes[0, 1].plot(range(len(success_rates)), success_rates,
                        marker='s', linewidth=2, color='#2ecc71')
        axes[0, 1].set_title('Success Rate Trend')
        axes[0, 1].set_xlabel('Problem Sequence')
        axes[0, 1].set_ylabel('Success Rate')
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].set_ylim(0, 1)

        # 3. æ–¹æ³•åå¥½åˆ†å¸ƒ
        method_counts = {}
        for record in history:
            method = record.get("method_used", "unknown")
            method_counts[method] = method_counts.get(method, 0) + 1

        if method_counts:
            methods = list(method_counts.keys())
            counts = list(method_counts.values())
            colors = plt.cm.Set3(np.linspace(0, 1, len(methods)))

            axes[1, 0].pie(counts, labels=methods, autopct='%1.1f%%',
                           colors=colors, startangle=90)
            axes[1, 0].set_title('Method Preference Distribution')

        # 4. é”™è¯¯ç±»å‹åˆ†æ
        error_types = {}
        for record in history:
            for error in record.get("errors", []):
                error_types[error] = error_types.get(error, 0) + 1

        if error_types:
            errors = list(error_types.keys())
            error_counts = list(error_types.values())

            bars = axes[1, 1].bar(range(len(errors)), error_counts,
                                  color='#f39c12', alpha=0.7)
            axes[1, 1].set_title('Error Type Analysis')
            axes[1, 1].set_xlabel('Error Type')
            axes[1, 1].set_ylabel('Frequency')
            axes[1, 1].set_xticks(range(len(errors)))
            axes[1, 1].set_xticklabels(errors, rotation=45)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, count in zip(bars, error_counts):
                axes[1, 1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,
                                f'{count}', ha='center', va='bottom')

        plt.tight_layout()

        # ä¿å­˜å›¾ç‰‡
        import os
        os.makedirs(output_dir, exist_ok=True)
        plot_file = os.path.join(output_dir, f"cognitive_progress_{self.experiment_recorder.timestamp}.png")
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.show()

        print(f"âœ… Cognitive progress visualization saved to: {plot_file}")
        return plot_file


def create_raw_dataset(data_paths, data_config):
    # ä¿æŒåŸæœ‰å®ç°
    dataset = RawDataset(parquet_file=data_paths, config=data_config)
    return dataset


def solve_with_dialogue(config, logger, val_dataset, multi_agent_system):
    """ä½¿ç”¨å¤šæ™ºèƒ½ä½“å¯¹è¯è§£å†³é—®é¢˜"""
    first_correct = torch.zeros(1, dtype=torch.float32, device=device_name)
    correct = torch.zeros(1, dtype=torch.float32, device=device_name)
    dialogue_count = torch.zeros(1, dtype=torch.float32, device=device_name)
    parallel_thinking_count = torch.zeros(1, dtype=torch.float32, device=device_name)
    thinking_paths_count = torch.zeros(1, dtype=torch.float32, device=device_name)
    leaked_answer_count = torch.zeros(1, dtype=torch.float32, device=device_name)
    total = torch.zeros(1, dtype=torch.float32, device=device_name)
    error_ans = torch.zeros(1, dtype=torch.float32, device=device_name)
    # æ–°æŒ‡æ ‡ï¼šä¸€é¢˜å¤šè§£èƒ½åŠ›
    multi_solution_scores = []
    cognitive_progress = []

    student_agent, teacher_agent = multi_agent_system.initialize_agents()

    explicit_mode = config.agent.get("explicit_interaction", True)
    eval_mode = 'Explicit Interaction' if explicit_mode else 'Tool-based'

    if config.agent.get("max_samples", None):
        max_samples = config.agent.get("max_samples", min(10, len(val_dataset.prompts)))
    else:
        max_samples = len(val_dataset.prompts)

    desc = f"Multi-agent Evaluation [{eval_mode}] - Dataset: {'GSM8k' if 'gsm' in config.data.val_files.lower() else 'MATH'}"

    for i, (raw_problem, prompt, label_ans) in tqdm(
            enumerate(zip(val_dataset.raw_problem[:max_samples], val_dataset.prompts[:max_samples], val_dataset.responses[:max_samples])),
            desc=desc, total=max_samples
    ):
        # åœ¨å¯¹è¯å¼€å§‹å‰æ¸…ç©ºå†…å­˜ï¼ˆç¡®ä¿æ¯æ¬¡é—®é¢˜ç‹¬ç«‹ï¼‰
        multi_agent_system.clear_all_memory()

        # ç”Ÿæˆè§£é¢˜æ ‘ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if config.agent.get("use_solution_tree", False):
            solution_tree = multi_agent_system.generate_solution_tree(raw_problem)
            if solution_tree:
                logger.info(f"ğŸŒ³ Generated solution tree with {len(solution_tree.solution_paths)} expert paths")

        # åˆ›å»ºå¯¹è¯è®°å½•
        dialogue_record = DialogueRecord(prompt, label_ans,
                                         debug_mode=True if i < config.agent.debug_samples else False)

        # è®°å½•å­¦ç”Ÿåˆå§‹è®¤çŸ¥çŠ¶æ€
        initial_cognitive_state = multi_agent_system.get_cognitive_state()

        if explicit_mode:
            # æ¨¡å¼1: æ˜¾å¼äº¤äº’
            final_answer, correct_answer, dialogue_record = student_agent.multi_agent_chat_explicit(
                teacher_agent, prompt, raw_problem, label_ans, dialogue_record
            )
        else:
            # æ¨¡å¼2: å·¥å…·è°ƒç”¨
            correct_answer = None
            final_answer = student_agent.multi_agent_chat_tool_based(
                prompt, label_ans, dialogue_record
            )

        total += 1

        if not final_answer:
            error_ans += 1
            dialogue_record.correct = False
            success = False
        else:
            if dialogue_record.first_correct:
                first_correct += 1
            if dialogue_record.correct:
                correct += 1
            success = dialogue_record.correct

        # æ·»åŠ è®°å½•åˆ°å®éªŒè®°å½•å™¨
        multi_agent_system.experiment_recorder.add_record(dialogue_record)

        if config.agent.get("use_cognitive_state", False):
            multi_agent_system.update_cognitive_state_based_on_dialogue(
                prompt, dialogue_record, success
            )

            # è®°å½•è®¤çŸ¥çŠ¶æ€å˜åŒ–
            current_state = multi_agent_system.get_cognitive_state()
            if initial_cognitive_state and current_state:
                logger.info(
                    f"ğŸ§  Cognitive State Updated - Carelessness: {initial_cognitive_state['carelessness_level']} -> {current_state['carelessness_level']}")

                # åˆ†æè¿›æ­¥æƒ…å†µ
                progress = multi_agent_system.analyze_student_progress(dialogue_record)
                if progress:
                    cognitive_progress.append(progress)
                    logger.info(
                        f"ğŸ“ˆ Progress: {progress['carelessness_trend']}, Success Rate: {progress['recent_success_rate']:.2f}")

        # è®¡ç®—ä¸€é¢˜å¤šè§£åˆ†æ•°ï¼ˆå¦‚æœå¯ç”¨è§£é¢˜æ ‘ï¼‰
        # åœ¨å¯¹è¯ç»“æŸåï¼Œè¾“å‡ºè§£é¢˜æ ‘ä¿¡æ¯
        if config.agent.get("use_solution_tree", False) and multi_agent_system.current_solution_tree:
            solution_tree = multi_agent_system.current_solution_tree
            logger.info(f"ğŸŒ³ è§£é¢˜æ ‘ç»Ÿè®¡:")
            logger.info(f"   ä¸“å®¶è·¯å¾„æ•°: {len([p for p in solution_tree.solution_paths if p.get('type') == 'expert'])}")
            logger.info(
                f"   å­¦ç”Ÿè·¯å¾„æ•°: {len([p for p in solution_tree.solution_paths if p.get('type') == 'student'])}")

            # è¾“å‡ºå­¦ç”Ÿè·¯å¾„è¯¦æƒ…
            student_paths = [p for p in solution_tree.solution_paths if p.get('type') == 'student']
            for i, path in enumerate(student_paths):
                logger.info(
                    f"   å­¦ç”Ÿè·¯å¾„ {i + 1}: æ­¥éª¤æ•°={len(path.get('steps', []))}, æˆåŠŸ={path.get('success', False)}, æ–¹æ³•={path.get('method', 'unknown')}")
            multi_solution_score = _calculate_multi_solution_score(multi_agent_system.current_solution_tree)
            multi_solution_scores.append(multi_solution_score)
            logger.info(f"ğŸ”¢ Multi-solution Score: {multi_solution_score:.2f}")

        dialogue_count += dialogue_record.total_turns
        parallel_thinking_count += dialogue_record.parallel_thinking_count
        thinking_paths_count += dialogue_record.thinking_paths_count
        leaked_answer_count += dialogue_record.leaked_answer

        # æ‰“å°å½“å‰è¿›åº¦
        current_first_accuracy = first_correct.item() / total.item() if total.item() > 0 else 0
        current_accuracy = correct.item() / total.item() if total.item() > 0 else 0
        avg_dialogue_count = dialogue_count.item() / total.item() if total.item() > 0 else 0
        avg_parallel_thinking_count = parallel_thinking_count.item() / total.item() if total.item() > 0 else 0
        avg_thinking_paths_count = thinking_paths_count.item() / total.item() if total.item() > 0 else 0
        avg_leaked_answer_count = leaked_answer_count.item() / total.item() if total.item() > 0 else 0

        logger.info(f'\nğŸ“Š Sample {i + 1}/{max_samples}:')
        logger.info(f'   First Correct: {dialogue_record.first_correct}')
        logger.info(f'   Final Correct: {dialogue_record.correct}')
        logger.info(f'   Current First Accuracy: {current_first_accuracy:.4f}')
        logger.info(f'   Current Final Accuracy: {current_accuracy:.4f}')
        logger.info(f'   Current Avg Dialogue Round: {avg_dialogue_count:.4f}')
        logger.info(f'   Current Avg Parallel Thinking Count: {avg_parallel_thinking_count:.4f}')
        logger.info(f'   Current Avg Thinking Paths Count: {avg_thinking_paths_count:.4f}')
        logger.info(f'   Current Avg Leaked Answer: {avg_leaked_answer_count:.4}')

        # æ¯å¤„ç†å®Œä¸€ä¸ªé—®é¢˜åæ‰“å°å†…å­˜ç»Ÿè®¡
        if config.agent.get("conversation_summary", False):
            memory_stats = multi_agent_system.get_memory_statistics()
            logger.info(f"ğŸ§  å†…å­˜ç»Ÿè®¡ - å­¦ç”Ÿ: {memory_stats['student']}, æ•™å¸ˆ: {memory_stats['teacher']}")

    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡å¹¶ä¿å­˜ç»“æœ
    multi_agent_system.experiment_recorder.calculate_statistics()
    multi_agent_system.experiment_recorder.save_results()
    multi_agent_system.experiment_recorder.print_summary()

    # è®¡ç®—ä¸€é¢˜å¤šè§£å¹³å‡åˆ†
    if multi_solution_scores:
        avg_multi_solution = sum(multi_solution_scores) / len(multi_solution_scores)
        logger.info(f"\nğŸ¯ Average Multi-Solution Score: {avg_multi_solution:.3f}")

    # åˆ†æè®¤çŸ¥è¿›æ­¥è¶‹åŠ¿
    if cognitive_progress:
        improving_count = sum(1 for progress in cognitive_progress if progress["carelessness_trend"] == "improving")
        improvement_rate = improving_count / len(cognitive_progress)
        logger.info(f"ğŸ§  Cognitive Improvement Rate: {improvement_rate:.3f} ({improving_count}/{len(cognitive_progress)})")

    # ç”Ÿæˆå¯è§†åŒ–
    multi_agent_system.visualize_results()
    multi_agent_system.visualize_cognitive_progress()

    final_accuracy = correct.item() / total.item() if total.item() > 0 else 0
    logger.info(f"\nğŸ¯ Final Results:")
    logger.info(f"   Total Samples: {total.item()}")
    logger.info(f"   Correct Answers: {correct.item()}")
    logger.info(f"   Error Answers: {error_ans.item()}")
    logger.info(f"   Final Accuracy: {final_accuracy:.4f}")

    return final_accuracy


def _calculate_multi_solution_score(solution_tree):
    """è®¡ç®—ä¸€é¢˜å¤šè§£åˆ†æ•°"""
    if not solution_tree or not solution_tree.solution_paths:
        return 0.0

    expert_paths = [p for p in solution_tree.solution_paths if p["type"] == "expert"]
    student_paths = [p for p in solution_tree.solution_paths if p["type"] == "student"]

    if not expert_paths:
        return 0.0

    # åŸºäºä¸“å®¶è·¯å¾„æ•°é‡å’Œå­¦ç”Ÿå°è¯•çš„æ–¹æ³•å¤šæ ·æ€§è¯„åˆ†
    base_score = min(len(expert_paths) / 3.0, 1.0)  # æœ€å¤š3ä¸ªä¸“å®¶è·¯å¾„

    if student_paths:
        student_methods = set(p["method"] for p in student_paths if p["method"] != "unknown")
        method_diversity = len(student_methods) / len(expert_paths)
        return (base_score + method_diversity) / 2.0
    else:
        return base_score * 0.5  # æ²¡æœ‰å­¦ç”Ÿè·¯å¾„æ—¶åˆ†æ•°å‡åŠ


def _analyze_student_approach(dialogue_record):
    """åˆ†æå­¦ç”Ÿè§£é¢˜æ–¹æ³•"""
    if not dialogue_record.turns:
        return "unknown"

    # ä»å¯¹è¯è®°å½•ä¸­æå–å­¦ç”Ÿå›å¤
    student_responses = []
    for turn in dialogue_record.turns:
        if 'student_response' in turn and turn['student_response']:
            student_responses.append(turn['student_response'])

    if not student_responses:
        return "unknown"

    # åˆ†ææœ€åçš„å­¦ç”Ÿå›å¤
    last_response = student_responses[-1].lower()

    # æ£€æµ‹æ–¹æ³•ç±»å‹
    if any(word in last_response for word in ["equation", "solve for", "variable", "x =", "let x"]):
        return "algebraic"
    elif any(word in last_response for word in ["diagram", "graph", "shape", "angle", "area", "triangle"]):
        return "geometric"
    elif any(word in last_response for word in ["calculate", "compute", "number", "digit", "sum", "total"]):
        return "computational"
    elif any(word in last_response for word in ["logic", "reason", "therefore", "because", "since"]):
        return "logical"
    elif any(word in last_response for word in ["guess", "try", "maybe", "perhaps"]):
        return "trial_and_error"
    else:
        return "general"


def _extract_errors(dialogue_record):
    """ä»å¯¹è¯è®°å½•ä¸­æå–é”™è¯¯æ¨¡å¼"""
    errors = []

    if not dialogue_record.turns:
        return errors

    # åˆ†ææ•™å¸ˆå›å¤ä¸­çš„çº æ­£ä¿¡æ¯
    for turn in dialogue_record.turns:
        if 'teacher_response' in turn and turn['teacher_response']:
            teacher_response = turn['teacher_response'].lower()

            # æ£€æµ‹é”™è¯¯ç±»å‹
            if any(word in teacher_response for word in ["wrong", "incorrect", "mistake", "error"]):
                if "calculation" in teacher_response or "compute" in teacher_response:
                    errors.append("calculation_error")
                elif "concept" in teacher_response or "understand" in teacher_response:
                    errors.append("conceptual_error")
                elif "method" in teacher_response or "approach" in teacher_response:
                    errors.append("methodological_error")
                elif "step" in teacher_response or "process" in teacher_response:
                    errors.append("procedural_error")
                else:
                    errors.append("general_error")

    return errors


def _detect_method_used(dialogue_record):
    """æ£€æµ‹å­¦ç”Ÿä½¿ç”¨çš„ä¸»è¦æ–¹æ³•"""
    if not dialogue_record.turns:
        return "unknown"

    # æ”¶é›†æ‰€æœ‰å­¦ç”Ÿå›å¤
    all_student_text = ""
    for turn in dialogue_record.turns:
        if 'student_response' in turn and turn['student_response']:
            all_student_text += " " + turn['student_response'].lower()

    # æ–¹æ³•æ£€æµ‹
    method_scores = {
        "algebraic": 0,
        "geometric": 0,
        "computational": 0,
        "logical": 0
    }

    # å…³é”®è¯åŒ¹é…
    algebraic_keywords = ["equation", "variable", "solve for", "x =", "let x", "algebra"]
    geometric_keywords = ["diagram", "graph", "shape", "angle", "area", "triangle", "circle"]
    computational_keywords = ["calculate", "compute", "number", "digit", "sum", "total", "multiply"]
    logical_keywords = ["logic", "reason", "therefore", "because", "since", "if then"]

    for keyword in algebraic_keywords:
        if keyword in all_student_text:
            method_scores["algebraic"] += 1

    for keyword in geometric_keywords:
        if keyword in all_student_text:
            method_scores["geometric"] += 1

    for keyword in computational_keywords:
        if keyword in all_student_text:
            method_scores["computational"] += 1

    for keyword in logical_keywords:
        if keyword in all_student_text:
            method_scores["logical"] += 1

    # è¿”å›å¾—åˆ†æœ€é«˜çš„æ–¹æ³•
    if not any(method_scores.values()):
        return "unknown"

    return max(method_scores.items(), key=lambda x: x[1])[0]


def _analyze_student_response_characteristics(dialogue_record):
    """åˆ†æå­¦ç”Ÿå›å¤ç‰¹å¾"""
    if not dialogue_record.turns:
        return "neutral"

    student_responses = []
    for turn in dialogue_record.turns:
        if 'student_response' in turn and turn['student_response']:
            student_responses.append(turn['student_response'])

    if not student_responses:
        return "neutral"

    # åˆ†æå›å¤é•¿åº¦å’Œå†…å®¹ç‰¹å¾
    total_length = sum(len(response) for response in student_responses)
    avg_length = total_length / len(student_responses)

    last_response = student_responses[-1].lower()

    # åˆ¤æ–­å›å¤é£æ ¼
    if avg_length > 300:
        return "detailed"
    elif avg_length < 100:
        return "brief"
    elif any(word in last_response for word in ["i think", "maybe", "perhaps", "not sure"]):
        return "thoughtful"
    elif any(word in last_response for word in ["obviously", "clearly", "definitely"]):
        return "confident"
    else:
        return "neutral"



def main(config, logger):
    val_dataset = create_raw_dataset(config.data.val_files, config.data)
    multi_agent_system = MultiAgentSystem(config)

    accuracy = solve_with_dialogue(config, logger, val_dataset, multi_agent_system)
    logger.info(f"\nğŸ¯ Final Accuracy: {accuracy:.4f}")


if __name__ == '__main__':
    import hydra
    from omegaconf import OmegaConf
    from hydra.core.global_hydra import GlobalHydra

    GlobalHydra.instance().clear()
    hydra.initialize(config_path="utils", version_base=None)

    overrides = []

    ''' 
    -----------------------------------
    ---------- ç³»ç»Ÿé…ç½®å’Œè¶…å‚æ•° ----------
    -----------------------------------
    '''
    overrides.append('data.val_files=D:\DeepLearning\Code\LangChain\dataset/APO_combine_with_source_test_without_path_2.parquet')
    # overrides.append('data.val_files=/mnt/t2-6tb/medical/SocraticLM_langchain/LangChain_3090/dataset/GSM8k_test_with_prompt4.parquet')
    overrides.append('+model.log_folder_path=D:\DeepLearning\Code\LangChain\log')
    # overrides.append('+model.log_folder_path=/mnt/t2-6tb/medical/SocraticLM_langchain/LangChain_3090/log')
    overrides.append('data.prompt_key=extra_info')
    overrides.append('data.response_key=extra_info')
    overrides.append('+data.raw_problem_key=extra_info')
    overrides.append('+data.first_prompt=raw')
    overrides.append('+data.prompt_dict_keys=[question]')
    overrides.append('+data.response_dict_keys=[answer]')
    overrides.append('+data.raw_problem_dict_keys=[raw_problem]')
    # overrides.append('+agent.max_samples=200')    # æµ‹è¯•ç”¨æ ·æœ¬æ•°é‡
    overrides.append('+agent.debug_samples=10')   # æŸ¥çœ‹å®Œæ•´response
    overrides.append('+agent.debug_mode=false')  # æŸ¥çœ‹å®Œæ•´prompt
    overrides.append('+agent.max_turns=3')

    ''' 
    -----------------------------------
    ---------- æ™ºèƒ½ä½“ä¸å¯¹è¯é…ç½® ----------
    -----------------------------------
    '''
    overrides.append('+model.api_name_student=deepseek-chat')
    overrides.append('+model.model_type_student=api')
    # overrides.append('+model.model_type_student=local')
    # overrides.append('+model.api_name_student=qwen3-4b-4k:latest')  # qwen3-4b-4k:latest   qwen3:4b-tuned-4k
    overrides.append('+model.base_url_student=http://localhost:11434')
    overrides.append('+model.temperature_student=0')
    overrides.append('+model.max_tokens_student=2000')

    overrides.append('+model.api_name_teacher=deepseek-chat')
    overrides.append('+model.model_type_teacher=api')
    # overrides.append('+model.model_type_teacher=local')
    # overrides.append('+model.api_name_teacher=qwen3-4b-4k:latest')  # qwen3-4b-4k:latest   qwen3:4b-tuned-4k
    overrides.append('+model.base_url_teacher=http://localhost:11434')
    overrides.append('+model.temperature_teacher=0')
    overrides.append('+model.max_tokens_teacher=2000')

    ''' 
    -----------------------------------
    ------------ è§£é¢˜æ ‘é…ç½® -------------
    -----------------------------------
    '''
    overrides.append('+agent.use_solution_tree=false')                 # å¯ç”¨è§£é¢˜æ ‘
    overrides.append('+agent.evaluate_multi_solution=false')           # è¯„ä¼°ä¸€é¢˜å¤šè§£èƒ½åŠ›
    overrides.append('+model.api_name_expert=deepseek-chat')
    overrides.append('+model.model_type_expert=api')
    # overrides.append('+model.model_type_expert=local')
    # overrides.append('+model.api_name_expert=qwen3-4b-4k:latest')  # qwen3-4b-4k:latest   qwen3:4b-tuned-4k
    overrides.append('+model.base_url_expert=http://localhost:11434')
    overrides.append('+model.temperature_expert=0')
    overrides.append('+model.max_tokens_expert=2000')

    ''' 
    -----------------------------------
    ----------- å¯¹è¯æ‘˜è¦é…ç½® ------------
    -----------------------------------
    '''
    overrides.append('+agent.conversation_summary=false')  # å¯ç”¨å¯¹è¯æ‘˜è¦
    overrides.append('+agent.summary_model_name=deepseek-chat')
    overrides.append('+agent.summary_model_type=api')
    # overrides.append('+agent.summary_model_type=local')
    # overrides.append('+agent.summary_model_name=qwen3-4b-4k:latest')  # qwen3-4b-4k:latest   qwen3:4b-tuned-4k
    overrides.append('+agent.summary_base_url=http://localhost:11434')
    overrides.append('+model.temperature_summary_model=0')
    overrides.append('+model.max_tokens_summary_model=2000')
    overrides.append('+agent.summary_max_turns=8')  # æ¯8è½®å¯¹è¯ç”Ÿæˆæ‘˜è¦
    overrides.append('+agent.summary_max_tokens=1500')  # æœ€å¤§tokené™åˆ¶
    overrides.append('+agent.explicit_interaction=true')  # true=æ˜¾å¼äº¤äº’, false=å·¥å…·è°ƒç”¨

    ''' 
    -----------------------------------
    ---------- Agentä¸ªæ€§åŒ–é…ç½® ----------
    -----------------------------------
    '''
    overrides.append('+agent.parallel_thinking=false')                 # å­¦ç”Ÿå¹¶è¡Œæ€è€ƒèƒ½åŠ›
    overrides.append('+agent.socratic_teaching=false')                 # æ•™å¸ˆè‹æ ¼æ‹‰åº•æ•™å­¦
    overrides.append('+agent.student_sys_prompt=STUDENT_PROMPT_EASY')  # é€‰æ‹©å­¦ç”Ÿç³»ç»Ÿæç¤ºè¯
    overrides.append('+agent.teacher_sys_prompt=TEACHER_PROMPT_EASY')  # é€‰æ‹©æ•™å¸ˆç³»ç»Ÿæç¤ºè¯
    overrides.append('+agent.teacher_tree_sys_prompt=TEACHER_WITH_TREE_PROMPT')  # é€‰æ‹©è§£é¢˜æ ‘ç³»ç»Ÿæç¤ºè¯
    overrides.append('+agent.student_tree_sys_prompt=STUDENT_WITH_TREE_PROMPT')  # é€‰æ‹©è§£é¢˜æ ‘ç³»ç»Ÿæç¤ºè¯
    overrides.append('+agent.expert_sys_prompt=EXPERT_STUDENT_PROMPT')  # é€‰æ‹©æ•™å¸ˆç³»ç»Ÿæç¤ºè¯
    overrides.append('+agent.use_cognitive_state=true')               # å­¦ç”Ÿè®¤çŸ¥çŠ¶æ€é…ç½® å¯ç”¨è®¤çŸ¥çŠ¶æ€
    overrides.append('+agent.carelessness_level=9')                    # ç²—å¿ƒç¨‹åº¦ (1-10)
    overrides.append('+agent.math_background=beginner')                    # æ•°å­¦èƒŒæ™¯
    overrides.append('+agent.response_style=brief')               # å›å¤é£æ ¼
    overrides.append('+agent.preferred_method=algebraic')              # åå¥½æ–¹æ³•
    overrides.append('+agent.learning_style=reading-writing')                   # å­¦ä¹ é£æ ¼

    config = hydra.compose(config_name="sft_trainer", overrides=overrides)
    get_logger(config)
    logger.info(OmegaConf.to_yaml(config))
    main(config, logger)