from typing import Dict, Any, Optional
from types import SimpleNamespace
from langchain.agents import create_agent
from langchain.chat_models import init_chat_model
from langchain_ollama import ChatOllama

from v6.utils.evaluator import logger, extract_answer
from v6.utils.MARIO_EVAL.demo import is_equiv_MATH
from v6.function.memory import SmartSummaryMemory, SummaryConfig
from v6.function.memory_fun import simple_checkpointer
from v6.function.solution_tree import SolutionTree
from v6.function.cognitive import StudentCognitiveState
from v6.function.middleware import CustomMiddleware, ModelConfig, MiddlewareFunc, wrap_model_call, ModelRequest, ModelResponse
from v6.function.contex_fun import Context
from v6.function.format_fun import ResponseFormat
from v6.function.record_eval import DialogueRecord
from v6.prompt.dialogue_tree_parallel import PARALLEL_THINKING_PROMPT, STUDENT_STANDARD_PARALLEL_THINKING_PROMPT, TREE_GENERATE_PROMPT
from v6.prompt.dialogue_socratic import SOCRATIC_TEACHING_PROMPT, BASIC_RESPONSE_TEACHING, SOCRATIC_RESPONSE_TEACHING
from v6.prompt.system import TEACHER_PROMPT, TEACHER_PROMPT_EASY, STUDENT_PROMPT, STUDENT_PROMPT_EASY


class SimpleLocalAgent:
    """ç®€åŒ–ç‰ˆæœ¬åœ°Agentï¼Œæ”¯æŒæ ¸å¿ƒåŠŸèƒ½"""

    def __init__(self, agent_type="student", debug_mode=False):
        self.agent_type = agent_type
        self.model = None
        self.system_prompt = ""
        self.tools = []
        self.context = {}
        self.dialogue_history = []
        self.debug_mode = debug_mode
        self.last_full_prompt = ""  # è®°å½•æœ€åä¸€æ¬¡å®Œæ•´prompt

    def init_model(self, model_config):
        """åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹"""
        self.model = LocalModelWrapper(model_config)

    def set_system_prompt(self, prompt):
        """è®¾ç½®ç³»ç»Ÿæç¤ºè¯"""
        self.system_prompt = prompt

    def add_tools(self, tools_list):
        """æ·»åŠ å·¥å…·ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        self.tools = tools_list

    def set_context(self, **kwargs):
        """è®¾ç½®ä¸Šä¸‹æ–‡"""
        self.context.update(kwargs)

    def invoke(self, input_dict, config=None, context=None):
        """è°ƒç”¨Agent"""
        user_input = self._extract_input(input_dict)
        full_prompt = user_input

        # æ„å»ºå®Œæ•´æç¤ºè¯
        # full_prompt = self._build_full_prompt(full_prompt)
        self.last_full_prompt = full_prompt  # ä¿å­˜å®Œæ•´prompt

        # Debugè¾“å‡º
        if self.debug_mode:
            logger.info("\n" + "=" * 80)
            logger.info(f"ğŸ” DEBUG - {self.agent_type.upper()} AGENT FULL PROMPT:")
            logger.info("=" * 80)
            logger.info(full_prompt)
            logger.info("=" * 80 + "\n")

        # è°ƒç”¨æ¨¡å‹
        response = self.model.invoke(full_prompt)

        # è®°å½•å¯¹è¯å†å²
        self.dialogue_history.append(("user", user_input))
        self.dialogue_history.append(("assistant", response['structured_response'].main_response))

        return response

    def get_last_prompt(self):
        """è·å–æœ€åä¸€æ¬¡çš„å®Œæ•´prompt"""
        return self.last_full_prompt

    def _extract_input(self, input_dict):
        """æå–è¾“å…¥æ–‡æœ¬"""
        messages = input_dict.get('messages', [])
        for msg in reversed(messages):
            if hasattr(msg, 'content'):
                return msg.content
            elif isinstance(msg, dict) and 'content' in msg:
                return msg['content']
        return ""

    def _build_full_prompt(self, user_input):
        """æ„å»ºå®Œæ•´æç¤ºè¯"""
        prompt_parts = []

        # ç³»ç»Ÿæç¤ºè¯
        if self.system_prompt:
            prompt_parts.append(f"user: {self.system_prompt}")  # dddd

        # ä¸Šä¸‹æ–‡ä¿¡æ¯
        if self.context:
            context_str = ", ".join([f"{k}: {v}" for k, v in self.context.items()])
            prompt_parts.append(f"Context: {context_str}")

        # å¯¹è¯å†å²
        if self.dialogue_history:
            d_list = []
            for role, content in self.dialogue_history[-6:]:  # æœ€è¿‘3è½®å¯¹è¯   # dddd
                if role == self.agent_type:
                    d_list.append(f"assistant\n{content}")
                else:
                    d_list.append(f"user\n{content}")
            history_str = "\n".join(d_list)
            prompt_parts.append(f"\n{history_str}")

        # å½“å‰è¾“å…¥
        prompt_parts.append(f"\nuser\n{user_input}")
        prompt_parts.append("<im_end>\n<im_start>assistant")

        return "\n\n".join(prompt_parts)
    

class SimpleAgent(object):
    """å¢å¼ºçš„Agentç±»ï¼Œæ”¯æŒå¤šæ¨¡å¼å’Œæœ¬åœ°è°ƒç”¨"""

    def __init__(self, agent_type: str = "student", debug_mode: bool = False):
        """åˆå§‹åŒ–agent

        Args:
            agent_type: "student" æˆ– "teacher"
        """
        self.agent_type = agent_type
        self.prompt_sys = None
        self.tools = []
        self.context_schema = None
        self.response_format = None
        self.checkpointer = None
        self.middleware_list = []

        self.agent_config = None
        self.response = None
        self.context = None

        self.middleware_er = None
        self.custom_middleware_er = CustomMiddleware
        self.model = None
        self.agent = None

        # å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé…ç½®
        self.dialogue_history = []
        self.max_turns = 5
        self.current_turn = 0
        self.correct_answer = None
        self.student_answer = None

        # åŠŸèƒ½å¼€å…³
        self.parallel_thinking_enabled = False
        self.socratic_teaching_enabled = False
        self.math_background_level = False
        self.debug_mode = debug_mode

        # Memory ä¸ Summary
        self.local_agent = None
        self.memory = None  # æ·»åŠ å†…å­˜ç®¡ç†
        self.summary_config = None
        self.summary_llm = None  # ä¸“é—¨çš„æ‘˜è¦LLM

        # è§£é¢˜æ ‘
        self.cognitive_state = None  # å­¦ç”Ÿè®¤çŸ¥çŠ¶æ€
        self.solution_tree = None    # å½“å‰è§£é¢˜æ ‘
        self.use_cognitive_state = False
        self.use_solution_tree = False

    def set_cognitive_state(self, cognitive_state: StudentCognitiveState):
        """è®¾ç½®è®¤çŸ¥çŠ¶æ€"""
        self.cognitive_state = cognitive_state
        self.use_cognitive_state = True

    def set_solution_tree(self, solution_tree: SolutionTree):
        """è®¾ç½®è§£é¢˜æ ‘"""
        self.solution_tree = solution_tree
        self.use_solution_tree = True

    def _build_full_prompt(self, user_input, memory_context, prompt_type='api'):
        """æ„å»ºå®Œæ•´æç¤ºè¯ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if self.agent_type == "student":
            if prompt_type == 'local':
                prompt_parts = ['<|im_start|>\n']
            else:
                prompt_parts = []

            # ç³»ç»Ÿæç¤ºè¯
            if self.prompt_sys and self.agent_type != 'student':
                # å¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„ç³»ç»Ÿæç¤ºè¯
                if isinstance(self.prompt_sys, str):
                    # å¦‚æœæ˜¯é¢„å®šä¹‰çš„æç¤ºè¯å˜é‡åï¼Œå°è¯•è·å–å…¶å€¼
                    if self.prompt_sys in globals():
                        system_prompt = globals()[self.prompt_sys]
                    else:
                        system_prompt = self.prompt_sys
                else:
                    system_prompt = str(self.prompt_sys)
                prompt_parts.append(f"{system_prompt}\n\n")

            # è®¤çŸ¥çŠ¶æ€ï¼ˆå¦‚æœæ˜¯å­¦ç”Ÿä¸”å¯ç”¨ï¼‰
            if (self.agent_type == "student" and self.use_cognitive_state and
                    self.cognitive_state and hasattr(self.cognitive_state, 'get_prompt_context')):
                cognitive_context = self.cognitive_state.get_prompt_context()
                prompt_parts.append(f"Student Profile: {cognitive_context}")

            # è§£é¢˜æ ‘ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.use_solution_tree and self.solution_tree:
                try:
                    tree_context = self._build_solution_tree_context()
                    if tree_context:
                        prompt_parts.append(f"Solution Context: {tree_context}")
                except Exception as e:
                    if self.debug_mode:
                        logger.info(f"âŒ è§£é¢˜æ ‘ä¸Šä¸‹æ–‡æ„å»ºå¤±è´¥: {e}")

            # å†…å­˜ä¸Šä¸‹æ–‡
            if memory_context:
                prompt_parts.append(f"Conversation Context: {memory_context}")

            # å¯¹è¯å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
            if self.dialogue_history:
                if self.dialogue_history[-1][1] == user_input:
                    dialogue_history = self.dialogue_history[:-1]
                else:
                    dialogue_history = self.dialogue_history
                d_list = []
                for role, content in dialogue_history[-6:]:  # æœ€è¿‘3è½®å¯¹è¯   # dddd
                    if role == self.agent_type:
                        d_list.append(f"assistant\n{content}\n\n")
                    else:
                        d_list.append(f"user\n{content}\n\n")
                history_str = "\n".join(d_list)
                prompt_parts.append(f"\n{history_str}")

            # å½“å‰è¾“å…¥
            if user_input and len(user_input):
                prompt_parts.append(f"\nuser\n{user_input}\n")
            if prompt_type == 'local':
                prompt_parts.append("<|im_end|>\n<|im_start|>assistant")

            return "\n\n".join(prompt_parts)
        elif self.agent_type == "teacher":
            return user_input
        else:
            raise NotImplementedError

    def _build_solution_tree_context(self):
        """æ„å»ºè§£é¢˜æ ‘ä¸Šä¸‹æ–‡"""
        if not self.solution_tree:
            return ""

        context_parts = []

        try:
            if self.agent_type == "teacher":
                try:
                    # æ•™å¸ˆçœ‹åˆ°ä¸“å®¶è·¯å¾„å’Œå­¦ç”Ÿè·¯å¾„çš„æ¯”è¾ƒ
                    if hasattr(self.solution_tree, 'compare_with_expert'):
                        comparison = self.solution_tree.compare_with_expert()
                        if comparison and comparison.get("closest_expert_path"):
                            expert_method = comparison['closest_expert_path'].get('method', 'unknown')
                            similarity = comparison.get('similarity', 0)
                            context_parts.append(
                                f"Expert solution available using {expert_method} method")
                            context_parts.append(f"Similarity to student approach: {similarity:.2f}")
                except Exception as e:
                    if self.debug_mode:
                        print(f"âš ï¸ è§£é¢˜æ ‘æ¯”è¾ƒå¤±è´¥: {e}")
                    context_parts.append("Expert guidance available for this problem")

            elif self.agent_type == "student":
                # å­¦ç”Ÿçœ‹åˆ°è‡ªå·±çš„è¿›åº¦
                if (hasattr(self.solution_tree, 'current_student_path') and
                    self.solution_tree.current_student_path):
                    context_parts.append(
                        f"Your current solution path has {len(self.solution_tree.current_student_path)} steps")
                    context_parts.append("Continue your approach or try a different method if stuck")

                # æ·»åŠ å¯ç”¨çš„ä¸“å®¶è·¯å¾„ä¿¡æ¯
            if (hasattr(self.solution_tree, 'solution_paths') and
                    self.solution_tree.solution_paths):
                expert_paths = [p for p in self.solution_tree.solution_paths
                                if hasattr(p, 'get') and p.get("type") == "expert"]
                if expert_paths:
                    methods = set(p.get("method", "unknown") for p in expert_paths)
                    context_parts.append(f"Available expert methods: {', '.join(methods)}")

        except Exception as e:
            if self.debug_mode:
                logger.info(f"âŒ è§£é¢˜æ ‘ä¸Šä¸‹æ–‡æ„å»ºå¼‚å¸¸: {e}")
            # æä¾›åŸºæœ¬çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
            context_parts.append("Solution guidance is available for this problem")

        return "\n".join(context_parts) if context_parts else ""

    def _parse_solution_tree_student(self, response, problem):
        """è§£æå­¦ç”Ÿçš„è§£é¢˜æ ‘å“åº”"""
        solution_tree = SolutionTree(problem)

        try:
            # ç®€å•çš„è§£æé€»è¾‘ - åœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„è§£æ
            if "<SolutionTree>" in response and self.agent_type == "student":
                # æå–è§£å†³æ–¹æ¡ˆè·¯å¾„
                paths_section = response.split("<SolutionPaths>")[1].split("</SolutionPaths>")[0]
                path_blocks = paths_section.split("</Path>")

                for block in path_blocks:
                    if "<Path" in block:
                        # æå–è·¯å¾„ä¿¡æ¯
                        method = self._extract_site_tag(block, "method")
                        # æå–æ­¥éª¤
                        steps = []
                        intermediate_answers = []
                        step_parts = block.split("<Step")
                        for step_part in step_parts[1:]:
                            if ">" in step_part and "</Step>" in step_part:
                                step_content = step_part.split(">", 1)[1].split("</Step>")[0]
                                steps.append(step_content)
                            if "<IntermediateAnswer>" in step_part and "</IntermediateAnswer>" in step_part:
                                intermediate_content = step_part.split("<IntermediateAnswer>", 1)[1].split("</IntermediateAnswer>")[0]
                                intermediate_answers.append(intermediate_content)

                        # æå–æœ€ç»ˆç­”æ¡ˆ
                        final_answer = self._extract_xml_tag(block, "FinalAnswer")

                        solution_tree.add_expert_path({
                            "method": method,
                            "steps": steps,
                            "intermediate_answers": intermediate_answers,
                            "final_answer": final_answer
                        })

        except Exception as e:
            print(f"âŒ Error parsing solution tree: {e}")
            # å¦‚æœè§£æå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„è§£å†³æ–¹æ¡ˆè·¯å¾„
            solution_tree.add_expert_path({
                "method": "algebraic",
                "steps": ["Apply standard algebraic approach", "Solve step by step"],
                "final_answer": "[[Answer will be determined]]"
            })

        return solution_tree

    def _extract_xml_tag(self, text, tag_name):
        """æå–XMLæ ‡ç­¾å†…å®¹"""
        start_tag = f"<{tag_name}>"
        end_tag = f"</{tag_name}>"

        if start_tag in text and end_tag in text:
            return text.split(start_tag)[1].split(end_tag)[0].strip()
        return ""

    def _extract_site_tag(self, text, tag_name):
        """æå–XMLæ ‡ç­¾å†…å®¹"""
        start_tag = f'{tag_name}="'
        end_tag = f'"'

        if start_tag in text and end_tag in text:
            return text.split(start_tag)[1].split(end_tag)[0].strip()
        return ""

    def record_student_step(self, step_content, method_used=None):
        """è®°å½•å­¦ç”Ÿè§£é¢˜æ­¥éª¤"""
        if (self.agent_type == "student" and self.use_solution_tree and
                self.solution_tree and hasattr(self.solution_tree, 'add_student_step')):
            try:
                self.solution_tree.add_student_step(step_content, method_used)

                # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                if hasattr(self, 'debug_mode') and self.debug_mode:
                    detected_method = method_used if method_used else self.solution_tree._detect_student_method(
                        step_content)
                    print(f"ğŸ“ è®°å½•è§£é¢˜æ­¥éª¤: æ–¹æ³•={detected_method}, å†…å®¹é•¿åº¦={len(step_content)}")
            except Exception as e:
                if self.debug_mode:
                    print(f"âŒ è®°å½•è§£é¢˜æ­¥éª¤å¤±è´¥: {e}")


    def complete_student_solution(self, success, final_answer=None):
        """å®Œæˆå­¦ç”Ÿè§£é¢˜"""
        if (self.agent_type == "student" and self.use_solution_tree and
                self.solution_tree and hasattr(self.solution_tree, 'complete_student_path')):
            result = self.solution_tree.complete_student_path(success, final_answer)

            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            if hasattr(self, 'debug_mode') and self.debug_mode:
                print(f"ğŸŒ³ è§£é¢˜æ ‘å®Œæˆ: æˆåŠŸ={success}, ç­”æ¡ˆ={final_answer}")
                if hasattr(self.solution_tree, 'current_student_path'):
                    print(f"   è·¯å¾„æ­¥éª¤æ•°: {len(self.solution_tree.current_student_path)}")
                if hasattr(self.solution_tree, 'solution_paths'):
                    print(f"   æ€»è§£å†³æ–¹æ¡ˆè·¯å¾„: {len(self.solution_tree.solution_paths)}")

            return result
        return None

    def enable_conversation_summary(self, summary_config: SummaryConfig, summary_llm=None):
        """å¯ç”¨å¯¹è¯æ‘˜è¦åŠŸèƒ½"""
        self.summary_config = summary_config
        self.summary_llm = summary_llm

        self.memory = SmartSummaryMemory(
            llm=summary_llm,
            max_turns=summary_config.max_turns,
            max_token_limit=summary_config.max_token_limit,
            enabled=summary_config.enabled
        )

        if hasattr(self.memory, 'debug_mode'):
            self.memory.debug_mode = hasattr(self, 'debug_mode') and self.debug_mode

        status = "enabled" if summary_config.enabled else "disabled"
        logger.info(f"âœ… Conversation summary {status} for {self.agent_type} agent")

    def model_init(self, model_config: ModelConfig, model_name=None):
        """åˆå§‹åŒ–æ¨¡å‹ï¼Œæ”¯æŒAPIå’Œæœ¬åœ°è°ƒç”¨"""
        if model_config.model_type == "local":
            # ä½¿ç”¨ç®€åŒ–çš„æœ¬åœ°Agent
            self.local_agent = SimpleLocalAgent(self.agent_type, self.debug_mode)
            self.local_agent.init_model(model_config)
            print(f"âœ… åˆå§‹åŒ– {model_name} æœ¬åœ°æ¨¡å‹: {model_config.model_name}")
        else:
            # APIæ¨¡å‹é…ç½®
            api_kwargs = {
                "temperature": model_config.temperature,
                "timeout": model_config.timeout,
                "max_tokens": model_config.max_tokens,
            }
            self.model = init_chat_model(model_config.model_name, **api_kwargs)

            # ä¸ºAPIæ¨¡å‹æ·»åŠ debugä¸­é—´ä»¶
            if self.debug_mode:
                self._add_debug_middleware()

            print(f"âœ… åˆå§‹åŒ– {model_name} APIæ¨¡å‹: {model_config.model_name}")
            self.middleware_er = MiddlewareFunc(self.model, self.model)

    def _add_debug_middleware(self):
        """ä¸ºAPIæ¨¡å‹æ·»åŠ debugä¸­é—´ä»¶"""

        @wrap_model_call
        def debug_middleware(request: ModelRequest, handler) -> ModelResponse:
            print("\n" + "=" * 80)
            print(f"ğŸ” DEBUG - {self.agent_type.upper()} AGENT API REQUEST:")
            print("=" * 80)

            # æ‰“å°ç³»ç»Ÿæç¤ºè¯
            if hasattr(request, 'system_prompt') and request.system_prompt:
                print("System Prompt:")
                print(request.system_prompt)
                print("-" * 40)

            # æ‰“å°æ¶ˆæ¯å†å²
            messages = request.state.get("messages", [])
            print("Message History:")
            for i, msg in enumerate(messages):
                role = getattr(msg, 'role', type(msg).__name__)
                content = getattr(msg, 'content', str(msg))
                print(f"{i}. {role}: {content}")

            print("=" * 80 + "\n")

            return handler(request)

        # æ·»åŠ åˆ°ä¸­é—´ä»¶åˆ—è¡¨
        if not hasattr(self, 'middleware_list'):
            self.middleware_list = []
        self.middleware_list.append(debug_middleware)

    def get_debug_info(self):
        """è·å–debugä¿¡æ¯"""
        if hasattr(self, 'local_agent') and self.local_agent:
            return {
                "agent_type": self.agent_type,
                "last_prompt": self.local_agent.get_last_prompt(),
                "dialogue_history": self.local_agent.dialogue_history
            }
        else:
            return {
                "agent_type": self.agent_type,
                "debug_mode": self.debug_mode
            }

    def set_correct_answer(self, correct_answer: str):
        """è®¾ç½®æ­£ç¡®ç­”æ¡ˆï¼ˆæ•™å¸ˆagentä½¿ç”¨ï¼‰"""
        self.correct_answer = correct_answer

    def agent_init(self, model_config, prompt_sys_name=None, tools_list=None, context_schema=Context,
                   response_format=ResponseFormat, checkpointer=simple_checkpointer,
                   middleware=None, **kwargs):
        """åˆå§‹åŒ–agenté…ç½®"""
        # åœ¨è§£é¢˜æ ‘æ¨¡å¼ä¸‹ï¼Œç¦ç”¨å¯èƒ½å¯¼è‡´å†²çªçš„å·¥å…·
        if self.use_solution_tree:
            # ä¿ç•™å¿…è¦çš„å·¥å…·ï¼Œç§»é™¤å¯èƒ½å¯¼è‡´å·¥å…·è°ƒç”¨çš„å¤æ‚å·¥å…·
            safe_tools = []
            if tools_list:
                for tool in tools_list:
                    tool_name = getattr(tool, 'name', str(tool))
                    # åªä¿ç•™ç®€å•ã€å®‰å…¨çš„å·¥å…·
                    if any(safe in tool_name.lower() for safe in ['format', 'memory', 'context']):
                        safe_tools.append(tool)
            self.tools = safe_tools
        else:
            self.tools = tools_list if tools_list else []

        self.context_schema = context_schema
        self.response_format = response_format
        self.checkpointer = checkpointer

        # è®¾ç½®åŠŸèƒ½å¼€å…³
        self.max_turns = kwargs.get("max_turns", 5)
        self.parallel_thinking_enabled = kwargs.get("parallel_thinking", False)
        self.socratic_teaching_enabled = kwargs.get("socratic_teaching", False)

        self.solution_tree = kwargs.get("prompt_solution_tree", False)
        solution_tree_prompt = kwargs.get("prompt_solution_tree", None)
        if solution_tree_prompt:
            self.prompt_sys = solution_tree_prompt
            self.use_solution_tree = True
        else:
            self.prompt_sys = prompt_sys_name
            self.use_solution_tree = False

        # æ ¹æ®åŠŸèƒ½å¼€å…³è°ƒæ•´ç³»ç»Ÿæç¤ºè¯
        self._adjust_prompt_based_on_settings(self.prompt_sys)

        # åœ¨è§£é¢˜æ ‘æ¨¡å¼ä¸‹ï¼Œç®€åŒ–ä¸­é—´ä»¶é…ç½®
        if middleware and self.use_solution_tree:
            # ç§»é™¤å¯èƒ½å¯¼è‡´å·¥å…·è°ƒç”¨å†²çªçš„ä¸­é—´ä»¶
            safe_middleware = []
            for m in middleware:
                if m not in ['handle_tool_errors', 'dynamic']:
                    safe_middleware.append(m)
            middleware = safe_middleware

        if middleware:
            for m in middleware:
                if m == 'dynamic':
                    self.middleware_list.append(self.middleware_er.middleware_dynamic_model_selection())
                elif m == 'handle_tool_errors':
                    self.middleware_list.append(self.middleware_er.middleware_handle_tool_errors())
                elif m == 'user_role_prompt':
                    self.middleware_list.append(self.middleware_er.middleware_user_role_prompt())
                elif m == 'CustomMiddleware':
                    self.middleware_list.append(self.custom_middleware_er())

        if model_config.model_type == "local":
            # æœ¬åœ°æ¨¡å‹åˆå§‹åŒ–
            self.local_agent.set_system_prompt(self.prompt_sys)
            if self.tools:
                self.local_agent.add_tools(self.tools)
            if self.context:
                self.local_agent.set_context(**self.context.__dict__)
        else:
            # å¯¹äºAPIæ¨¡å‹ï¼Œåœ¨è§£é¢˜æ ‘æ¨¡å¼ä¸‹ä½¿ç”¨ç®€åŒ–çš„agentåˆ›å»ºæ–¹å¼
            if self.use_solution_tree:
                try:
                    self.agent = self._create_simple_agent()
                    if self.debug_mode:
                        print("âœ… ä½¿ç”¨ç®€åŒ–Agentï¼ˆè§£é¢˜æ ‘æ¨¡å¼ï¼‰")
                except Exception as e:
                    if self.debug_mode:
                        print(f"âš ï¸ ç®€åŒ–Agentåˆ›å»ºå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†Agent: {e}")
                    self.agent = create_agent(
                        model=self.model,
                        system_prompt=self.prompt_sys,
                        tools=self.tools,  # ä½¿ç”¨è¿‡æ»¤åçš„å·¥å…·
                        context_schema=self.context_schema,
                        response_format=self.response_format,
                        checkpointer=self.checkpointer,
                        middleware=self.middleware_list,
                    )
            else:
                self.agent = create_agent(
                    model=self.model,
                    system_prompt=self.prompt_sys,
                    tools=self.tools,  # ä½¿ç”¨è¿‡æ»¤åçš„å·¥å…·
                    context_schema=self.context_schema,
                    response_format=self.response_format,
                    checkpointer=self.checkpointer,
                    middleware=self.middleware_list,
                )

    def _create_simple_agent(self):
        """åˆ›å»ºä¸åŒ…å«å·¥å…·çš„ç®€å•å¯¹è¯agent"""
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.messages import AIMessage, HumanMessage

        # åˆ›å»ºç®€å•çš„å¯¹è¯é“¾
        prompt = ChatPromptTemplate.from_messages([
            ("system", self.prompt_sys),
            ("user", "{input}")
        ])

        # ç®€å•çš„å¯¹è¯é“¾ï¼Œä¸åŒ…å«ä»»ä½•å·¥å…·
        chain = prompt | self.model

        # åŒ…è£…æˆç±»ä¼¼agentçš„æ¥å£
        class SimpleAgentWrapper:
            def __init__(self, chain, debug_mode=False):
                self.chain = chain
                self.debug_mode = debug_mode

            def invoke(self, input_dict, config=None, context=None):
                try:
                    # æå–ç”¨æˆ·è¾“å…¥
                    messages = input_dict.get("messages", [])
                    user_input = ""
                    for msg in reversed(messages):
                        if hasattr(msg, 'content'):
                            user_input = msg.content
                            break
                        elif isinstance(msg, dict) and 'content' in msg:
                            user_input = msg['content']
                            break

                    if not user_input:
                        user_input = str(messages)
                    # è°ƒç”¨æ¨¡å‹
                    response_content = self.chain.invoke({"input": user_input})

                    # ç¡®ä¿response_contentæ˜¯å­—ç¬¦ä¸²
                    if hasattr(response_content, 'content'):
                        response_text = response_content.content
                    else:
                        response_text = str(response_content)

                    # è¿”å›ä¸ç°æœ‰ä»£ç å…¼å®¹çš„æ ¼å¼
                    return {
                        'messages': [{'role': 'assistant', 'content': response_text}],
                        'structured_response': SimpleNamespace(main_response=response_text)
                    }

                except Exception as e:
                    if self.debug_mode:
                        print(f"âŒ ç®€å•Agentè°ƒç”¨é”™è¯¯: {e}")
                    return {
                        'messages': [{'role': 'assistant', 'content': "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ã€‚è¯·é‡æ–°æé—®ã€‚"}],
                        'structured_response': SimpleNamespace(main_response="æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ã€‚è¯·é‡æ–°æé—®ã€‚")
                    }

            def stream(self, input_dict, config=None, context=None, stream_mode="values"):
                # ç®€å•çš„æµå¼å“åº”å®ç°
                result = self.invoke(input_dict, config, context)
                yield result

        return SimpleAgentWrapper(chain, self.debug_mode)

    def _adjust_prompt_based_on_settings(self, prompt_sys):
        """æ ¹æ®åŠŸèƒ½å¼€å…³è°ƒæ•´ç³»ç»Ÿæç¤ºè¯"""
        if isinstance(prompt_sys, str):
            # å¦‚æœæ˜¯é¢„å®šä¹‰çš„æç¤ºè¯å˜é‡åï¼Œå°è¯•è·å–å…¶å€¼
            if prompt_sys in globals():
                base_prompt = globals()[prompt_sys]
            else:
                base_prompt = prompt_sys
        else:
            base_prompt = str(prompt_sys)

        if self.agent_type == "student":

            if self.parallel_thinking_enabled:
                base_prompt += "\n\n" + PARALLEL_THINKING_PROMPT

            self.prompt_sys = base_prompt

        elif self.agent_type == "teacher":

            if self.correct_answer:
                base_prompt += f"\n\nYou know the correct solution is: {self.correct_answer}. But DO NOT reveal this answer directly to the student. Guide them to discover it themselves."

            if self.socratic_teaching_enabled:
                base_prompt += "\n\n" + SOCRATIC_TEACHING_PROMPT

            self.prompt_sys = base_prompt

        elif self.agent_type == "expert_student":

            self.prompt_sys = base_prompt

    def config_create(self, key_i, value_i):
        self.agent_config = {"configurable": {key_i: value_i}, "recursion_limit": 100}

    def context_set(self, **kwargs):
        self.context = self.context_schema(**kwargs)

    def chat_once(self, user_input, response_type='invoke', silence=False, **kwargs):
        if response_type == 'invoke':
            self.agent_response_invoke(user_input, **kwargs)
            if not silence:
                self.agent_output()
        else:
            self.agent_response_stream(user_input, **kwargs)

    def _extract_response_text(self, response):
        """ä»å“åº”å¯¹è±¡ä¸­æå–æ–‡æœ¬å†…å®¹"""
        if isinstance(response, str):
            return response

        if hasattr(response, 'get') and isinstance(response, dict):
            # å­—å…¸ç±»å‹çš„å“åº”
            if 'structured_response' in response and hasattr(response['structured_response'], 'main_response'):
                return response['structured_response'].main_response
            elif 'messages' in response and response['messages']:
                last_msg = response['messages'][-1]
                if hasattr(last_msg, 'content'):
                    return last_msg.content
                elif isinstance(last_msg, dict) and 'content' in last_msg:
                    return last_msg['content']
            # å°è¯•ç›´æ¥è·å–content
            elif 'content' in response:
                return response['content']

        if hasattr(response, 'structured_response') and hasattr(response.structured_response, 'main_response'):
            return response.structured_response.main_response
        elif hasattr(response, 'content'):
            return response.content

        # æœ€åå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        return str(response)

    def multi_agent_chat_explicit(self, teacher_agent, problem: str, raw_problem: str, correct_answer: str,
                                  dialogue_record: DialogueRecord, **kwargs):
        """æ¨¡å¼1: æ˜¾å¼äº¤äº’ - æ•™å¸ˆå’Œå­¦ç”Ÿç›´æ¥å¯¹è¯"""
        if dialogue_record.debug_mode:
            logger.info(f"\nğŸ¯ å¼€å§‹è§£é¢˜: {problem}")
            logger.info("=" * 50)

        # è®¾ç½®æ•™å¸ˆçŸ¥é“çš„æ­£ç¡®ç­”æ¡ˆ
        teacher_agent.set_correct_answer(correct_answer)
        teacher_agent._adjust_prompt_based_on_settings(teacher_agent.prompt_sys)  # é‡æ–°è°ƒæ•´æç¤ºè¯

        # é‡ç½®å¯¹è¯å†å²
        self.dialogue_history = [('teacher', problem)]
        self.current_turn = 0

        try:
            # å­¦ç”Ÿé¦–æ¬¡å°è¯•
            student_response_obj = self._invoke_agent('')
            # ç¡®ä¿æ­£ç¡®æå–å“åº”æ–‡æœ¬
            student_response = self._extract_response_text(student_response_obj)
            self.dialogue_history.append(("student", student_response))

            # è®°å½•å­¦ç”Ÿç¬¬ä¸€æ­¥è§£é¢˜æ­¥éª¤
            if self.use_solution_tree and self.solution_tree:
                self.record_student_step(student_response, method_used=self._detect_method_from_response(student_response))
                if dialogue_record.debug_mode:
                    logger.info("ğŸ“ å·²è®°å½•å­¦ç”Ÿç¬¬ä¸€æ¬¡è§£é¢˜æ­¥éª¤")

            # åˆ†æå­¦ç”Ÿå›å¤
            parallel_count, path_count = dialogue_record.analyze_student_response(student_response)

            # è®°å½•ç¬¬ä¸€è½®å¯¹è¯
            dialogue_record.add_turn({
                "turn": 1,
                "student_response": student_response,
                "teacher_response": "",
                "teacher_intent": "initial_response",
                "parallel_thinking_count": parallel_count,
                "thinking_paths_count": path_count,
                "answer_leakage": False
            })

            if dialogue_record.debug_mode:
                logger.info(f"ğŸ‘¨â€ğŸ“ å­¦ç”Ÿ [è½®æ¬¡1]: {student_response}")

            self.correct_answer = extract_answer(correct_answer)
            dialogue_record.first_correct = self._has_correct_answer(extract_answer(student_response), self.correct_answer)

            # å¦‚æœç¬¬ä¸€æ¬¡å°±ç­”å¯¹äº†ï¼Œç›´æ¥å®Œæˆè§£é¢˜
            if dialogue_record.first_correct:
                if self.use_solution_tree and self.solution_tree:
                    self.complete_student_solution(success=True, final_answer=student_response)
                    if dialogue_record.debug_mode:
                        logger.info("âœ… å­¦ç”Ÿç¬¬ä¸€æ¬¡å°±ç­”å¯¹äº†ï¼Œè§£é¢˜è·¯å¾„å·²å®Œæˆ")
                dialogue_record.correct = True
                dialogue_record.final_student_answer = student_response
                return student_response, self.correct_answer, dialogue_record

            for turn in range(self.max_turns):
                # æ£€æŸ¥å­¦ç”Ÿæ˜¯å¦å¾—å‡ºæ­£ç¡®ç­”æ¡ˆ
                self.student_answer = extract_answer(student_response)
                if self._has_correct_answer(self.student_answer, self.correct_answer):
                    if dialogue_record.debug_mode:
                        logger.info("ğŸ‰ å­¦ç”Ÿå¾—å‡ºæ­£ç¡®ç­”æ¡ˆ!")
                    dialogue_record.correct = True
                    dialogue_record.final_student_answer = self.student_answer

                    # å®Œæˆè§£é¢˜è·¯å¾„
                    if self.use_solution_tree and self.solution_tree:
                        self.complete_student_solution(success=True, final_answer=student_response)
                        if dialogue_record.debug_mode:
                            logger.info("âœ… å­¦ç”Ÿç­”å¯¹äº†ï¼Œè§£é¢˜è·¯å¾„å·²å®Œæˆ")

                    return self.student_answer, self.correct_answer, dialogue_record

                current_turn = turn + 2  # ä»ç¬¬äºŒè½®å¼€å§‹

                if dialogue_record.debug_mode:
                    logger.info(f"\nğŸ”„ ç¬¬ {current_turn} è½®å¯¹è¯:")
                    logger.info("-" * 30)

                # æ•™å¸ˆå›åº”
                # teacher_input = student_response
                if self.parallel_thinking_enabled and turn == 0:
                    teacher_response = STUDENT_STANDARD_PARALLEL_THINKING_PROMPT.format(question=raw_problem)
                else:
                    teacher_input = teacher_agent._format_teacher_input(self.dialogue_history)  # dddd
                    teacher_response_obj = teacher_agent._invoke_agent(teacher_input)
                    teacher_response = self._extract_response_text(teacher_response_obj)

                self.dialogue_history.append(("teacher", teacher_response))

                # æ£€æŸ¥ç­”æ¡ˆæ³„éœ²
                leakage_detected = dialogue_record.check_answer_leakage(teacher_response, self.correct_answer)

                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»“æŸå¯¹è¯
                if self._should_end_dialogue(teacher_response):
                    if dialogue_record.debug_mode:
                        logger.info("âœ… æ•™å¸ˆè®¤ä¸ºè§£é¢˜å®Œæˆ")
                    dialogue_record.final_student_answer = self.student_answer

                    # å®Œæˆè§£é¢˜è·¯å¾„ï¼ˆå¯èƒ½æˆåŠŸæˆ–å¤±è´¥ï¼‰
                    if self.use_solution_tree and self.solution_tree:
                        success = self._has_correct_answer(self.student_answer, self.correct_answer)
                        self.complete_student_solution(success=success, final_answer=student_response)
                        if dialogue_record.debug_mode:
                            status = "æˆåŠŸ" if success else "å¤±è´¥"
                            logger.info(f"ğŸ“ æ•™å¸ˆç»“æŸå¯¹è¯ï¼Œè§£é¢˜è·¯å¾„{status}")

                    return self.student_answer, self.correct_answer, dialogue_record

                if dialogue_record.debug_mode:
                    logger.info(f"ğŸ‘¨â€ğŸ« æ•™å¸ˆ [è½®æ¬¡{current_turn}]: {teacher_response}")
                if leakage_detected:
                    if dialogue_record.debug_mode:
                        logger.info("âš ï¸  æ£€æµ‹åˆ°ç­”æ¡ˆæ³„éœ²!")

                # å­¦ç”Ÿå›åº”
                student_input = teacher_response
                student_response_obj = self._invoke_agent(student_input)
                student_response = self._extract_response_text(student_response_obj)
                self.dialogue_history.append(("student", student_response))

                # è®°å½•å­¦ç”Ÿè§£é¢˜æ­¥éª¤
                if self.use_solution_tree and self.solution_tree:
                    method_used = self._detect_method_from_response(student_response)
                    self.record_student_step(student_response, method_used=method_used)
                    if dialogue_record.debug_mode:
                        logger.info(f"ğŸ“ å·²è®°å½•å­¦ç”Ÿç¬¬{current_turn}æ­¥è§£é¢˜æ­¥éª¤ï¼Œæ–¹æ³•: {method_used}")

                # åˆ†æå­¦ç”Ÿå›å¤
                parallel_count, path_count = dialogue_record.analyze_student_response(student_response)

                # è®°å½•æœ¬è½®å¯¹è¯
                dialogue_record.add_turn({
                    "answer": self.correct_answer,
                    "answer_response": correct_answer,
                    "turn": current_turn,
                    "student_response": student_response,
                    "teacher_response": teacher_response,
                    "teacher_intent": self._analyze_teacher_intent(teacher_response),
                    "parallel_thinking_count": parallel_count,
                    "thinking_paths_count": path_count,
                    "answer_leakage": leakage_detected
                })

                if dialogue_record.debug_mode:
                    logger.info(f"ğŸ‘¨â€ğŸ“ å­¦ç”Ÿ [è½®æ¬¡{current_turn}]: {student_response}")

                self.current_turn = current_turn

            # è®¾ç½®æœ€ç»ˆç­”æ¡ˆ
            dialogue_record.final_student_answer = self.student_answer

            # å®Œæˆè§£é¢˜è·¯å¾„ï¼ˆè¾¾åˆ°æœ€å¤§è½®æ¬¡ï¼‰
            if self.use_solution_tree and self.solution_tree:
                success = self._has_correct_answer(self.student_answer, self.correct_answer)
                self.complete_student_solution(success=success, final_answer=student_response)
                if dialogue_record.debug_mode:
                    status = "æˆåŠŸ" if success else "å¤±è´¥"
                    logger.info(f"ğŸ“ è¾¾åˆ°æœ€å¤§è½®æ¬¡ï¼Œè§£é¢˜è·¯å¾„{status}")

            return self._get_final_answer(), self.correct_answer, dialogue_record

        except Exception as e:
            if dialogue_record.debug_mode:
                logger.error(f"âŒ å¯¹è¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            # åœ¨å‡ºé”™æ—¶ä¹Ÿè¦å®Œæˆè§£é¢˜è·¯å¾„
            if self.use_solution_tree and self.solution_tree:
                self.complete_student_solution(success=False, final_answer="é”™è¯¯")
                if dialogue_record.debug_mode:
                    logger.info("ğŸ“ å¯¹è¯å‡ºé”™ï¼Œè§£é¢˜è·¯å¾„æ ‡è®°ä¸ºå¤±è´¥")

            # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„é”™è¯¯å“åº”
            dialogue_record.final_student_answer = "æŠ±æ­‰ï¼Œè§£é¢˜è¿‡ç¨‹ä¸­å‡ºç°äº†æŠ€æœ¯é—®é¢˜ã€‚"
            return "æŠ±æ­‰ï¼Œè§£é¢˜è¿‡ç¨‹ä¸­å‡ºç°äº†æŠ€æœ¯é—®é¢˜ã€‚", self.correct_answer, dialogue_record

    def _analyze_teacher_intent(self, teacher_response: str) -> str:
        """åˆ†ææ•™å¸ˆå›å¤çš„æ„å›¾"""
        response_lower = teacher_response.lower()

        if any(word in response_lower for word in ["question", "ask", "what do you think"]):
            return "socratic_questioning"
        elif any(word in response_lower for word in ["hint", "suggest", "try"]):
            return "providing_hint"
        elif any(word in response_lower for word in ["correct", "right", "good"]):
            return "positive_feedback"
        elif any(word in response_lower for word in ["wrong", "incorrect", "mistake"]):
            return "correcting_error"
        elif any(word in response_lower for word in ["explain", "concept", "principle"]):
            return "explaining_concept"
        else:
            return "general_guidance"

    def _detect_method_from_response(self, response: str) -> str:
        """ä»å­¦ç”Ÿå“åº”ä¸­æ£€æµ‹ä½¿ç”¨çš„æ–¹æ³•"""
        if not response:
            return "unknown"

        response_lower = response.lower()

        # æ£€æµ‹æ–¹æ³•ç±»å‹
        if any(word in response_lower for word in ["equation", "solve for", "variable", "x =", "let x", "algebra"]):
            return "algebraic"
        elif any(word in response_lower for word in
                 ["diagram", "graph", "shape", "angle", "area", "triangle", "circle"]):
            return "geometric"
        elif any(word in response_lower for word in
                 ["calculate", "compute", "number", "digit", "sum", "total", "multiply"]):
            return "computational"
        elif any(word in response_lower for word in ["logic", "reason", "therefore", "because", "since", "if then"]):
            return "logical"
        elif any(word in response_lower for word in ["guess", "try", "maybe", "perhaps", "i think"]):
            return "trial_and_error"
        else:
            return "general"

    def multi_agent_chat_tool_based(self, problem: str, correct_answer: str,
                                    dialogue_record: DialogueRecord, **kwargs):
        """æ¨¡å¼2: å·¥å…·è°ƒç”¨ - å­¦ç”Ÿä½œä¸ºcontrollerè°ƒç”¨æ•™å¸ˆå·¥å…·"""
        print(f"\nğŸ¯ å¼€å§‹å·¥å…·è°ƒç”¨æ¨¡å¼è§£é¢˜: {problem}")
        print("=" * 50)

        # é…ç½®å­¦ç”Ÿagentä»¥åŒ…å«æ•™å¸ˆå·¥å…·ï¼ˆåŒ…å«æ­£ç¡®ç­”æ¡ˆï¼‰
        teacher_tool = self._create_teacher_tool(correct_answer)
        self.tools.append(teacher_tool)

        # é‡æ–°åˆå§‹åŒ–agentä»¥åŒ…å«æ–°å·¥å…·
        self.agent = create_agent(
            model=self.model,
            system_prompt=self.prompt_sys + "\n\nYou can use the ask_teacher tool when you need guidance.",
            tools=self.tools,
            context_schema=self.context_schema,
            response_format=self.response_format,
            checkpointer=self.checkpointer,
            middleware=self.middleware_list,
        )

        # å­¦ç”Ÿè‡ªä¸»è§£é¢˜ï¼Œå¯åœ¨éœ€è¦æ—¶è°ƒç”¨æ•™å¸ˆå·¥å…·
        final_response = self._invoke_agent(problem)

        # åˆ†æå­¦ç”Ÿå›å¤
        parallel_count, path_count = dialogue_record.analyze_student_response(final_response)

        # è®°å½•å·¥å…·è°ƒç”¨æ¨¡å¼çš„å¯¹è¯ï¼ˆç®€åŒ–ä¸ºå•è½®ï¼‰
        dialogue_record.add_turn({
            "turn": 1,
            "student_response": final_response,
            "teacher_response": "Tool-based interaction",
            "teacher_intent": "tool_guidance",
            "parallel_thinking_count": parallel_count,
            "thinking_paths_count": path_count,
            "answer_leakage": False
        })

        dialogue_record.final_student_answer = final_response

        print(f"ğŸ‘¨â€ğŸ“ å­¦ç”Ÿæœ€ç»ˆå›ç­”: {final_response}")

        return final_response

    def _create_teacher_tool(self, correct_answer: str):
        """åˆ›å»ºæ•™å¸ˆå·¥å…·ä¾›å­¦ç”Ÿè°ƒç”¨ï¼ˆåŒ…å«æ­£ç¡®ç­”æ¡ˆçŸ¥è¯†ï¼‰"""
        from langchain.tools import tool

        @tool
        def ask_teacher(question: str) -> str:
            """Ask the teacher for guidance on a specific question or problem.

            The teacher knows the correct answer but will not reveal it directly.
            Instead, the teacher will provide helpful guidance and hints.

            Use this tool when:
            - You're stuck on a math problem
            - You need clarification on concepts
            - You want to check your approach
            - You need step-by-step guidance
            """
            # åŸºäºæ­£ç¡®ç­”æ¡ˆæä¾›å¼•å¯¼æ€§æç¤º
            guidance_responses = [
                "Let me guide you through this step by step. What part are you finding difficult?",
                "Good attempt! Let's break this down. What's your current approach?",
                "I see where you might be confused. Let me ask you a question to help you think differently...",
                "Remember the key concept here is to identify the known values and what you're solving for.",
                "Try breaking the problem into smaller parts. What's the first step you would take?",
                "Consider what information you have and what you're trying to find. How can you connect them?",
                "That's a good start. Now think about what mathematical operations might be needed here."
            ]
            import random
            return random.choice(guidance_responses)

        return ask_teacher

    def _invoke_agent(self, input_text):
        """è°ƒç”¨agentå¹¶è¿”å›å“åº”"""
        if self.memory and self.memory.enabled:
            self.memory.add_message("user", input_text)

        if hasattr(self, 'local_agent') and self.local_agent:
            response = self._invoke_local_with_memory(input_text)
        else:
            response = self._invoke_api_with_memory(input_text)

        # è®°å½•åŠ©æ‰‹å“åº”åˆ°å†…å­˜
        if self.memory and self.memory.enabled and response:
            self.memory.add_message("assistant", response)

        return response

    def _invoke_local_with_memory(self, input_text):
        """æœ¬åœ°æ¨¡å‹è°ƒç”¨ï¼ˆæ”¯æŒå†…å­˜å’Œè§£é¢˜æ ‘ï¼‰"""
        # æ„å»ºåŒ…å«å†…å­˜ä¸Šä¸‹æ–‡å’Œè§£é¢˜æ ‘ä¸Šä¸‹æ–‡çš„è¾“å…¥
        memory_context = ""
        if self.memory and self.memory.enabled:
            memory_context = self.memory.get_context()

        # ä½¿ç”¨å¢å¼ºçš„å®Œæ•´æç¤ºè¯æ„å»ºæ–¹æ³•
        full_input = self._build_full_prompt(input_text, memory_context, 'local')

        response = self.local_agent.invoke({
            "messages": [{"role": "user", "content": full_input}]
        })
        return response['structured_response'].main_response

    def _invoke_api_with_memory(self, input_text):
        """APIæ¨¡å‹è°ƒç”¨ï¼ˆæ”¯æŒå†…å­˜å’Œè§£é¢˜æ ‘ï¼‰"""
        memory_context = ""
        if self.memory and self.memory.enabled:
            memory_context = self.memory.get_context()

        # ä½¿ç”¨å¢å¼ºçš„å®Œæ•´æç¤ºè¯æ„å»ºæ–¹æ³•
        full_input = self._build_full_prompt(input_text, memory_context)

        try:
            # è°ƒç”¨agent
            response = self.agent.invoke({
                "messages": [{"role": "user", "content": full_input}]
            }, config=self.agent_config, context=self.context)

            # å¤„ç†å·¥å…·è°ƒç”¨æƒ…å†µ
            if response and 'messages' in response:
                # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
                last_message = response['messages'][-1]

                # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†å®ƒä»¬
                if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                    if self.debug_mode:
                        logger.info(f"ğŸ› ï¸ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {[tc['name'] for tc in last_message.tool_calls]}")

                    # å¯¹äºå¤šæ™ºèƒ½ä½“å¯¹è¯ï¼Œæˆ‘ä»¬æš‚æ—¶ç®€åŒ–å¤„ç†ï¼šä¸æ‰§è¡Œå®é™…å·¥å…·è°ƒç”¨
                    # è€Œæ˜¯è¿”å›ä¸€ä¸ªæç¤ºä¿¡æ¯
                    tool_response = "åœ¨å½“å‰å¯¹è¯æ¨¡å¼ä¸‹ï¼Œå·¥å…·è°ƒç”¨åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ã€‚è¯·ç›´æ¥æä¾›æŒ‡å¯¼æˆ–å›ç­”ã€‚"

                    # åˆ›å»ºå·¥å…·å“åº”æ¶ˆæ¯
                    tool_messages = []
                    for tool_call in last_message.tool_calls:
                        tool_messages.append({
                            "role": "tool",
                            "content": tool_response,
                            "tool_call_id": tool_call['id']
                        })

                    # å¦‚æœéœ€è¦ç»§ç»­å¤„ç†å·¥å…·è°ƒç”¨ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šé€»è¾‘
                    # ä½†ç›®å‰æˆ‘ä»¬è¿”å›ä¸€ä¸ªç®€åŒ–å“åº”
                    return tool_response

                # æ­£å¸¸è¿”å›ç»“æ„åŒ–å“åº”
                return response['structured_response'].main_response

            return ""

        except Exception as e:
            if self.debug_mode:
                logger.info(f"âŒ APIè°ƒç”¨é”™è¯¯: {e}")

            # å¦‚æœå‡ºç°å·¥å…·è°ƒç”¨ç›¸å…³é”™è¯¯ï¼Œå›é€€åˆ°ç®€å•å“åº”
            if "tool_calls" in str(e) or "tool_call_id" in str(e):
                return "æˆ‘ç†è§£æ‚¨éœ€è¦å¸®åŠ©ï¼Œä½†åœ¨å½“å‰è®¾ç½®ä¸‹ï¼Œè¯·ç›´æ¥æå‡ºæ‚¨çš„é—®é¢˜æˆ–å›°æƒ‘ã€‚"

            raise e

    def _build_input_with_memory(self, input_text, memory_context):
        """æ„å»ºåŒ…å«å†…å­˜ä¸Šä¸‹æ–‡çš„è¾“å…¥"""
        if memory_context:
            return f"{memory_context}\n\nCurrent Question: {input_text}\n\nPlease respond based on the conversation context:"
        else:
            return input_text

    def _format_teacher_input(self, history):
        """æ ¼å¼åŒ–æ•™å¸ˆè¾“å…¥"""
        # context = f"Dialogue History:\n"  # dddd
        context = ''
        d_list = []
        for role, content in history[-6:]:  # æœ€è¿‘3è½®å¯¹è¯   # dddd
            if role == self.agent_type:
                d_list.append(f"assistant\n{content}\n\n")
            else:
                d_list.append(f"user\n{content}\n\n")
        history_str = "\n".join(d_list)
        context += f"\n{history_str}"
        # context += 'System: '
        # æ ¹æ®è‹æ ¼æ‹‰åº•æ•™å­¦å¼€å…³è°ƒæ•´æç¤º
        if self.socratic_teaching_enabled:
            context += SOCRATIC_RESPONSE_TEACHING
        else:
            context += BASIC_RESPONSE_TEACHING

        return context

    def get_memory_stats(self):
        """è·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if self.memory:
            return self.memory.get_stats()
        return {"enabled": False}

    def clear_memory(self):
        """æ¸…ç©ºå†…å­˜"""
        if self.memory:
            self.memory.clear()

    def _format_student_input(self, problem, history):
        """æ ¼å¼åŒ–å­¦ç”Ÿè¾“å…¥"""
        context = f"Original Problem: {problem}\n\nDialogue History:\n"
        for role, content in history:
            context += f"{role}: {content}\n\n"
        context += 'System: '
        # æ ¹æ®å¹¶è¡Œæ€è€ƒå¼€å…³è°ƒæ•´æç¤º
        if self.parallel_thinking_enabled:
            context += STUDENT_STANDARD_PARALLEL_THINKING_PROMPT.format(question=problem)

        context += "Please continue solving the problem or respond to the teacher's guidance:"
        return context

    def _should_end_dialogue(self, teacher_response):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»“æŸå¯¹è¯"""
        end_phrases = ["finished", "completed"]
        return any(phrase in teacher_response.lower() for phrase in end_phrases)

    def _has_correct_answer(self, student_answer, correct_answer):
        """åˆ¤æ–­å­¦ç”Ÿæ˜¯å¦å¾—å‡ºæ­£ç¡®ç­”æ¡ˆ"""
        # ä½¿ç”¨ç°æœ‰çš„ç­”æ¡ˆæå–å’Œæ¯”è¾ƒé€»è¾‘
        return is_equiv_MATH(correct_answer, student_answer)

    def _get_final_answer(self):
        """è·å–æœ€ç»ˆç­”æ¡ˆ"""
        student_final_answer = ""
        if self.dialogue_history:
            for role, content in self.dialogue_history:
                if role == "student":
                    student_final_answer = content[1]
            return student_final_answer
        return ""

    def agent_response_invoke(self, user_input, **kwargs):
        self.response = self.agent.invoke({
            "messages": [{"role": "user", "content": user_input}]
        }, config=self.agent_config, context=self.context, **kwargs)

    def agent_response_stream(self, user_input, **kwargs):
        for chunk in self.agent.stream({
            "messages": [{"role": "user", "content": user_input}]
        }, config=self.agent_config, context=self.context, stream_mode="values", **kwargs):
            latest_message = chunk["messages"][-1]
            if latest_message.content:
                print(f"Agent: {latest_message.content}")
            elif latest_message.tool_calls:
                print(f"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}")

    def agent_output(self, all_messages=False):
        if all_messages:
            for i in self.response['messages']:
                print(f"{type(i)}: {i.content}")
        else:
            print(f"{type(self.response['messages'][-1])}: {self.response['structured_response'].main_response}")

    def get_dialogue_summary(self):
        """è·å–å¯¹è¯æ‘˜è¦"""
        summary = f"Dialogue Summary ({self.current_turn} turns):\n"
        for i, (role, content) in enumerate(self.dialogue_history):
            summary += f"Turn {i + 1} ({role}): {content[:100]}...\n"
        return summary


class LocalModelWrapper:
    """æœ¬åœ°æ¨¡å‹åŒ…è£…å™¨ï¼Œæ¨¡æ‹ŸAPIæ¨¡å‹çš„åŠŸèƒ½"""

    def __init__(self, model_config):
        self.model_config = model_config
        self.model = ChatOllama(
            model=model_config.model_name,
            base_url=model_config.base_url,
            temperature=model_config.temperature,
            num_predict=model_config.max_tokens,
            **model_config.extra_params
        )
        self.model_name = model_config.model_name

    def invoke(self, messages):
        """æ¨¡æ‹ŸAPIè°ƒç”¨"""
        if isinstance(messages, dict) and 'messages' in messages:
            # å¤„ç†LangChainæ ¼å¼çš„è¾“å…¥
            input_text = self._extract_user_input(messages['messages'])
        else:
            input_text = str(messages)

        response = self.model.invoke(input_text)
        return self._format_response(response)

    def stream(self, messages):
        """æ¨¡æ‹Ÿæµå¼å“åº”"""
        input_text = self._extract_user_input(messages['messages'])
        response = self.model.invoke(input_text)
        yield self._format_response(response)

    def _extract_user_input(self, messages):
        """ä»æ¶ˆæ¯åˆ—è¡¨ä¸­æå–ç”¨æˆ·è¾“å…¥"""
        for msg in reversed(messages):
            if hasattr(msg, 'content'):
                return msg.content
            elif isinstance(msg, dict) and 'content' in msg:
                return msg['content']
        return ""

    def _format_response(self, response):
        """æ ¼å¼åŒ–å“åº”ä»¥å…¼å®¹APIæ ¼å¼"""
        if hasattr(response, 'content'):
            content = response.content
        else:
            content = str(response)

        return {
            'messages': [{'role': 'assistant', 'content': content}],
            'structured_response': SimpleNamespace(main_response=content)
        }


class ExpertStudentAgent(SimpleAgent):
    """å­¦éœ¸Agent"""

    def __init__(self, debug_mode=False):
        super().__init__(agent_type="expert_student", debug_mode=debug_mode)

    def generate_solution_tree(self, problem):
        """ç”Ÿæˆè§£é¢˜æ ‘"""
        prompt = TREE_GENERATE_PROMPT.format(problem)

        response = self._invoke_agent(prompt)
        return self._parse_solution_tree(response, problem)

    def _parse_solution_tree(self, response, problem):
        """è§£æè§£é¢˜æ ‘å“åº”"""
        solution_tree = SolutionTree(problem)

        try:
            # ç®€å•çš„è§£æé€»è¾‘ - åœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„è§£æ
            if "<SolutionTree>" in response:
                # æå–è§£å†³æ–¹æ¡ˆè·¯å¾„
                paths_section = response.split("<SolutionPaths>")[1].split("</SolutionPaths>")[0]
                path_blocks = paths_section.split("</Path>")

                for block in path_blocks:
                    if "<Path" in block:
                        # æå–è·¯å¾„ä¿¡æ¯
                        method = self._extract_site_tag(block, "method")
                        complexity = self._extract_site_tag(block, "complexity")
                        innovation = self._extract_site_tag(block, "innovation")

                        # æå–æ­¥éª¤
                        steps = []
                        intermediate_answers = []
                        step_parts = block.split("<Step")
                        for step_part in step_parts[1:]:
                            if ">" in step_part and "</Step>" in step_part:
                                step_content = step_part.split(">", 1)[1].split("</Step>")[0]
                                steps.append(step_content)
                            if "<IntermediateAnswer>" in step_part and "</IntermediateAnswer>" in step_part:
                                intermediate_content = \
                                step_part.split("<IntermediateAnswer>", 1)[1].split("</IntermediateAnswer>")[0]
                                intermediate_answers.append(intermediate_content)

                        # æå–æœ€ç»ˆç­”æ¡ˆ
                        final_answer = self._extract_xml_tag(block, "FinalAnswer")

                        solution_tree.add_expert_path({
                            "method": method,
                            "complexity": complexity,
                            "innovation": innovation,
                            "steps": steps,
                            "intermediate_answers": intermediate_answers,
                            "final_answer": final_answer
                        })

        except Exception as e:
            print(f"âŒ Error parsing solution tree: {e}")
            # å¦‚æœè§£æå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„è§£å†³æ–¹æ¡ˆè·¯å¾„
            solution_tree.add_expert_path({
                "method": "algebraic",
                "complexity": "medium",
                "innovation": "medium",
                "steps": ["Apply standard algebraic approach", "Solve step by step"],
                "intermediate_answers": [],
                "final_answer": "[[Answer will be determined]]"
            })

        return solution_tree

    def _extract_xml_tag(self, text, tag_name):
        """æå–XMLæ ‡ç­¾å†…å®¹"""
        start_tag = f"<{tag_name}>"
        end_tag = f"</{tag_name}>"

        if start_tag in text and end_tag in text:
            return text.split(start_tag)[1].split(end_tag)[0].strip()
        return ""

    def _extract_site_tag(self, text, tag_name):
        """æå–XMLæ ‡ç­¾å†…å®¹"""
        start_tag = f'{tag_name}="'
        end_tag = f'"'

        if start_tag in text and end_tag in text:
            return text.split(start_tag)[1].split(end_tag)[0].strip()
        return ""
    


