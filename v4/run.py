# run.py
from function.agent_IO import SimpleAgent, ModelConfig, DialogueRecord, ExperimentRecorder
from prompt.system import *
from function.tools_fun import *
from function.contex_fun import *
from function.format_fun import *
from function.memory_fun import *
from utils.evaluator import *
from utils.MARIO_EVAL.demo import is_equiv_MATH
from utils.dataset.parallel_thinking_sft_dataset import RawDataset

from langchain.agents.structured_output import ToolStrategy, ProviderStrategy
from tqdm import tqdm
import torch
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

device_name = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"Using device: {device_name}")


class MultiAgentSystem:
    """å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç®¡ç†å™¨"""

    def __init__(self, config):
        self.config = config
        self.student_agent = None
        self.teacher_agent = None
        exp_name = f"math_tutoring_explicit_interaction" if config.agent.explicit_interaction else "math_tutoring_explicit"
        self.experiment_recorder = ExperimentRecorder(exp_name)

    def initialize_agents(self):
        """åˆå§‹åŒ–å­¦ç”Ÿå’Œæ•™å¸ˆæ™ºèƒ½ä½“"""
        # å­¦ç”Ÿæ¨¡å‹é…ç½®
        student_model_config = ModelConfig(
            model_type=self.config.model.model_type_student,
            model_name=self.config.model.api_name_student,
            base_url=self.config.model.base_url_student if hasattr(self.config.model, 'base_url_student') else None,
            temperature=self.config.model.temperature_student,
            max_tokens=self.config.model.max_tokens_student
        )

        # æ•™å¸ˆæ¨¡å‹é…ç½®
        teacher_model_config = ModelConfig(
            model_type=self.config.model.model_type_teacher,
            model_name=self.config.model.api_name_teacher,
            base_url=self.config.model.base_url_teacher if hasattr(self.config.model, 'base_url_teacher') else None,
            temperature=self.config.model.temperature_teacher,
            max_tokens=self.config.model.max_tokens_teacher
        )

        # åˆå§‹åŒ–å­¦ç”Ÿæ™ºèƒ½ä½“
        self.student_agent = SimpleAgent(agent_type="student")
        self.student_agent.model_init(student_model_config)
        self.student_agent.config_create("thread_id", "student_1")

        # åˆå§‹åŒ–æ•™å¸ˆæ™ºèƒ½ä½“
        self.teacher_agent = SimpleAgent(agent_type="teacher")
        self.teacher_agent.model_init(teacher_model_config)
        self.teacher_agent.config_create("thread_id", "teacher_1")

        # é…ç½®æ™ºèƒ½ä½“å‚æ•°
        student_kwargs = {
            "max_turns": self.config.agent.get("max_turns", 5),
            "parallel_thinking": self.config.agent.get("parallel_thinking", False),
            "math_background": self.config.agent.get("math_background", "intermediate")
        }

        teacher_kwargs = {
            "socratic_teaching": self.config.agent.get("socratic_teaching", True)
        }

        # å·¥å…·é…ç½®
        student_tools = []
        teacher_tools = []

        if self.config.agent.get("parallel_thinking", False):
            student_tools.append(parallel_thinking)

        if self.config.agent.get("socratic_teaching", True):
            teacher_tools.append(socratic_questioning)
            teacher_tools.append(math_concept_explainer)

        self.student_agent.agent_init(
            student_model_config,
            prompt_sys_name=STUDENT_PROMPT,
            tools_list=student_tools,
            **student_kwargs
        )
        self.teacher_agent.agent_init(
            teacher_model_config,
            prompt_sys_name=TEACHER_PROMPT,
            tools_list=teacher_tools,
            **teacher_kwargs
        )

        # è®¾ç½®ä¸Šä¸‹æ–‡
        self.student_agent.context_set(
            user_id="student_1",
            user_role="student",
            math_background=student_kwargs["math_background"],
            parallel_thinking=student_kwargs["parallel_thinking"],
            conversation_mode="explicit" if self.config.agent.get("explicit_interaction", True) else "tool_based"
        )
        self.teacher_agent.context_set(
            user_id="teacher_1",
            user_role="teacher",
            socratic_teaching=teacher_kwargs["socratic_teaching"]
        )

        logger.info("âœ… Multi-agent system initialized successfully!")
        logger.info(f"   Student: {self.config.model.api_name_student}")
        logger.info(f"   Teacher: {self.config.model.api_name_teacher}")
        logger.info(
            f"   Mode: {'Explicit Interaction' if self.config.agent.get('explicit_interaction', True) else 'Tool-based'}")
        logger.info(f"   Parallel Thinking: {student_kwargs['parallel_thinking']}")
        logger.info(f"   Socratic Teaching: {teacher_kwargs['socratic_teaching']}")
        logger.info(f"   Math Background: {student_kwargs['math_background']}")

        return self.student_agent, self.teacher_agent

    def visualize_results(self, output_dir: str = "results"):
        """å¯è§†åŒ–å®éªŒç»“æœ"""
        if not self.experiment_recorder.records:
            print("âŒ æ²¡æœ‰å®éªŒæ•°æ®å¯ä¾›å¯è§†åŒ–")
            return

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self.experiment_recorder.calculate_statistics()
        stats = self.experiment_recorder.summary_stats

        # è®¾ç½®ç»˜å›¾é£æ ¼
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Multi-Agent Math Tutoring Experiment Results', fontsize=16, fontweight='bold')

        # 1. å‡†ç¡®ç‡å’Œç­”æ¡ˆæ³„éœ²ç‡
        metrics = ['Accuracy', 'Answer Leakage Rate']
        values = [stats['accuracy'], stats['answer_leakage_rate']]
        colors = ['#2ecc71', '#e74c3c']

        bars1 = axes[0, 0].bar(metrics, values, color=colors, alpha=0.7)
        axes[0, 0].set_title('Accuracy vs Answer Leakage')
        axes[0, 0].set_ylabel('Rate')
        axes[0, 0].set_ylim(0, 1)

        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars1, values):
            axes[0, 0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02,
                            f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

        # 2. å¹³å‡å¯¹è¯è½®æ•°åˆ†å¸ƒ
        turn_counts = [r.total_turns for r in self.experiment_recorder.records]
        axes[0, 1].hist(turn_counts, bins=range(1, max(turn_counts) + 2), alpha=0.7, color='#3498db', edgecolor='black')
        axes[0, 1].set_title('Distribution of Dialogue Turns')
        axes[0, 1].set_xlabel('Number of Turns')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].axvline(stats['avg_turns_per_problem'], color='red', linestyle='--',
                           label=f'Average: {stats["avg_turns_per_problem"]:.2f}')
        axes[0, 1].legend()

        # 3. å¹¶è¡Œæ€è€ƒå’Œæ€è€ƒè·¯å¾„
        thinking_data = ['Parallel Thinking', 'Thinking Paths']
        thinking_values = [stats['avg_parallel_thinking'], stats['avg_thinking_paths']]

        bars2 = axes[0, 2].bar(thinking_data, thinking_values, color=['#9b59b6', '#f39c12'], alpha=0.7)
        axes[0, 2].set_title('Average Thinking Metrics per Problem')
        axes[0, 2].set_ylabel('Average Count')

        for bar, value in zip(bars2, thinking_values):
            axes[0, 2].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,
                            f'{value:.2f}', ha='center', va='bottom', fontweight='bold')

        # 4. æ•™å¸ˆæ„å›¾åˆ†å¸ƒ
        teacher_intents = []
        for record in self.experiment_recorder.records:
            for turn in record.turns:
                if 'teacher_intent' in turn:
                    teacher_intents.append(turn['teacher_intent'])

        if teacher_intents:
            intent_counts = pd.Series(teacher_intents).value_counts()
            axes[1, 0].pie(intent_counts.values, labels=intent_counts.index, autopct='%1.1f%%',
                           startangle=90, colors=plt.cm.Set3(np.linspace(0, 1, len(intent_counts))))
            axes[1, 0].set_title('Teacher Response Intent Distribution')

        # 5. æ­£ç¡®ç­”æ¡ˆ vs é”™è¯¯ç­”æ¡ˆ
        correct_data = ['Correct', 'Incorrect']
        correct_values = [stats['correct_answers'], stats['total_problems'] - stats['correct_answers']]

        bars3 = axes[1, 1].bar(correct_data, correct_values, color=['#27ae60', '#c0392b'], alpha=0.7)
        axes[1, 1].set_title('Correct vs Incorrect Answers')
        axes[1, 1].set_ylabel('Count')

        for bar, value in zip(bars3, correct_values):
            axes[1, 1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,
                            f'{value}', ha='center', va='bottom', fontweight='bold')

        # 6. è½®æ¬¡ä¸å‡†ç¡®ç‡çš„å…³ç³»
        turns_vs_correct = []
        for record in self.experiment_recorder.records:
            turns_vs_correct.append((record.total_turns, 1 if record.correct else 0))

        if turns_vs_correct:
            df = pd.DataFrame(turns_vs_correct, columns=['turns', 'correct'])
            accuracy_by_turns = df.groupby('turns')['correct'].mean().reset_index()
            axes[1, 2].plot(accuracy_by_turns['turns'], accuracy_by_turns['correct'],
                            marker='o', linewidth=2, markersize=8, color='#e67e22')
            axes[1, 2].set_title('Accuracy by Number of Turns')
            axes[1, 2].set_xlabel('Number of Turns')
            axes[1, 2].set_ylabel('Accuracy Rate')
            axes[1, 2].grid(True, alpha=0.3)

        plt.tight_layout()

        # ä¿å­˜å›¾ç‰‡
        import os
        os.makedirs(output_dir, exist_ok=True)
        plot_file = os.path.join(output_dir,
                                 f"{self.experiment_recorder.experiment_name}_{self.experiment_recorder.timestamp}_plots.png")
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.show()

        print(f"âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {plot_file}")
        return plot_file


def create_raw_dataset(data_paths, data_config):
    # ä¿æŒåŸæœ‰å®ç°
    dataset = RawDataset(parquet_file=data_paths, config=data_config)
    return dataset


def solve_with_dialogue(config, logger, val_dataset, multi_agent_system):
    """ä½¿ç”¨å¤šæ™ºèƒ½ä½“å¯¹è¯è§£å†³é—®é¢˜"""
    first_correct = torch.zeros(1, dtype=torch.float32, device=device_name)
    correct = torch.zeros(1, dtype=torch.float32, device=device_name)
    total = torch.zeros(1, dtype=torch.float32, device=device_name)
    error_ans = torch.zeros(1, dtype=torch.float32, device=device_name)

    student_agent, teacher_agent = multi_agent_system.initialize_agents()

    explicit_mode = config.agent.get("explicit_interaction", True)
    eval_mode = 'Explicit Interaction' if explicit_mode else 'Tool-based'

    max_samples = config.agent.get("max_samples", min(10, len(val_dataset.prompts)))

    desc = f"Multi-agent Evaluation [{eval_mode}] - Dataset: {'GSM8k' if 'gsm' in config.data.val_files.lower() else 'MATH'}"

    for i, (prompt, label_ans) in tqdm(
            enumerate(zip(val_dataset.prompts[:max_samples], val_dataset.responses[:max_samples])),
            desc=desc, total=max_samples
    ):
        # åˆ›å»ºå¯¹è¯è®°å½•
        dialogue_record = DialogueRecord(prompt, label_ans, debug_mode=True if i < config.agent.debug_samples else False)

        if explicit_mode:
            # æ¨¡å¼1: æ˜¾å¼äº¤äº’
            final_answer, correct_answer, dialogue_record = student_agent.multi_agent_chat_explicit(
                teacher_agent, prompt, label_ans, dialogue_record
            )
        else:
            # æ¨¡å¼2: å·¥å…·è°ƒç”¨
            correct_answer = None
            final_answer = student_agent.multi_agent_chat_tool_based(
                prompt, label_ans, dialogue_record
            )

        total += 1

        if not final_answer:
            error_ans += 1
            dialogue_record.correct = False
        else:
            if dialogue_record.first_correct:
                first_correct += 1
            if dialogue_record.correct:
                correct += 1

        # æ·»åŠ è®°å½•åˆ°å®éªŒè®°å½•å™¨
        multi_agent_system.experiment_recorder.add_record(dialogue_record)

        # æ‰“å°å½“å‰è¿›åº¦
        current_first_accuracy = first_correct.item() / total.item() if total.item() > 0 else 0
        current_accuracy = correct.item() / total.item() if total.item() > 0 else 0
        logger.info(f'\nğŸ“Š Sample {i + 1}/{max_samples}:')
        logger.info(f'   First Correct: {dialogue_record.first_correct}')
        logger.info(f'   Final Correct: {dialogue_record.correct}')
        logger.info(f'   Total Dialogue Round: {dialogue_record.total_turns}')
        logger.info(f'   Parallel Thinking Count: {dialogue_record.parallel_thinking_count}')
        logger.info(f'   Thinking Paths Count: {dialogue_record.thinking_paths_count}')
        logger.info(f'   Leaked Answer: {dialogue_record.leaked_answer}')
        logger.info(f'   Current First Accuracy: {current_first_accuracy:.4f}')
        logger.info(f'   Current Final Accuracy: {current_accuracy:.4f}')

    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡å¹¶ä¿å­˜ç»“æœ
    multi_agent_system.experiment_recorder.calculate_statistics()
    multi_agent_system.experiment_recorder.save_results()
    multi_agent_system.experiment_recorder.print_summary()

    # ç”Ÿæˆå¯è§†åŒ–
    multi_agent_system.visualize_results()

    final_accuracy = correct.item() / total.item() if total.item() > 0 else 0
    logger.info(f"\nğŸ¯ Final Results:")
    logger.info(f"   Total Samples: {total.item()}")
    logger.info(f"   Correct Answers: {correct.item()}")
    logger.info(f"   Error Answers: {error_ans.item()}")
    logger.info(f"   Final Accuracy: {final_accuracy:.4f}")

    return final_accuracy


def main(config, logger):
    val_dataset = create_raw_dataset(config.data.val_files, config.data)
    multi_agent_system = MultiAgentSystem(config)

    accuracy = solve_with_dialogue(config, logger, val_dataset, multi_agent_system)
    logger.info(f"\nğŸ¯ Final Accuracy: {accuracy:.4f}")


if __name__ == '__main__':
    import hydra
    from omegaconf import OmegaConf
    from hydra.core.global_hydra import GlobalHydra

    GlobalHydra.instance().clear()
    hydra.initialize(config_path="utils", version_base=None)

    overrides = [
        # 'data.val_files=D:\DeepLearning\Code\LangChain\dataset/GSM8k_test_with_prompt2.parquet',
        'data.val_files=/mnt/t2-6tb/medical/SocraticLM_langchain/LangChain_3090/dataset/GSM8k_test_with_prompt2.parquet',
        'data.prompt_key=extra_info',
        'data.response_key=extra_info',
        'data.max_length=4096',
        '+data.first_prompt=raw',
        '+data.prompt_dict_keys=[question]',
        '+data.response_dict_keys=[answer]',

        # æ¨¡å‹é…ç½®
        '+model.temperature_student=0.7',
        '+model.max_tokens_student=2000',
        '+model.temperature_teacher=0.7',
        '+model.max_tokens_teacher=2000',

        '+model.api_name_student=deepseek-chat',
        '+model.model_type_student=api',
        # '+model.model_type_student=local',
        # '+model.api_name_student=qwen3:4b-tuned-4k',  # qwen3-4b-4k:latest
        # '+model.base_url_student=http://localhost:11434',

        '+model.api_name_teacher=deepseek-chat',
        '+model.model_type_teacher=api',
        # '+model.model_type_teacher=local',
        # '+model.api_name_teacher=qwen2.5:0.5b',
        # '+model.base_url_teacher=http://localhost:11434',

        # æ™ºèƒ½ä½“é…ç½® - åŠŸèƒ½å¼€å…³
        '+agent.max_turns=3',
        '+agent.max_samples=10',  # æµ‹è¯•ç”¨æ ·æœ¬æ•°é‡
        '+agent.debug_samples=2',
        '+agent.explicit_interaction=true',  # true=æ˜¾å¼äº¤äº’, false=å·¥å…·è°ƒç”¨
        '+agent.parallel_thinking=false',  # å­¦ç”Ÿå¹¶è¡Œæ€è€ƒèƒ½åŠ›
        '+agent.socratic_teaching=false',  # æ•™å¸ˆè‹æ ¼æ‹‰åº•æ•™å­¦
        '+agent.math_background=intermediate',  # beginner/intermediate/advanced

        # 'model.partial_pretrain=/mnt/t2-6tb/medical/pretrained/Qwen3_merged_with_lora_global_step_14775',
        # '+model.log_folder_path=D:\DeepLearning\Code\LangChain\log',
        '+model.log_folder_path=/mnt/t2-6tb/medical/SocraticLM_langchain/LangChain_3090/log',
        '+trainer.checkpoint_path=True',
    ]

    config = hydra.compose(config_name="sft_trainer", overrides=overrides)
    get_logger(config)
    logger.info(OmegaConf.to_yaml(config))
    main(config, logger)