import datetime
from typing import Optional, Dict, Any

# Import logger if available, otherwise use print
try:
    from v9.utils.evaluator import logger
except ImportError:
    import logging
    logger = logging.getLogger(__name__)

class SmartSummaryMemory:
    """æ™ºèƒ½å¯¹è¯æ‘˜è¦å†…å­˜ç®¡ç†"""

    def __init__(self, llm=None, max_turns=10, max_token_limit=2000, enabled=False, debug_mode=False,
                 summary_mode: str = 'batch'):
        self.debug_mode = debug_mode
        self.enabled = enabled
        self.max_turns = max_turns
        self.max_token_limit = max_token_limit
        self.llm = llm
        self.conversation_history = []
        self.summary_history = []  # å­˜å‚¨å†å²æ‘˜è¦
        self.turn_count = 0
        self.current_summary = ""
        self.summary_mode = summary_mode  # 'batch' or 'per_message'

    def add_message(self, role: str, content: str, first_msg=False):
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å²"""
        if not self.enabled:
            return

        if self.summary_mode == 'per_message':
            if first_msg:
                refined_content = content
            else:
                refined_content = self._streamline_message(role, content)
            self.conversation_history.append({"role": role, "content": refined_content})
        else:
            self.conversation_history.append({"role": role, "content": content})

        self.turn_count += 1

        # å¯¹äºbatchæ¨¡å¼ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦
        if self.summary_mode == 'batch' and self._should_generate_summary():
            self._generate_summary()

    def _streamline_message(self, role: str, content: str) -> str:
        """ç²¾ç‚¼å•æ¡æ¶ˆæ¯çš„æ ¸å¿ƒç‚¹"""
        if not self.llm:
            return content

        try:
            streamline_prompt = f"""Refine this message from a {role} in a math tutoring conversation to its essential core points. 
Keep key mathematical concepts, questions, guidance, difficulties, intermediate steps, and progress indicators. 
Make it concise while preserving all necessary meaning for continuing the conversation.

Original message:
{content}

Refined message (output only the refined content):"""

            # è°ƒç”¨LLM
            if hasattr(self.llm, 'invoke'):
                messages = [{"role": "user", "content": streamline_prompt}]
                response = self.llm.invoke(messages)

                if isinstance(response, dict):
                    if 'structured_response' in response and hasattr(response['structured_response'], 'main_response'):
                        refined = response['structured_response'].main_response
                    elif 'messages' in response and response['messages']:
                        refined = response['messages'][0].get('content', str(response))
                    else:
                        refined = str(response)
                elif hasattr(response, 'content'):
                    refined = response.content
                else:
                    refined = str(response)
            else:
                refined = content  # ç®€åŒ– fallback

            if self.debug_mode:
                logger.info(f"ğŸ“ å·²ç²¾ç‚¼æ¶ˆæ¯ ({role}): {refined[:100]}...")

            return refined.strip()

        except Exception as e:
            logger.warning(f"âŒ ç²¾ç‚¼æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            return content

    def _should_generate_summary(self):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦"""
        if not self.enabled:
            return False

        # æ¯3è½®ï¼ˆå‡è®¾æ¯è½®2æ¶ˆæ¯ï¼Œå…±6æ¶ˆæ¯ï¼‰æ€»ç»“ä¸€æ¬¡ï¼Œä¸”ä¸æ€»ç»“å‰6æ¶ˆæ¯ä¹‹å‰
        if self.turn_count >= 6 and self.turn_count % 6 == 0:
            return True

        # # åŸºäºtokenæ•°é‡åˆ¤æ–­ï¼ˆç®€å•ä¼°ç®—ï¼‰
        # total_chars = sum(len(msg["content"]) for msg in self.conversation_history)
        # estimated_tokens = total_chars // 4  # ç®€å•ä¼°ç®—ï¼š1 token â‰ˆ 4 characters
        # if estimated_tokens > self.max_token_limit:
        #     return True

        return False

    def _generate_summary(self):
        """ç”Ÿæˆå¯¹è¯æ‘˜è¦"""
        if not self.enabled or not self.llm:
            return

        try:
            # æ„å»ºæ‘˜è¦æç¤ºè¯
            conversation_text = "\n".join(
                [f"{msg['role']}: {msg['content']}" for msg in self.conversation_history]
            )

            summary_prompt = f"""Please generate a high-quality summary of the following math tutoring conversation. This summary requires:

1. Keep the core information of the conversation intact, including:
- Math problem being discussed
- Studentsâ€™ current problem-solving ideas and difficulties encountered
- Key guidance and questions from teachers
- Important intermediate steps and mathematical concepts
- Current problem-solving progress
2. The summary should be detailed enough to directly replace the original conversation history so that subsequent conversations can continue based on the summary.
3. Maintain conversational coherence and contextual integrity
4. Use clear and concise language to highlight key information

Conversation:
{conversation_text}

Provide a concise but informative summary in English (output only the concise summary):"""

            # è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦
            if hasattr(self.llm, 'invoke'):
                # ä½¿ç”¨æ¶ˆæ¯æ ¼å¼è°ƒç”¨
                messages = [{"role": "user", "content": summary_prompt}]
                response = self.llm.invoke(messages)
                
                # å¤„ç†å“åº”æ ¼å¼
                if isinstance(response, dict):
                    if 'structured_response' in response and hasattr(response['structured_response'], 'main_response'):
                        summary = response['structured_response'].main_response
                    elif 'messages' in response and response['messages']:
                        summary = response['messages'][0].get('content', str(response))
                    else:
                        summary = str(response)
                elif hasattr(response, 'content'):
                    summary = response.content
                else:
                    summary = str(response)
            else:
                # ç®€åŒ–è°ƒç”¨
                summary = "Summary: Math problem discussion in progress"

            self.current_summary = summary
            self.summary_history.append({
                "turn_count": self.turn_count,
                "summary": summary,
                "timestamp": datetime.datetime.now().isoformat()
            })

            # ä¿ç•™åˆå§‹æ¶ˆæ¯ + æœ€è¿‘å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡
            keep_recent = min(3, len(self.conversation_history))
            if len(self.conversation_history) > keep_recent:
                self.conversation_history = [self.conversation_history[0]] + self.conversation_history[-keep_recent:]
            self.turn_count = len(self.conversation_history)

            if self.debug_mode:
                logger.info(f"ğŸ“ å·²ç”Ÿæˆå¯¹è¯æ‘˜è¦ (ç¬¬{len(self.summary_history)}æ¬¡æ‘˜è¦)")
                logger.info(f"Summary: {summary[:200]}...")

        except Exception as e:
            logger.warning(f"âŒ ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")
            self.current_summary = f"Conversation history: {len(self.conversation_history)} turns about math problem"

    def get_context(self):
        """è·å–å½“å‰ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«æ‘˜è¦ï¼‰"""
        if not self.enabled:
            return ""

        context_parts = []

        # æ·»åŠ å½“å‰æ‘˜è¦
        if self.current_summary:
            context_parts.append(f"Previous Conversation Summary:\n{self.current_summary}")

        # æ·»åŠ æœ€è¿‘å¯¹è¯å†å²
        recent_messages = self.conversation_history[-3:]  # ä¿ç•™æœ€è¿‘3è½®
        if recent_messages:
            recent_text = "\n".join(
                [f"{msg['role']}: {msg['content']}" for msg in recent_messages]
            )
            context_parts.append(f"Recent Dialogue:\n{recent_text}")

        return "\n\n".join(context_parts) if context_parts else ""

    def clear(self):
        """æ¸…ç©ºå†…å­˜"""
        self.conversation_history.clear()
        self.summary_history.clear()
        self.turn_count = 0
        self.current_summary = ""

    def get_stats(self):
        """è·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "enabled": self.enabled,
            "summary_mode": self.summary_mode,
            "total_turns": self.turn_count,
            "conversation_history_count": len(self.conversation_history),
            "summary_history_count": len(self.summary_history),
            "current_summary_length": len(self.current_summary)
        }


class SummaryConfig:
    """æ‘˜è¦é…ç½®ç±»"""

    def __init__(self, enabled=False, max_turns=10, max_token_limit=2000, summary_model=None):
        self.enabled = enabled
        self.max_turns = max_turns
        self.max_token_limit = max_token_limit
        self.summary_model = summary_model  # ä¸“é—¨ç”¨äºæ‘˜è¦çš„æ¨¡å‹é…ç½®


class DialogueRecord:
    """å¯¹è¯è®°å½•ç±»"""

    def __init__(self, problem: str, correct_answer: str, debug_mode: bool = False):
        self.problem = problem
        self.correct_answer = correct_answer
        self.debug_mode = debug_mode
        self.turns = []
        self.final_student_answer = ""
        self.first_correct = False
        self.correct = False
        self.leaked_answer = False
        self.parallel_thinking_count = 0
        self.thinking_paths_count = 0
        self.total_turns = 0

    def add_turn(self, turn_data: Dict[str, Any]):
        """æ·»åŠ ä¸€è½®å¯¹è¯è®°å½•"""
        self.turns.append(turn_data)
        self.total_turns = len(self.turns)

    def analyze_student_response(self, response: str):
        """åˆ†æå­¦ç”Ÿå›å¤"""
        # ç»Ÿè®¡å¹¶è¡Œæ€è€ƒæ ‡ç­¾
        parallel_count = response.count('<Parallel')
        self.parallel_thinking_count += parallel_count

        # ç»Ÿè®¡æ€è€ƒè·¯å¾„æ ‡ç­¾
        path_count = response.count('<Path')
        self.thinking_paths_count += path_count

        return parallel_count, path_count

    def check_answer_leakage(self, teacher_response: str, answer_num: str):
        """æ£€æŸ¥æ•™å¸ˆæ˜¯å¦æ³„éœ²ç­”æ¡ˆ"""
        # ç®€å•çš„ç­”æ¡ˆæ³„éœ²æ£€æµ‹é€»è¾‘
        leakage_indicators = [
            "Final Answer: " + self.correct_answer,
            "the result is " + self.correct_answer,
            "equals to " + self.correct_answer,
            "= " + self.correct_answer,
            answer_num
        ]

        leakage_detected = any(
            indicator.lower() in teacher_response.lower()
            for indicator in leakage_indicators
            if indicator.strip()
        )

        if leakage_detected:
            self.leaked_answer = True

        return leakage_detected

    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "problem": self.problem,
            "correct_answer": self.correct_answer,
            "final_student_answer": self.final_student_answer,
            "correct": self.correct,
            "leaked_answer": self.leaked_answer,
            "parallel_thinking_count": self.parallel_thinking_count,
            "thinking_paths_count": self.thinking_paths_count,
            "total_turns": self.total_turns,
            "turns": self.turns
        }
