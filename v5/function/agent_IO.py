# agent_IO.py
import logging
import os
import sys
sys.path.append(r'../..')
from typing import Callable, List, Optional, Dict, Any
from types import SimpleNamespace
from utils.api_config import *
from utils.evaluator import extract_answer, logger
from utils.MARIO_EVAL.demo import is_equiv_MATH
from prompt.system import *
from function.contex_fun import *
from function.format_fun import *
from function.memory_fun import *
selected_model = 'deepseek-official'
os.environ["OPENAI_API_KEY"] = key_config.get(selected_model, "")
os.environ["OPENAI_BASE_URL"] = key_url
os.environ["DEEPSEEK_API_KEY"] = key_config.get(selected_model, "")
os.environ["DEEPSEEK_BASE_URL"] = ds_key_url
os.environ["LANGSMITH_TRACING"] = "false"
os.environ["LANGSMITH_API_KEY"] = smith_key
os.environ.pop('HTTP_PROXY', None)
os.environ.pop('HTTPS_PROXY', None)
os.environ.pop('http_proxy', None)
os.environ.pop('https_proxy', None)
os.environ.pop('ALL_PROXY', None)
os.environ.pop('all_proxy', None)

from langchain_core.messages import HumanMessage, ToolMessage
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, wrap_tool_call, dynamic_prompt, ModelRequest, ModelResponse
from langchain.chat_models import init_chat_model
from langchain_ollama import ChatOllama
from langchain.agents import AgentState
from langchain.agents.middleware import AgentMiddleware
import datetime
import sys

sys.path.append(r'D:\DeepLearning\Code\LangChain\prompt')
sys.path.append(r'D:\DeepLearning\Code\LangChain\function')
from prompt.system import *
from function.tools_fun import *
from function.contex_fun import *
from function.format_fun import *
from function.memory_fun import *


class LocalModelWrapper:
    """æœ¬åœ°æ¨¡å‹åŒ…è£…å™¨ï¼Œæ¨¡æ‹ŸAPIæ¨¡å‹çš„åŠŸèƒ½"""

    def __init__(self, model_config):
        self.model_config = model_config
        self.model = ChatOllama(
            model=model_config.model_name,
            base_url=model_config.base_url,
            temperature=model_config.temperature,
            num_predict=model_config.max_tokens,
            **model_config.extra_params
        )
        self.model_name = model_config.model_name

    def invoke(self, messages):
        """æ¨¡æ‹ŸAPIè°ƒç”¨"""
        if isinstance(messages, dict) and 'messages' in messages:
            # å¤„ç†LangChainæ ¼å¼çš„è¾“å…¥
            input_text = self._extract_user_input(messages['messages'])
        else:
            input_text = str(messages)

        response = self.model.invoke(input_text)
        return self._format_response(response)

    def stream(self, messages):
        """æ¨¡æ‹Ÿæµå¼å“åº”"""
        input_text = self._extract_user_input(messages['messages'])
        response = self.model.invoke(input_text)
        yield self._format_response(response)

    def _extract_user_input(self, messages):
        """ä»æ¶ˆæ¯åˆ—è¡¨ä¸­æå–ç”¨æˆ·è¾“å…¥"""
        for msg in reversed(messages):
            if hasattr(msg, 'content'):
                return msg.content
            elif isinstance(msg, dict) and 'content' in msg:
                return msg['content']
        return ""

    def _format_response(self, response):
        """æ ¼å¼åŒ–å“åº”ä»¥å…¼å®¹APIæ ¼å¼"""
        if hasattr(response, 'content'):
            content = response.content
        else:
            content = str(response)

        return {
            'messages': [{'role': 'assistant', 'content': content}],
            'structured_response': SimpleNamespace(main_response=content)
        }


class SimpleLocalAgent:
    """ç®€åŒ–ç‰ˆæœ¬åœ°Agentï¼Œæ”¯æŒæ ¸å¿ƒåŠŸèƒ½"""

    def __init__(self, agent_type="student", debug_mode=False):
        self.agent_type = agent_type
        self.model = None
        self.system_prompt = ""
        self.tools = []
        self.context = {}
        self.dialogue_history = []
        self.debug_mode = debug_mode
        self.last_full_prompt = ""  # è®°å½•æœ€åä¸€æ¬¡å®Œæ•´prompt

    def init_model(self, model_config):
        """åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹"""
        self.model = LocalModelWrapper(model_config)

    def set_system_prompt(self, prompt):
        """è®¾ç½®ç³»ç»Ÿæç¤ºè¯"""
        self.system_prompt = prompt

    def add_tools(self, tools_list):
        """æ·»åŠ å·¥å…·ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        self.tools = tools_list

    def set_context(self, **kwargs):
        """è®¾ç½®ä¸Šä¸‹æ–‡"""
        self.context.update(kwargs)

    def invoke(self, input_dict, config=None, context=None):
        """è°ƒç”¨Agent"""
        user_input = self._extract_input(input_dict)

        # æ„å»ºå®Œæ•´æç¤ºè¯
        full_prompt = self._build_full_prompt(user_input)
        self.last_full_prompt = full_prompt  # ä¿å­˜å®Œæ•´prompt

        # Debugè¾“å‡º
        if self.debug_mode:
            logger.info("\n" + "=" * 80)
            logger.info(f"ğŸ” DEBUG - {self.agent_type.upper()} AGENT FULL PROMPT:")
            logger.info("=" * 80)
            logger.info(full_prompt)
            logger.info("=" * 80 + "\n")

        # è°ƒç”¨æ¨¡å‹
        response = self.model.invoke(full_prompt)

        # è®°å½•å¯¹è¯å†å²
        self.dialogue_history.append(("user", user_input))
        self.dialogue_history.append(("assistant", response['structured_response'].main_response))

        return response

    def get_last_prompt(self):
        """è·å–æœ€åä¸€æ¬¡çš„å®Œæ•´prompt"""
        return self.last_full_prompt

    def _extract_input(self, input_dict):
        """æå–è¾“å…¥æ–‡æœ¬"""
        messages = input_dict.get('messages', [])
        for msg in reversed(messages):
            if hasattr(msg, 'content'):
                return msg.content
            elif isinstance(msg, dict) and 'content' in msg:
                return msg['content']
        return ""

    def _build_full_prompt(self, user_input):
        """æ„å»ºå®Œæ•´æç¤ºè¯"""
        prompt_parts = []

        # ç³»ç»Ÿæç¤ºè¯
        if self.system_prompt:
            prompt_parts.append(f"System: {self.system_prompt}")

        # ä¸Šä¸‹æ–‡ä¿¡æ¯
        if self.context:
            context_str = ", ".join([f"{k}: {v}" for k, v in self.context.items()])
            prompt_parts.append(f"Context: {context_str}")

        # å¯¹è¯å†å²
        if self.dialogue_history:
            history_str = "\n".join([f"{role}: {content}" for role, content in self.dialogue_history[-6:]])  # æœ€è¿‘3è½®å¯¹è¯
            prompt_parts.append(f"Dialogue History:\n{history_str}")

        # å½“å‰è¾“å…¥
        prompt_parts.append(f"Teacher: {user_input}")
        prompt_parts.append("Assistant:")

        return "\n\n".join(prompt_parts)


class SmartSummaryMemory:
    """æ™ºèƒ½å¯¹è¯æ‘˜è¦å†…å­˜ç®¡ç†"""

    def __init__(self, llm=None, max_turns=10, max_token_limit=2000, enabled=False, debug_mode=False):
        self.debug_mode = debug_mode
        self.enabled = enabled
        self.max_turns = max_turns
        self.max_token_limit = max_token_limit
        self.llm = llm
        self.conversation_history = []
        self.summary_history = []  # å­˜å‚¨å†å²æ‘˜è¦
        self.turn_count = 0
        self.current_summary = ""

    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å²"""
        if not self.enabled:
            return

        self.conversation_history.append({"role": role, "content": content})
        self.turn_count += 1

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦
        if self._should_generate_summary():
            self._generate_summary()

    def _should_generate_summary(self):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦"""
        if not self.enabled:
            return False

        # åŸºäºè½®æ¬¡åˆ¤æ–­
        if self.turn_count >= self.max_turns:
            return True

        # åŸºäºtokenæ•°é‡åˆ¤æ–­ï¼ˆç®€å•ä¼°ç®—ï¼‰
        total_chars = sum(len(msg["content"]) for msg in self.conversation_history)
        estimated_tokens = total_chars // 4  # ç®€å•ä¼°ç®—ï¼š1 token â‰ˆ 4 characters
        if estimated_tokens > self.max_token_limit:
            return True

        return False

    def _generate_summary(self):
        """ç”Ÿæˆå¯¹è¯æ‘˜è¦"""
        if not self.enabled or not self.llm:
            return

        try:
            # æ„å»ºæ‘˜è¦æç¤ºè¯
            conversation_text = "\n".join(
                [f"{msg['role']}: {msg['content']}" for msg in self.conversation_history]
            )

            summary_prompt = f"""Please summarize the key information from this math tutoring conversation, focusing on:

1. The main math problem being discussed
2. Student's current approach and difficulties
3. Teacher's guidance and key questions asked
4. Important intermediate steps and concepts
5. Current progress toward solution

Conversation:
{conversation_text}

Provide a concise but informative summary in English:"""

            # è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦
            if hasattr(self.llm, 'invoke'):
                response = self.llm.invoke(summary_prompt)
                summary = response.content if hasattr(response, 'content') else str(response)
            else:
                # ç®€åŒ–è°ƒç”¨
                summary = "Summary: Math problem discussion in progress"

            self.current_summary = summary
            self.summary_history.append({
                "turn_count": self.turn_count,
                "summary": summary,
                "timestamp": datetime.datetime.now().isoformat()
            })

            # ä¿ç•™æœ€è¿‘å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡
            keep_recent = min(3, len(self.conversation_history))
            self.conversation_history = self.conversation_history[-keep_recent:]
            self.turn_count = keep_recent

            if self.debug_mode:
                logger.info(f"ğŸ“ å·²ç”Ÿæˆå¯¹è¯æ‘˜è¦ (ç¬¬{len(self.summary_history)}æ¬¡æ‘˜è¦)")
                logger.info(f"Summary: {summary[:200]}...")

        except Exception as e:
            logger.warning(f"âŒ ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")
            self.current_summary = f"Conversation history: {len(self.conversation_history)} turns about math problem"

    def get_context(self):
        """è·å–å½“å‰ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«æ‘˜è¦ï¼‰"""
        if not self.enabled:
            return ""

        context_parts = []

        # æ·»åŠ å½“å‰æ‘˜è¦
        if self.current_summary:
            context_parts.append(f"Previous Conversation Summary:\n{self.current_summary}")

        # æ·»åŠ æœ€è¿‘å¯¹è¯å†å²
        recent_messages = self.conversation_history[-3:]  # ä¿ç•™æœ€è¿‘3è½®
        if recent_messages:
            recent_text = "\n".join(
                [f"{msg['role']}: {msg['content']}" for msg in recent_messages]
            )
            context_parts.append(f"Recent Dialogue:\n{recent_text}")

        return "\n\n".join(context_parts) if context_parts else ""

    def clear(self):
        """æ¸…ç©ºå†…å­˜"""
        self.conversation_history.clear()
        self.summary_history.clear()
        self.turn_count = 0
        self.current_summary = ""

    def get_stats(self):
        """è·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "enabled": self.enabled,
            "total_turns": self.turn_count,
            "conversation_history_count": len(self.conversation_history),
            "summary_history_count": len(self.summary_history),
            "current_summary_length": len(self.current_summary)
        }


class SummaryConfig:
    """æ‘˜è¦é…ç½®ç±»"""
    def __init__(self, enabled=False, max_turns=10, max_token_limit=2000, summary_model=None):
        self.enabled = enabled
        self.max_turns = max_turns
        self.max_token_limit = max_token_limit
        self.summary_model = summary_model  # ä¸“é—¨ç”¨äºæ‘˜è¦çš„æ¨¡å‹é…ç½®


class DialogueRecord:
    """å¯¹è¯è®°å½•ç±»"""

    def __init__(self, problem: str, correct_answer: str, debug_mode: bool = False):
        self.problem = problem
        self.correct_answer = correct_answer
        self.debug_mode = debug_mode
        self.turns = []
        self.final_student_answer = ""
        self.first_correct = False
        self.correct = False
        self.leaked_answer = False
        self.parallel_thinking_count = 0
        self.thinking_paths_count = 0
        self.total_turns = 0

    def add_turn(self, turn_data: Dict[str, Any]):
        """æ·»åŠ ä¸€è½®å¯¹è¯è®°å½•"""
        self.turns.append(turn_data)
        self.total_turns = len(self.turns)

    def analyze_student_response(self, response: str):
        """åˆ†æå­¦ç”Ÿå›å¤"""
        # ç»Ÿè®¡å¹¶è¡Œæ€è€ƒæ ‡ç­¾
        parallel_count = response.count('<Parallel')
        self.parallel_thinking_count += parallel_count

        # ç»Ÿè®¡æ€è€ƒè·¯å¾„æ ‡ç­¾
        path_count = response.count('<Path')
        self.thinking_paths_count += path_count

        return parallel_count, path_count

    def check_answer_leakage(self, teacher_response: str):
        """æ£€æŸ¥æ•™å¸ˆæ˜¯å¦æ³„éœ²ç­”æ¡ˆ"""
        # ç®€å•çš„ç­”æ¡ˆæ³„éœ²æ£€æµ‹é€»è¾‘
        leakage_indicators = [
            "the answer is " + self.correct_answer,
            "the result is " + self.correct_answer,
            "equals to " + self.correct_answer,
            "= " + self.correct_answer
        ]

        leakage_detected = any(
            indicator.lower() in teacher_response.lower()
            for indicator in leakage_indicators
            if indicator.strip()
        )

        if leakage_detected:
            self.leaked_answer = True

        return leakage_detected

    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "problem": self.problem,
            "correct_answer": self.correct_answer,
            "final_student_answer": self.final_student_answer,
            "correct": self.correct,
            "leaked_answer": self.leaked_answer,
            "parallel_thinking_count": self.parallel_thinking_count,
            "thinking_paths_count": self.thinking_paths_count,
            "total_turns": self.total_turns,
            "turns": self.turns
        }



class StudentCognitiveState:
    """å­¦ç”Ÿè®¤çŸ¥çŠ¶æ€ç®¡ç†"""

    def __init__(self,
                 carelessness_level=5,
                 math_background="intermediate",
                 response_style="thoughtful",
                 preferred_method="balanced",
                 learning_style="visual"):

        self.carelessness_level = carelessness_level  # 1-10
        self.math_background = math_background  # beginner/intermediate/advanced
        self.response_style = response_style  # thoughtful/impulsive/detailed/brief
        self.preferred_method = preferred_method  # algebraic/geometric/computational/balanced
        self.learning_style = learning_style  # visual/auditory/kinesthetic/reading-writing

        # åŠ¨æ€å±æ€§
        self.problem_solving_history = []
        self.error_patterns = []
        self.method_preferences = []
        self.conceptual_understanding = {}

    def update_based_on_interaction(self, problem, student_approach, errors, method_used, success):
        """åŸºäºäº¤äº’æ›´æ–°è®¤çŸ¥çŠ¶æ€"""
        # è®°å½•é—®é¢˜è§£å†³å†å²
        self.problem_solving_history.append({
            "problem": problem,
            "approach": student_approach,
            "errors": errors,
            "method_used": method_used,
            "success": success,
            "timestamp": datetime.datetime.now().isoformat()
        })

        # æ›´æ–°ç²—å¿ƒç¨‹åº¦ï¼ˆåŸºäºé”™è¯¯æ¨¡å¼ï¼‰
        if errors:
            self.error_patterns.extend(errors)
            # å¦‚æœé¢‘ç¹å‡ºç°è®¡ç®—é”™è¯¯ï¼Œå¢åŠ ç²—å¿ƒç¨‹åº¦
            calculation_errors = sum(1 for error in errors if "calculation" in error.lower())
            if calculation_errors > 0:
                self.carelessness_level = min(10, self.carelessness_level + 0.5)

        # æ›´æ–°æ–¹æ³•åå¥½
        self.method_preferences.append(method_used)

        # æ›´æ–°æ•°å­¦èƒŒæ™¯ï¼ˆåŸºäºæˆåŠŸç‡å’Œæ–¹æ³•å¤æ‚åº¦ï¼‰
        if success and len(student_approach) > 3:  # å¤æ‚çš„æˆåŠŸè§£æ³•
            if self.math_background == "beginner":
                self.math_background = "intermediate"
            elif self.math_background == "intermediate" and len(self.problem_solving_history) > 5:
                self.math_background = "advanced"

    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "carelessness_level": self.carelessness_level,
            "math_background": self.math_background,
            "response_style": self.response_style,
            "preferred_method": self.preferred_method,
            "learning_style": self.learning_style,
            "problem_solving_count": len(self.problem_solving_history),
            "recent_success_rate": self._calculate_recent_success_rate()
        }

    def _calculate_recent_success_rate(self):
        """è®¡ç®—æœ€è¿‘çš„æˆåŠŸç‡"""
        if len(self.problem_solving_history) == 0:
            return 0.0
        recent = self.problem_solving_history[-5:]  # æœ€è¿‘5ä¸ªé—®é¢˜
        successes = sum(1 for record in recent if record["success"])
        return successes / len(recent)

    def get_prompt_context(self):
        """è·å–æç¤ºè¯ä¸Šä¸‹æ–‡"""
        return STUDENT_COGNITIVE_STATE_PROMPT.format(
            carelessness_level=self.carelessness_level,
            math_background=self.math_background,
            response_style=self.response_style,
            preferred_method=self.preferred_method,
            learning_style=self.learning_style
        )


class SolutionTree:
    """è§£é¢˜æ ‘ç®¡ç†"""

    def __init__(self, problem_statement):
        self.problem_statement = problem_statement
        self.root_node = {"type": "problem", "content": problem_statement}
        self.solution_paths = []
        self.current_student_path = []

    def add_expert_path(self, path_data):
        """æ·»åŠ ä¸“å®¶è§£é¢˜è·¯å¾„"""
        self.solution_paths.append({
            "type": "expert",
            "path_id": len(self.solution_paths) + 1,
            "method": path_data.get("method", "unknown"),
            "complexity": path_data.get("complexity", "medium"),
            "innovation": path_data.get("innovation", "medium"),
            "steps": path_data.get("steps", []),
            "intermediate_answers": path_data.get("intermediate_answers", []),
            "final_answer": path_data.get("final_answer", "")
        })

    def add_student_step(self, step_content, method_used=None):
        """æ·»åŠ å­¦ç”Ÿè§£é¢˜æ­¥éª¤"""
        # å¦‚æœæ²¡æœ‰æä¾›æ–¹æ³•ï¼Œå°è¯•ä»æ­¥éª¤å†…å®¹ä¸­æ£€æµ‹
        if method_used is None:
            method_used = self._detect_student_method(step_content)

        self.current_student_path.append({
            "step_number": len(self.current_student_path) + 1,
            "content": step_content,
            "method": method_used,
            "timestamp": datetime.datetime.now().isoformat()
        })

    def complete_student_path(self, success, final_answer=None):
        """å®Œæˆå­¦ç”Ÿè§£é¢˜è·¯å¾„"""
        student_path = {
            "type": "student",
            "path_id": f"student_{len(self.solution_paths) + 1}",
            "method": self._detect_student_method(),
            "steps": self.current_student_path.copy(),
            "success": success,
            "final_answer": final_answer
        }
        self.solution_paths.append(student_path)
        self.current_student_path = []

        return student_path

    def _detect_student_method(self, response=None):
        """æ£€æµ‹å­¦ç”Ÿä½¿ç”¨çš„æ–¹æ³•
        Args:
            response: å¯é€‰çš„å“åº”æ–‡æœ¬ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨å½“å‰å­¦ç”Ÿè·¯å¾„
        """
        if response:
            # å¦‚æœæä¾›äº†å“åº”æ–‡æœ¬ï¼Œç›´æ¥åˆ†æè¯¥æ–‡æœ¬
            content = response.lower()
        elif self.current_student_path:
            # å¦åˆ™ä½¿ç”¨å½“å‰å­¦ç”Ÿè·¯å¾„çš„æ‰€æœ‰å†…å®¹
            content = " ".join([step["content"].lower() for step in self.current_student_path])
        else:
            return "unknown"

        # if not self.current_student_path:
        #     return "unknown"
        #
        # content = " ".join([step["content"].lower() for step in self.current_student_path])

        if any(word in content for word in ["equation", "solve for", "variable", "x ="]):
            return "algebraic"
        elif any(word in content for word in ["diagram", "graph", "shape", "angle", "area"]):
            return "geometric"
        elif any(word in content for word in ["calculate", "compute", "number", "digit"]):
            return "computational"
        elif any(word in content for word in ["logic", "reason", "therefore", "because", "since", "if then"]):
            return "logical"
        elif any(word in content for word in ["guess", "try", "maybe", "perhaps", "i think"]):
            return "trial_and_error"
        else:
            return "unknown"

    def compare_with_expert(self):
        """æ¯”è¾ƒå­¦ç”Ÿè·¯å¾„ä¸ä¸“å®¶è·¯å¾„"""
        if not self.current_student_path:
            return {"similarity": 0, "closest_expert_path": None}

        student_method = self._detect_student_method()
        expert_paths = [p for p in self.solution_paths if p["type"] == "expert"]

        if not expert_paths:
            return {"similarity": 0, "closest_expert_path": None}

        # ç®€å•çš„æ–¹æ³•åŒ¹é…
        matching_paths = [p for p in expert_paths if p["method"] == student_method]
        if matching_paths:
            closest = matching_paths[0]
            similarity = 0.8  # æ–¹æ³•åŒ¹é…çš„åŸºç¡€ç›¸ä¼¼åº¦
        else:
            closest = expert_paths[0]
            similarity = 0.3  # æ–¹æ³•ä¸åŒ¹é…çš„åŸºç¡€ç›¸ä¼¼åº¦

        return {
            "similarity": similarity,
            "closest_expert_path": closest,
            "method_match": student_method == closest["method"]
        }

    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "problem_statement": self.problem_statement,
            "solution_paths": self.solution_paths,
            "current_student_path": self.current_student_path
        }


class ExperimentRecorder:
    """å®éªŒè®°å½•å™¨"""

    def __init__(self, experiment_name: str = "multi_agent_math"):
        self.experiment_name = experiment_name
        self.records = []
        self.summary_stats = {}
        self.timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

    def add_record(self, record: DialogueRecord):
        """æ·»åŠ å¯¹è¯è®°å½•"""
        self.records.append(record)

    def calculate_statistics(self):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        if not self.records:
            return {}

        total_problems = len(self.records)
        correct_answers = sum(1 for r in self.records if r.correct)
        leaked_answers = sum(1 for r in self.records if r.leaked_answer)
        total_turns = sum(r.total_turns for r in self.records)
        total_parallel_thinking = sum(r.parallel_thinking_count for r in self.records)
        total_thinking_paths = sum(r.thinking_paths_count for r in self.records)

        self.summary_stats = {
            "total_problems": total_problems,
            "accuracy": correct_answers / total_problems if total_problems > 0 else 0,
            "answer_leakage_rate": leaked_answers / total_problems if total_problems > 0 else 0,
            "avg_turns_per_problem": total_turns / total_problems if total_problems > 0 else 0,
            "avg_parallel_thinking": total_parallel_thinking / total_problems if total_problems > 0 else 0,
            "avg_thinking_paths": total_thinking_paths / total_problems if total_problems > 0 else 0,
            "correct_answers": correct_answers,
            "leaked_answers": leaked_answers
        }

        return self.summary_stats

    def save_results(self, output_dir: str = "results"):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        import os
        os.makedirs(output_dir, exist_ok=True)

        # ä¿å­˜è¯¦ç»†è®°å½•
        detailed_data = {
            "experiment_name": self.experiment_name,
            "timestamp": self.timestamp,
            "records": [record.to_dict() for record in self.records],
            "summary": self.summary_stats
        }

        return detailed_data, self.summary_stats

    def print_summary(self):
        """æ‰“å°å®éªŒæ‘˜è¦"""
        if not self.summary_stats:
            self.calculate_statistics()

        print("\n" + "=" * 60)
        print("ğŸ¯ å®éªŒæ‘˜è¦ç»Ÿè®¡")
        print("=" * 60)
        print(f"æ€»é—®é¢˜æ•°: {self.summary_stats['total_problems']}")
        print(f"å‡†ç¡®ç‡: {self.summary_stats['accuracy']:.4f}")
        print(f"ç­”æ¡ˆæ³„éœ²ç‡: {self.summary_stats['answer_leakage_rate']:.4f}")
        print(f"å¹³å‡å¯¹è¯è½®æ•°: {self.summary_stats['avg_turns_per_problem']:.2f}")
        print(f"å¹³å‡å¹¶è¡Œæ€è€ƒæ¬¡æ•°: {self.summary_stats['avg_parallel_thinking']:.2f}")
        print(f"å¹³å‡æ€è€ƒè·¯å¾„æ•°: {self.summary_stats['avg_thinking_paths']:.2f}")
        print(f"æ­£ç¡®ç­”æ¡ˆæ•°: {self.summary_stats['correct_answers']}")
        print(f"æ³„éœ²ç­”æ¡ˆæ•°: {self.summary_stats['leaked_answers']}")
        print("=" * 60)


class ModelConfig:
    """ç»Ÿä¸€çš„æ¨¡å‹é…ç½®ç±»"""

    def __init__(self, model_type: str = "api", **kwargs):
        self.model_type = model_type  # "api" æˆ– "local"
        self.model_name = kwargs.get("model_name", "")
        self.base_url = kwargs.get("base_url", "http://localhost:11434")
        self.temperature = kwargs.get("temperature", 0.7)
        self.max_tokens = kwargs.get("max_tokens", 2000)
        self.timeout = kwargs.get("timeout", 30)
        self.extra_params = kwargs.get("extra_params", {})


class SimpleAgent(object):
    """å¢å¼ºçš„Agentç±»ï¼Œæ”¯æŒå¤šæ¨¡å¼å’Œæœ¬åœ°è°ƒç”¨"""

    def __init__(self, agent_type: str = "student", debug_mode: bool = False):
        """åˆå§‹åŒ–agent

        Args:
            agent_type: "student" æˆ– "teacher"
        """
        self.agent_type = agent_type
        self.prompt_sys = None
        self.tools = []
        self.context_schema = None
        self.response_format = None
        self.checkpointer = None
        self.middleware_list = []

        self.agent_config = None
        self.response = None
        self.context = None

        self.middleware_er = None
        self.custom_middleware_er = CustomMiddleware
        self.model = None
        self.agent = None

        # å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé…ç½®
        self.dialogue_history = []
        self.max_turns = 5
        self.current_turn = 0
        self.correct_answer = None
        self.student_answer = None

        # åŠŸèƒ½å¼€å…³
        self.parallel_thinking_enabled = False
        self.socratic_teaching_enabled = False
        self.math_background_level = False
        self.debug_mode = debug_mode

        # Memory ä¸ Summary
        self.local_agent = None
        self.memory = None  # æ·»åŠ å†…å­˜ç®¡ç†
        self.summary_config = None
        self.summary_llm = None  # ä¸“é—¨çš„æ‘˜è¦LLM

        # è§£é¢˜æ ‘
        self.cognitive_state = None  # å­¦ç”Ÿè®¤çŸ¥çŠ¶æ€
        self.solution_tree = None    # å½“å‰è§£é¢˜æ ‘
        self.use_cognitive_state = False
        self.use_solution_tree = False

    def set_cognitive_state(self, cognitive_state: StudentCognitiveState):
        """è®¾ç½®è®¤çŸ¥çŠ¶æ€"""
        self.cognitive_state = cognitive_state
        self.use_cognitive_state = True

    def set_solution_tree(self, solution_tree: SolutionTree):
        """è®¾ç½®è§£é¢˜æ ‘"""
        self.solution_tree = solution_tree
        self.use_solution_tree = True

    def _build_full_prompt(self, user_input, memory_context):
        """æ„å»ºå®Œæ•´æç¤ºè¯ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        prompt_parts = []

        # ç³»ç»Ÿæç¤ºè¯
        if self.prompt_sys:
            # å¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„ç³»ç»Ÿæç¤ºè¯
            if isinstance(self.prompt_sys, str):
                # å¦‚æœæ˜¯é¢„å®šä¹‰çš„æç¤ºè¯å˜é‡åï¼Œå°è¯•è·å–å…¶å€¼
                if self.prompt_sys in globals():
                    system_prompt = globals()[self.prompt_sys]
                else:
                    system_prompt = self.prompt_sys
            else:
                system_prompt = str(self.prompt_sys)
            prompt_parts.append(f"System: {system_prompt}")

        # è®¤çŸ¥çŠ¶æ€ï¼ˆå¦‚æœæ˜¯å­¦ç”Ÿä¸”å¯ç”¨ï¼‰
        if (self.agent_type == "student" and self.use_cognitive_state and
                self.cognitive_state and hasattr(self.cognitive_state, 'get_prompt_context')):
            cognitive_context = self.cognitive_state.get_prompt_context()
            prompt_parts.append(f"Student Profile: {cognitive_context}")

        # è§£é¢˜æ ‘ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_solution_tree and self.solution_tree:
            try:
                tree_context = self._build_solution_tree_context()
                if tree_context:
                    prompt_parts.append(f"Solution Context: {tree_context}")
            except Exception as e:
                if self.debug_mode:
                    logger.info(f"âŒ è§£é¢˜æ ‘ä¸Šä¸‹æ–‡æ„å»ºå¤±è´¥: {e}")

        # å†…å­˜ä¸Šä¸‹æ–‡
        if memory_context:
            prompt_parts.append(f"Conversation Context: {memory_context}")

        # å¯¹è¯å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(self, 'dialogue_history') and self.dialogue_history:
            history_str = "\n".join([f"{role}: {content}" for role, content in self.dialogue_history[-4:]])
            prompt_parts.append(f"Recent Dialogue:\n{history_str}")

        # å½“å‰è¾“å…¥
        prompt_parts.append(f"Current Input: {user_input}")
        prompt_parts.append("Please respond:")

        return "\n\n".join(prompt_parts)

    def _build_solution_tree_context(self):
        """æ„å»ºè§£é¢˜æ ‘ä¸Šä¸‹æ–‡"""
        if not self.solution_tree:
            return ""

        context_parts = []

        try:
            if self.agent_type == "teacher":
                try:
                    # æ•™å¸ˆçœ‹åˆ°ä¸“å®¶è·¯å¾„å’Œå­¦ç”Ÿè·¯å¾„çš„æ¯”è¾ƒ
                    if hasattr(self.solution_tree, 'compare_with_expert'):
                        comparison = self.solution_tree.compare_with_expert()
                        if comparison and comparison.get("closest_expert_path"):
                            expert_method = comparison['closest_expert_path'].get('method', 'unknown')
                            similarity = comparison.get('similarity', 0)
                            context_parts.append(
                                f"Expert solution available using {expert_method} method")
                            context_parts.append(f"Similarity to student approach: {similarity:.2f}")
                except Exception as e:
                    if self.debug_mode:
                        print(f"âš ï¸ è§£é¢˜æ ‘æ¯”è¾ƒå¤±è´¥: {e}")
                    context_parts.append("Expert guidance available for this problem")

            elif self.agent_type == "student":
                # å­¦ç”Ÿçœ‹åˆ°è‡ªå·±çš„è¿›åº¦
                if (hasattr(self.solution_tree, 'current_student_path') and
                    self.solution_tree.current_student_path):
                    context_parts.append(
                        f"Your current solution path has {len(self.solution_tree.current_student_path)} steps")
                    context_parts.append("Continue your approach or try a different method if stuck")

                # æ·»åŠ å¯ç”¨çš„ä¸“å®¶è·¯å¾„ä¿¡æ¯
            if (hasattr(self.solution_tree, 'solution_paths') and
                    self.solution_tree.solution_paths):
                expert_paths = [p for p in self.solution_tree.solution_paths
                                if hasattr(p, 'get') and p.get("type") == "expert"]
                if expert_paths:
                    methods = set(p.get("method", "unknown") for p in expert_paths)
                    context_parts.append(f"Available expert methods: {', '.join(methods)}")

        except Exception as e:
            if self.debug_mode:
                logger.info(f"âŒ è§£é¢˜æ ‘ä¸Šä¸‹æ–‡æ„å»ºå¼‚å¸¸: {e}")
            # æä¾›åŸºæœ¬çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
            context_parts.append("Solution guidance is available for this problem")

        return "\n".join(context_parts) if context_parts else ""

    def _parse_solution_tree_student(self, response, problem):
        """è§£æå­¦ç”Ÿçš„è§£é¢˜æ ‘å“åº”"""
        solution_tree = SolutionTree(problem)

        try:
            # ç®€å•çš„è§£æé€»è¾‘ - åœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„è§£æ
            if "<SolutionTree>" in response and self.agent_type == "student":
                # æå–è§£å†³æ–¹æ¡ˆè·¯å¾„
                paths_section = response.split("<SolutionPaths>")[1].split("</SolutionPaths>")[0]
                path_blocks = paths_section.split("</Path>")

                for block in path_blocks:
                    if "<Path" in block:
                        # æå–è·¯å¾„ä¿¡æ¯
                        method = self._extract_site_tag(block, "method")
                        # æå–æ­¥éª¤
                        steps = []
                        intermediate_answers = []
                        step_parts = block.split("<Step")
                        for step_part in step_parts[1:]:
                            if ">" in step_part and "</Step>" in step_part:
                                step_content = step_part.split(">", 1)[1].split("</Step>")[0]
                                steps.append(step_content)
                            if "<IntermediateAnswer>" in step_part and "</IntermediateAnswer>" in step_part:
                                intermediate_content = step_part.split("<IntermediateAnswer>", 1)[1].split("</IntermediateAnswer>")[0]
                                intermediate_answers.append(intermediate_content)

                        # æå–æœ€ç»ˆç­”æ¡ˆ
                        final_answer = self._extract_xml_tag(block, "FinalAnswer")

                        solution_tree.add_expert_path({
                            "method": method,
                            "steps": steps,
                            "intermediate_answers": intermediate_answers,
                            "final_answer": final_answer
                        })

        except Exception as e:
            print(f"âŒ Error parsing solution tree: {e}")
            # å¦‚æœè§£æå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„è§£å†³æ–¹æ¡ˆè·¯å¾„
            solution_tree.add_expert_path({
                "method": "algebraic",
                "steps": ["Apply standard algebraic approach", "Solve step by step"],
                "final_answer": "[[Answer will be determined]]"
            })

        return solution_tree

    def _extract_xml_tag(self, text, tag_name):
        """æå–XMLæ ‡ç­¾å†…å®¹"""
        start_tag = f"<{tag_name}>"
        end_tag = f"</{tag_name}>"

        if start_tag in text and end_tag in text:
            return text.split(start_tag)[1].split(end_tag)[0].strip()
        return ""

    def _extract_site_tag(self, text, tag_name):
        """æå–XMLæ ‡ç­¾å†…å®¹"""
        start_tag = f'{tag_name}="'
        end_tag = f'"'

        if start_tag in text and end_tag in text:
            return text.split(start_tag)[1].split(end_tag)[0].strip()
        return ""

    def record_student_step(self, step_content, method_used=None):
        """è®°å½•å­¦ç”Ÿè§£é¢˜æ­¥éª¤"""
        if (self.agent_type == "student" and self.use_solution_tree and
                self.solution_tree and hasattr(self.solution_tree, 'add_student_step')):
            try:
                self.solution_tree.add_student_step(step_content, method_used)

                # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                if hasattr(self, 'debug_mode') and self.debug_mode:
                    detected_method = method_used if method_used else self.solution_tree._detect_student_method(
                        step_content)
                    print(f"ğŸ“ è®°å½•è§£é¢˜æ­¥éª¤: æ–¹æ³•={detected_method}, å†…å®¹é•¿åº¦={len(step_content)}")
            except Exception as e:
                if self.debug_mode:
                    print(f"âŒ è®°å½•è§£é¢˜æ­¥éª¤å¤±è´¥: {e}")


    def complete_student_solution(self, success, final_answer=None):
        """å®Œæˆå­¦ç”Ÿè§£é¢˜"""
        if (self.agent_type == "student" and self.use_solution_tree and
                self.solution_tree and hasattr(self.solution_tree, 'complete_student_path')):
            result = self.solution_tree.complete_student_path(success, final_answer)

            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            if hasattr(self, 'debug_mode') and self.debug_mode:
                print(f"ğŸŒ³ è§£é¢˜æ ‘å®Œæˆ: æˆåŠŸ={success}, ç­”æ¡ˆ={final_answer}")
                if hasattr(self.solution_tree, 'current_student_path'):
                    print(f"   è·¯å¾„æ­¥éª¤æ•°: {len(self.solution_tree.current_student_path)}")
                if hasattr(self.solution_tree, 'solution_paths'):
                    print(f"   æ€»è§£å†³æ–¹æ¡ˆè·¯å¾„: {len(self.solution_tree.solution_paths)}")

            return result
        return None

    def enable_conversation_summary(self, summary_config: SummaryConfig, summary_llm=None):
        """å¯ç”¨å¯¹è¯æ‘˜è¦åŠŸèƒ½"""
        self.summary_config = summary_config
        self.summary_llm = summary_llm

        self.memory = SmartSummaryMemory(
            llm=summary_llm,
            max_turns=summary_config.max_turns,
            max_token_limit=summary_config.max_token_limit,
            enabled=summary_config.enabled
        )

        if hasattr(self.memory, 'debug_mode'):
            self.memory.debug_mode = hasattr(self, 'debug_mode') and self.debug_mode

        status = "enabled" if summary_config.enabled else "disabled"
        logger.info(f"âœ… Conversation summary {status} for {self.agent_type} agent")

    def model_init(self, model_config: ModelConfig, model_name=None):
        """åˆå§‹åŒ–æ¨¡å‹ï¼Œæ”¯æŒAPIå’Œæœ¬åœ°è°ƒç”¨"""
        if model_config.model_type == "local":
            # ä½¿ç”¨ç®€åŒ–çš„æœ¬åœ°Agent
            self.local_agent = SimpleLocalAgent(self.agent_type, self.debug_mode)
            self.local_agent.init_model(model_config)
            print(f"âœ… åˆå§‹åŒ– {model_name} æœ¬åœ°æ¨¡å‹: {model_config.model_name}")
        else:
            # APIæ¨¡å‹é…ç½®
            api_kwargs = {
                "temperature": model_config.temperature,
                "timeout": model_config.timeout,
                "max_tokens": model_config.max_tokens,
            }
            self.model = init_chat_model(model_config.model_name, **api_kwargs)

            # ä¸ºAPIæ¨¡å‹æ·»åŠ debugä¸­é—´ä»¶
            if self.debug_mode:
                self._add_debug_middleware()

            print(f"âœ… åˆå§‹åŒ– {model_name} APIæ¨¡å‹: {model_config.model_name}")
            self.middleware_er = MiddlewareFunc(self.model, self.model)

    def _add_debug_middleware(self):
        """ä¸ºAPIæ¨¡å‹æ·»åŠ debugä¸­é—´ä»¶"""

        @wrap_model_call
        def debug_middleware(request: ModelRequest, handler) -> ModelResponse:
            print("\n" + "=" * 80)
            print(f"ğŸ” DEBUG - {self.agent_type.upper()} AGENT API REQUEST:")
            print("=" * 80)

            # æ‰“å°ç³»ç»Ÿæç¤ºè¯
            if hasattr(request, 'system_prompt') and request.system_prompt:
                print("System Prompt:")
                print(request.system_prompt)
                print("-" * 40)

            # æ‰“å°æ¶ˆæ¯å†å²
            messages = request.state.get("messages", [])
            print("Message History:")
            for i, msg in enumerate(messages):
                role = getattr(msg, 'role', type(msg).__name__)
                content = getattr(msg, 'content', str(msg))
                print(f"{i}. {role}: {content}")

            print("=" * 80 + "\n")

            return handler(request)

        # æ·»åŠ åˆ°ä¸­é—´ä»¶åˆ—è¡¨
        if not hasattr(self, 'middleware_list'):
            self.middleware_list = []
        self.middleware_list.append(debug_middleware)

    def get_debug_info(self):
        """è·å–debugä¿¡æ¯"""
        if hasattr(self, 'local_agent') and self.local_agent:
            return {
                "agent_type": self.agent_type,
                "last_prompt": self.local_agent.get_last_prompt(),
                "dialogue_history": self.local_agent.dialogue_history
            }
        else:
            return {
                "agent_type": self.agent_type,
                "debug_mode": self.debug_mode
            }

    def set_correct_answer(self, correct_answer: str):
        """è®¾ç½®æ­£ç¡®ç­”æ¡ˆï¼ˆæ•™å¸ˆagentä½¿ç”¨ï¼‰"""
        self.correct_answer = correct_answer

    def agent_init(self, model_config, prompt_sys_name=None, tools_list=None, context_schema=Context,
                   response_format=ResponseFormat, checkpointer=simple_checkpointer,
                   middleware=None, **kwargs):
        """åˆå§‹åŒ–agenté…ç½®"""
        # åœ¨è§£é¢˜æ ‘æ¨¡å¼ä¸‹ï¼Œç¦ç”¨å¯èƒ½å¯¼è‡´å†²çªçš„å·¥å…·
        if self.use_solution_tree:
            # ä¿ç•™å¿…è¦çš„å·¥å…·ï¼Œç§»é™¤å¯èƒ½å¯¼è‡´å·¥å…·è°ƒç”¨çš„å¤æ‚å·¥å…·
            safe_tools = []
            if tools_list:
                for tool in tools_list:
                    tool_name = getattr(tool, 'name', str(tool))
                    # åªä¿ç•™ç®€å•ã€å®‰å…¨çš„å·¥å…·
                    if any(safe in tool_name.lower() for safe in ['format', 'memory', 'context']):
                        safe_tools.append(tool)
            self.tools = safe_tools
        else:
            self.tools = tools_list if tools_list else []

        self.context_schema = context_schema
        self.response_format = response_format
        self.checkpointer = checkpointer

        # è®¾ç½®åŠŸèƒ½å¼€å…³
        self.max_turns = kwargs.get("max_turns", 5)
        self.parallel_thinking_enabled = kwargs.get("parallel_thinking", False)
        self.socratic_teaching_enabled = kwargs.get("socratic_teaching", False)

        self.solution_tree = kwargs.get("prompt_solution_tree", False)
        solution_tree_prompt = kwargs.get("prompt_solution_tree", None)
        if solution_tree_prompt:
            self.prompt_sys = solution_tree_prompt
            self.use_solution_tree = True
        else:
            self.prompt_sys = prompt_sys_name
            self.use_solution_tree = False

        # æ ¹æ®åŠŸèƒ½å¼€å…³è°ƒæ•´ç³»ç»Ÿæç¤ºè¯
        self._adjust_prompt_based_on_settings(self.prompt_sys)

        # åœ¨è§£é¢˜æ ‘æ¨¡å¼ä¸‹ï¼Œç®€åŒ–ä¸­é—´ä»¶é…ç½®
        if middleware and self.use_solution_tree:
            # ç§»é™¤å¯èƒ½å¯¼è‡´å·¥å…·è°ƒç”¨å†²çªçš„ä¸­é—´ä»¶
            safe_middleware = []
            for m in middleware:
                if m not in ['handle_tool_errors', 'dynamic']:
                    safe_middleware.append(m)
            middleware = safe_middleware

        if middleware:
            for m in middleware:
                if m == 'dynamic':
                    self.middleware_list.append(self.middleware_er.middleware_dynamic_model_selection())
                elif m == 'handle_tool_errors':
                    self.middleware_list.append(self.middleware_er.middleware_handle_tool_errors())
                elif m == 'user_role_prompt':
                    self.middleware_list.append(self.middleware_er.middleware_user_role_prompt())
                elif m == 'CustomMiddleware':
                    self.middleware_list.append(self.custom_middleware_er())

        if model_config.model_type == "local":
            # æœ¬åœ°æ¨¡å‹åˆå§‹åŒ–
            self.local_agent.set_system_prompt(self.prompt_sys)
            if self.tools:
                self.local_agent.add_tools(self.tools)
            if self.context:
                self.local_agent.set_context(**self.context.__dict__)
        else:
            # å¯¹äºAPIæ¨¡å‹ï¼Œåœ¨è§£é¢˜æ ‘æ¨¡å¼ä¸‹ä½¿ç”¨ç®€åŒ–çš„agentåˆ›å»ºæ–¹å¼
            if self.use_solution_tree:
                try:
                    self.agent = self._create_simple_agent()
                    if self.debug_mode:
                        print("âœ… ä½¿ç”¨ç®€åŒ–Agentï¼ˆè§£é¢˜æ ‘æ¨¡å¼ï¼‰")
                except Exception as e:
                    if self.debug_mode:
                        print(f"âš ï¸ ç®€åŒ–Agentåˆ›å»ºå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†Agent: {e}")
                    self.agent = create_agent(
                        model=self.model,
                        system_prompt=self.prompt_sys,
                        tools=self.tools,  # ä½¿ç”¨è¿‡æ»¤åçš„å·¥å…·
                        context_schema=self.context_schema,
                        response_format=self.response_format,
                        checkpointer=self.checkpointer,
                        middleware=self.middleware_list,
                    )
            else:
                self.agent = create_agent(
                    model=self.model,
                    system_prompt=self.prompt_sys,
                    tools=self.tools,  # ä½¿ç”¨è¿‡æ»¤åçš„å·¥å…·
                    context_schema=self.context_schema,
                    response_format=self.response_format,
                    checkpointer=self.checkpointer,
                    middleware=self.middleware_list,
                )

    def _create_simple_agent(self):
        """åˆ›å»ºä¸åŒ…å«å·¥å…·çš„ç®€å•å¯¹è¯agent"""
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.messages import AIMessage, HumanMessage

        # åˆ›å»ºç®€å•çš„å¯¹è¯é“¾
        prompt = ChatPromptTemplate.from_messages([
            ("system", self.prompt_sys),
            ("user", "{input}")
        ])

        # ç®€å•çš„å¯¹è¯é“¾ï¼Œä¸åŒ…å«ä»»ä½•å·¥å…·
        chain = prompt | self.model

        # åŒ…è£…æˆç±»ä¼¼agentçš„æ¥å£
        class SimpleAgentWrapper:
            def __init__(self, chain, debug_mode=False):
                self.chain = chain
                self.debug_mode = debug_mode

            def invoke(self, input_dict, config=None, context=None):
                try:
                    # æå–ç”¨æˆ·è¾“å…¥
                    messages = input_dict.get("messages", [])
                    user_input = ""
                    for msg in reversed(messages):
                        if hasattr(msg, 'content'):
                            user_input = msg.content
                            break
                        elif isinstance(msg, dict) and 'content' in msg:
                            user_input = msg['content']
                            break

                    if not user_input:
                        user_input = str(messages)
                    # è°ƒç”¨æ¨¡å‹
                    response_content = self.chain.invoke({"input": user_input})

                    # ç¡®ä¿response_contentæ˜¯å­—ç¬¦ä¸²
                    if hasattr(response_content, 'content'):
                        response_text = response_content.content
                    else:
                        response_text = str(response_content)

                    # è¿”å›ä¸ç°æœ‰ä»£ç å…¼å®¹çš„æ ¼å¼
                    return {
                        'messages': [{'role': 'assistant', 'content': response_text}],
                        'structured_response': SimpleNamespace(main_response=response_text)
                    }

                except Exception as e:
                    if self.debug_mode:
                        print(f"âŒ ç®€å•Agentè°ƒç”¨é”™è¯¯: {e}")
                    return {
                        'messages': [{'role': 'assistant', 'content': "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ã€‚è¯·é‡æ–°æé—®ã€‚"}],
                        'structured_response': SimpleNamespace(main_response="æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ã€‚è¯·é‡æ–°æé—®ã€‚")
                    }

            def stream(self, input_dict, config=None, context=None, stream_mode="values"):
                # ç®€å•çš„æµå¼å“åº”å®ç°
                result = self.invoke(input_dict, config, context)
                yield result

        return SimpleAgentWrapper(chain, self.debug_mode)

    def _adjust_prompt_based_on_settings(self, prompt_sys):
        """æ ¹æ®åŠŸèƒ½å¼€å…³è°ƒæ•´ç³»ç»Ÿæç¤ºè¯"""
        if isinstance(prompt_sys, str):
            # å¦‚æœæ˜¯é¢„å®šä¹‰çš„æç¤ºè¯å˜é‡åï¼Œå°è¯•è·å–å…¶å€¼
            if prompt_sys in globals():
                base_prompt = globals()[prompt_sys]
            else:
                base_prompt = prompt_sys
        else:
            base_prompt = str(prompt_sys)

        if self.agent_type == "student":

            if self.parallel_thinking_enabled:
                base_prompt += "\n\n" + PARALLEL_THINKING_PROMPT

            self.prompt_sys = base_prompt

        elif self.agent_type == "teacher":

            if self.correct_answer:
                base_prompt += f"\n\nYou know the correct solution is: {self.correct_answer}. But DO NOT reveal this answer directly to the student. Guide them to discover it themselves."

            if self.socratic_teaching_enabled:
                base_prompt += "\n\n" + SOCRATIC_TEACHING_PROMPT

            self.prompt_sys = base_prompt

        elif self.agent_type == "expert_student":

            self.prompt_sys = base_prompt

    def config_create(self, key_i, value_i):
        self.agent_config = {"configurable": {key_i: value_i}, "recursion_limit": 100}

    def context_set(self, **kwargs):
        self.context = self.context_schema(**kwargs)

    def chat_once(self, user_input, response_type='invoke', silence=False, **kwargs):
        if response_type == 'invoke':
            self.agent_response_invoke(user_input, **kwargs)
            if not silence:
                self.agent_output()
        else:
            self.agent_response_stream(user_input, **kwargs)

    def _extract_response_text(self, response):
        """ä»å“åº”å¯¹è±¡ä¸­æå–æ–‡æœ¬å†…å®¹"""
        if isinstance(response, str):
            return response

        if hasattr(response, 'get') and isinstance(response, dict):
            # å­—å…¸ç±»å‹çš„å“åº”
            if 'structured_response' in response and hasattr(response['structured_response'], 'main_response'):
                return response['structured_response'].main_response
            elif 'messages' in response and response['messages']:
                last_msg = response['messages'][-1]
                if hasattr(last_msg, 'content'):
                    return last_msg.content
                elif isinstance(last_msg, dict) and 'content' in last_msg:
                    return last_msg['content']
            # å°è¯•ç›´æ¥è·å–content
            elif 'content' in response:
                return response['content']

        if hasattr(response, 'structured_response') and hasattr(response.structured_response, 'main_response'):
            return response.structured_response.main_response
        elif hasattr(response, 'content'):
            return response.content

        # æœ€åå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        return str(response)

    def multi_agent_chat_explicit(self, teacher_agent, problem: str, raw_problem: str, correct_answer: str,
                                  dialogue_record: DialogueRecord, **kwargs):
        """æ¨¡å¼1: æ˜¾å¼äº¤äº’ - æ•™å¸ˆå’Œå­¦ç”Ÿç›´æ¥å¯¹è¯"""
        if dialogue_record.debug_mode:
            logger.info(f"\nğŸ¯ å¼€å§‹è§£é¢˜: {problem}")
            logger.info("=" * 50)

        # è®¾ç½®æ•™å¸ˆçŸ¥é“çš„æ­£ç¡®ç­”æ¡ˆ
        teacher_agent.set_correct_answer(correct_answer)
        teacher_agent._adjust_prompt_based_on_settings(teacher_agent.prompt_sys)  # é‡æ–°è°ƒæ•´æç¤ºè¯

        # é‡ç½®å¯¹è¯å†å²
        self.dialogue_history = []
        self.current_turn = 0

        try:
            # å­¦ç”Ÿé¦–æ¬¡å°è¯•
            student_response_obj = self._invoke_agent(problem)
            # ç¡®ä¿æ­£ç¡®æå–å“åº”æ–‡æœ¬
            student_response = self._extract_response_text(student_response_obj)
            self.dialogue_history.append(("student", student_response))

            # è®°å½•å­¦ç”Ÿç¬¬ä¸€æ­¥è§£é¢˜æ­¥éª¤
            if self.use_solution_tree and self.solution_tree:
                self.record_student_step(student_response, method_used=self._detect_method_from_response(student_response))
                if dialogue_record.debug_mode:
                    logger.info("ğŸ“ å·²è®°å½•å­¦ç”Ÿç¬¬ä¸€æ¬¡è§£é¢˜æ­¥éª¤")

            # åˆ†æå­¦ç”Ÿå›å¤
            parallel_count, path_count = dialogue_record.analyze_student_response(student_response)

            # è®°å½•ç¬¬ä¸€è½®å¯¹è¯
            dialogue_record.add_turn({
                "turn": 1,
                "student_response": student_response,
                "teacher_response": "",
                "teacher_intent": "initial_response",
                "parallel_thinking_count": parallel_count,
                "thinking_paths_count": path_count,
                "answer_leakage": False
            })

            if dialogue_record.debug_mode:
                logger.info(f"ğŸ‘¨â€ğŸ“ å­¦ç”Ÿ [è½®æ¬¡1]: {student_response}")

            self.correct_answer = extract_answer(correct_answer)
            dialogue_record.first_correct = self._has_correct_answer(extract_answer(student_response), self.correct_answer)

            # å¦‚æœç¬¬ä¸€æ¬¡å°±ç­”å¯¹äº†ï¼Œç›´æ¥å®Œæˆè§£é¢˜
            if dialogue_record.first_correct:
                if self.use_solution_tree and self.solution_tree:
                    self.complete_student_solution(success=True, final_answer=student_response)
                    if dialogue_record.debug_mode:
                        logger.info("âœ… å­¦ç”Ÿç¬¬ä¸€æ¬¡å°±ç­”å¯¹äº†ï¼Œè§£é¢˜è·¯å¾„å·²å®Œæˆ")
                dialogue_record.correct = True
                dialogue_record.final_student_answer = student_response
                return student_response, self.correct_answer, dialogue_record

            for turn in range(self.max_turns):
                # æ£€æŸ¥å­¦ç”Ÿæ˜¯å¦å¾—å‡ºæ­£ç¡®ç­”æ¡ˆ
                self.student_answer = extract_answer(student_response)
                if self._has_correct_answer(self.student_answer, self.correct_answer):
                    if dialogue_record.debug_mode:
                        logger.info("ğŸ‰ å­¦ç”Ÿå¾—å‡ºæ­£ç¡®ç­”æ¡ˆ!")
                    dialogue_record.correct = True
                    dialogue_record.final_student_answer = self.student_answer

                    # å®Œæˆè§£é¢˜è·¯å¾„
                    if self.use_solution_tree and self.solution_tree:
                        self.complete_student_solution(success=True, final_answer=student_response)
                        if dialogue_record.debug_mode:
                            logger.info("âœ… å­¦ç”Ÿç­”å¯¹äº†ï¼Œè§£é¢˜è·¯å¾„å·²å®Œæˆ")

                    return self.student_answer, self.correct_answer, dialogue_record

                current_turn = turn + 2  # ä»ç¬¬äºŒè½®å¼€å§‹

                if dialogue_record.debug_mode:
                    logger.info(f"\nğŸ”„ ç¬¬ {current_turn} è½®å¯¹è¯:")
                    logger.info("-" * 30)

                # æ•™å¸ˆå›åº”
                teacher_input = self._format_teacher_input(problem, self.dialogue_history)
                if dialogue_record.debug_mode:
                    logger.info(f"ğŸ‘¨â€ğŸ« teacher_input [è½®æ¬¡{current_turn}]: {teacher_input}")
                teacher_response_obj = teacher_agent._invoke_agent(teacher_input)
                teacher_response = self._extract_response_text(teacher_response_obj)
                self.dialogue_history.append(("teacher", teacher_response))

                # æ£€æŸ¥ç­”æ¡ˆæ³„éœ²
                leakage_detected = dialogue_record.check_answer_leakage(teacher_response)

                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»“æŸå¯¹è¯
                if self._should_end_dialogue(teacher_response):
                    if dialogue_record.debug_mode:
                        logger.info("âœ… æ•™å¸ˆè®¤ä¸ºè§£é¢˜å®Œæˆ")
                    dialogue_record.final_student_answer = self.student_answer

                    # å®Œæˆè§£é¢˜è·¯å¾„ï¼ˆå¯èƒ½æˆåŠŸæˆ–å¤±è´¥ï¼‰
                    if self.use_solution_tree and self.solution_tree:
                        success = self._has_correct_answer(self.student_answer, self.correct_answer)
                        self.complete_student_solution(success=success, final_answer=student_response)
                        if dialogue_record.debug_mode:
                            status = "æˆåŠŸ" if success else "å¤±è´¥"
                            logger.info(f"ğŸ“ æ•™å¸ˆç»“æŸå¯¹è¯ï¼Œè§£é¢˜è·¯å¾„{status}")

                    return self.student_answer, self.correct_answer, dialogue_record

                if dialogue_record.debug_mode:
                    logger.info(f"ğŸ‘¨â€ğŸ« æ•™å¸ˆ [è½®æ¬¡{current_turn}]: {teacher_response}")
                if leakage_detected:
                    if dialogue_record.debug_mode:
                        logger.info("âš ï¸  æ£€æµ‹åˆ°ç­”æ¡ˆæ³„éœ²!")

                # å­¦ç”Ÿå›åº”
                student_input = self._format_student_input(raw_problem, self.dialogue_history)
                if dialogue_record.debug_mode:
                    logger.info(f"ğŸ‘¨â€ğŸ“ student_input [è½®æ¬¡{current_turn}]: {student_input}")
                student_response_obj = self._invoke_agent(student_input)
                student_response = self._extract_response_text(student_response_obj)
                self.dialogue_history.append(("student", student_response))

                # è®°å½•å­¦ç”Ÿè§£é¢˜æ­¥éª¤
                if self.use_solution_tree and self.solution_tree:
                    method_used = self._detect_method_from_response(student_response)
                    self.record_student_step(student_response, method_used=method_used)
                    if dialogue_record.debug_mode:
                        logger.info(f"ğŸ“ å·²è®°å½•å­¦ç”Ÿç¬¬{current_turn}æ­¥è§£é¢˜æ­¥éª¤ï¼Œæ–¹æ³•: {method_used}")

                # åˆ†æå­¦ç”Ÿå›å¤
                parallel_count, path_count = dialogue_record.analyze_student_response(student_response)

                # è®°å½•æœ¬è½®å¯¹è¯
                dialogue_record.add_turn({
                    "turn": current_turn,
                    "student_response": student_response,
                    "teacher_response": teacher_response,
                    "teacher_intent": self._analyze_teacher_intent(teacher_response),
                    "parallel_thinking_count": parallel_count,
                    "thinking_paths_count": path_count,
                    "answer_leakage": leakage_detected
                })

                if dialogue_record.debug_mode:
                    logger.info(f"ğŸ‘¨â€ğŸ“ å­¦ç”Ÿ [è½®æ¬¡{current_turn}]: {student_response}")

                self.current_turn = current_turn

            # è®¾ç½®æœ€ç»ˆç­”æ¡ˆ
            dialogue_record.final_student_answer = self.student_answer

            # å®Œæˆè§£é¢˜è·¯å¾„ï¼ˆè¾¾åˆ°æœ€å¤§è½®æ¬¡ï¼‰
            if self.use_solution_tree and self.solution_tree:
                success = self._has_correct_answer(self.student_answer, self.correct_answer)
                self.complete_student_solution(success=success, final_answer=student_response)
                if dialogue_record.debug_mode:
                    status = "æˆåŠŸ" if success else "å¤±è´¥"
                    logger.info(f"ğŸ“ è¾¾åˆ°æœ€å¤§è½®æ¬¡ï¼Œè§£é¢˜è·¯å¾„{status}")

            return self._get_final_answer(), self.correct_answer, dialogue_record

        except Exception as e:
            if dialogue_record.debug_mode:
                logger.error(f"âŒ å¯¹è¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            # åœ¨å‡ºé”™æ—¶ä¹Ÿè¦å®Œæˆè§£é¢˜è·¯å¾„
            if self.use_solution_tree and self.solution_tree:
                self.complete_student_solution(success=False, final_answer="é”™è¯¯")
                if dialogue_record.debug_mode:
                    logger.info("ğŸ“ å¯¹è¯å‡ºé”™ï¼Œè§£é¢˜è·¯å¾„æ ‡è®°ä¸ºå¤±è´¥")

            # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„é”™è¯¯å“åº”
            dialogue_record.final_student_answer = "æŠ±æ­‰ï¼Œè§£é¢˜è¿‡ç¨‹ä¸­å‡ºç°äº†æŠ€æœ¯é—®é¢˜ã€‚"
            return "æŠ±æ­‰ï¼Œè§£é¢˜è¿‡ç¨‹ä¸­å‡ºç°äº†æŠ€æœ¯é—®é¢˜ã€‚", self.correct_answer, dialogue_record

    def _analyze_teacher_intent(self, teacher_response: str) -> str:
        """åˆ†ææ•™å¸ˆå›å¤çš„æ„å›¾"""
        response_lower = teacher_response.lower()

        if any(word in response_lower for word in ["question", "ask", "what do you think"]):
            return "socratic_questioning"
        elif any(word in response_lower for word in ["hint", "suggest", "try"]):
            return "providing_hint"
        elif any(word in response_lower for word in ["correct", "right", "good"]):
            return "positive_feedback"
        elif any(word in response_lower for word in ["wrong", "incorrect", "mistake"]):
            return "correcting_error"
        elif any(word in response_lower for word in ["explain", "concept", "principle"]):
            return "explaining_concept"
        else:
            return "general_guidance"

    def _detect_method_from_response(self, response: str) -> str:
        """ä»å­¦ç”Ÿå“åº”ä¸­æ£€æµ‹ä½¿ç”¨çš„æ–¹æ³•"""
        if not response:
            return "unknown"

        response_lower = response.lower()

        # æ£€æµ‹æ–¹æ³•ç±»å‹
        if any(word in response_lower for word in ["equation", "solve for", "variable", "x =", "let x", "algebra"]):
            return "algebraic"
        elif any(word in response_lower for word in
                 ["diagram", "graph", "shape", "angle", "area", "triangle", "circle"]):
            return "geometric"
        elif any(word in response_lower for word in
                 ["calculate", "compute", "number", "digit", "sum", "total", "multiply"]):
            return "computational"
        elif any(word in response_lower for word in ["logic", "reason", "therefore", "because", "since", "if then"]):
            return "logical"
        elif any(word in response_lower for word in ["guess", "try", "maybe", "perhaps", "i think"]):
            return "trial_and_error"
        else:
            return "general"

    def multi_agent_chat_tool_based(self, problem: str, correct_answer: str,
                                    dialogue_record: DialogueRecord, **kwargs):
        """æ¨¡å¼2: å·¥å…·è°ƒç”¨ - å­¦ç”Ÿä½œä¸ºcontrollerè°ƒç”¨æ•™å¸ˆå·¥å…·"""
        print(f"\nğŸ¯ å¼€å§‹å·¥å…·è°ƒç”¨æ¨¡å¼è§£é¢˜: {problem}")
        print("=" * 50)

        # é…ç½®å­¦ç”Ÿagentä»¥åŒ…å«æ•™å¸ˆå·¥å…·ï¼ˆåŒ…å«æ­£ç¡®ç­”æ¡ˆï¼‰
        teacher_tool = self._create_teacher_tool(correct_answer)
        self.tools.append(teacher_tool)

        # é‡æ–°åˆå§‹åŒ–agentä»¥åŒ…å«æ–°å·¥å…·
        self.agent = create_agent(
            model=self.model,
            system_prompt=self.prompt_sys + "\n\nYou can use the ask_teacher tool when you need guidance.",
            tools=self.tools,
            context_schema=self.context_schema,
            response_format=self.response_format,
            checkpointer=self.checkpointer,
            middleware=self.middleware_list,
        )

        # å­¦ç”Ÿè‡ªä¸»è§£é¢˜ï¼Œå¯åœ¨éœ€è¦æ—¶è°ƒç”¨æ•™å¸ˆå·¥å…·
        final_response = self._invoke_agent(problem)

        # åˆ†æå­¦ç”Ÿå›å¤
        parallel_count, path_count = dialogue_record.analyze_student_response(final_response)

        # è®°å½•å·¥å…·è°ƒç”¨æ¨¡å¼çš„å¯¹è¯ï¼ˆç®€åŒ–ä¸ºå•è½®ï¼‰
        dialogue_record.add_turn({
            "turn": 1,
            "student_response": final_response,
            "teacher_response": "Tool-based interaction",
            "teacher_intent": "tool_guidance",
            "parallel_thinking_count": parallel_count,
            "thinking_paths_count": path_count,
            "answer_leakage": False
        })

        dialogue_record.final_student_answer = final_response

        print(f"ğŸ‘¨â€ğŸ“ å­¦ç”Ÿæœ€ç»ˆå›ç­”: {final_response}")

        return final_response

    def _create_teacher_tool(self, correct_answer: str):
        """åˆ›å»ºæ•™å¸ˆå·¥å…·ä¾›å­¦ç”Ÿè°ƒç”¨ï¼ˆåŒ…å«æ­£ç¡®ç­”æ¡ˆçŸ¥è¯†ï¼‰"""
        from langchain.tools import tool

        @tool
        def ask_teacher(question: str) -> str:
            """Ask the teacher for guidance on a specific question or problem.

            The teacher knows the correct answer but will not reveal it directly.
            Instead, the teacher will provide helpful guidance and hints.

            Use this tool when:
            - You're stuck on a math problem
            - You need clarification on concepts
            - You want to check your approach
            - You need step-by-step guidance
            """
            # åŸºäºæ­£ç¡®ç­”æ¡ˆæä¾›å¼•å¯¼æ€§æç¤º
            guidance_responses = [
                "Let me guide you through this step by step. What part are you finding difficult?",
                "Good attempt! Let's break this down. What's your current approach?",
                "I see where you might be confused. Let me ask you a question to help you think differently...",
                "Remember the key concept here is to identify the known values and what you're solving for.",
                "Try breaking the problem into smaller parts. What's the first step you would take?",
                "Consider what information you have and what you're trying to find. How can you connect them?",
                "That's a good start. Now think about what mathematical operations might be needed here."
            ]
            import random
            return random.choice(guidance_responses)

        return ask_teacher

    def _invoke_agent(self, input_text):
        """è°ƒç”¨agentå¹¶è¿”å›å“åº”"""
        if self.memory and self.memory.enabled:
            self.memory.add_message("user", input_text)

        if hasattr(self, 'local_agent') and self.local_agent:
            response = self._invoke_local_with_memory(input_text)
        else:
            response = self._invoke_api_with_memory(input_text)

        # è®°å½•åŠ©æ‰‹å“åº”åˆ°å†…å­˜
        if self.memory and self.memory.enabled and response:
            self.memory.add_message("assistant", response)

        return response

    def _invoke_local_with_memory(self, input_text):
        """æœ¬åœ°æ¨¡å‹è°ƒç”¨ï¼ˆæ”¯æŒå†…å­˜å’Œè§£é¢˜æ ‘ï¼‰"""
        # æ„å»ºåŒ…å«å†…å­˜ä¸Šä¸‹æ–‡å’Œè§£é¢˜æ ‘ä¸Šä¸‹æ–‡çš„è¾“å…¥
        memory_context = ""
        if self.memory and self.memory.enabled:
            memory_context = self.memory.get_context()

        # ä½¿ç”¨å¢å¼ºçš„å®Œæ•´æç¤ºè¯æ„å»ºæ–¹æ³•
        full_input = self._build_full_prompt(input_text, memory_context)

        response = self.local_agent.invoke({
            "messages": [{"role": "user", "content": full_input}]
        })
        return response['structured_response'].main_response

    def _invoke_api_with_memory(self, input_text):
        """APIæ¨¡å‹è°ƒç”¨ï¼ˆæ”¯æŒå†…å­˜å’Œè§£é¢˜æ ‘ï¼‰"""
        memory_context = ""
        if self.memory and self.memory.enabled:
            memory_context = self.memory.get_context()

        # ä½¿ç”¨å¢å¼ºçš„å®Œæ•´æç¤ºè¯æ„å»ºæ–¹æ³•
        full_input = self._build_full_prompt(input_text, memory_context)

        try:
            # è°ƒç”¨agent
            response = self.agent.invoke({
                "messages": [{"role": "user", "content": full_input}]
            }, config=self.agent_config, context=self.context)

            # å¤„ç†å·¥å…·è°ƒç”¨æƒ…å†µ
            if response and 'messages' in response:
                # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
                last_message = response['messages'][-1]

                # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†å®ƒä»¬
                if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                    if self.debug_mode:
                        logger.info(f"ğŸ› ï¸ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {[tc['name'] for tc in last_message.tool_calls]}")

                    # å¯¹äºå¤šæ™ºèƒ½ä½“å¯¹è¯ï¼Œæˆ‘ä»¬æš‚æ—¶ç®€åŒ–å¤„ç†ï¼šä¸æ‰§è¡Œå®é™…å·¥å…·è°ƒç”¨
                    # è€Œæ˜¯è¿”å›ä¸€ä¸ªæç¤ºä¿¡æ¯
                    tool_response = "åœ¨å½“å‰å¯¹è¯æ¨¡å¼ä¸‹ï¼Œå·¥å…·è°ƒç”¨åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ã€‚è¯·ç›´æ¥æä¾›æŒ‡å¯¼æˆ–å›ç­”ã€‚"

                    # åˆ›å»ºå·¥å…·å“åº”æ¶ˆæ¯
                    tool_messages = []
                    for tool_call in last_message.tool_calls:
                        tool_messages.append({
                            "role": "tool",
                            "content": tool_response,
                            "tool_call_id": tool_call['id']
                        })

                    # å¦‚æœéœ€è¦ç»§ç»­å¤„ç†å·¥å…·è°ƒç”¨ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šé€»è¾‘
                    # ä½†ç›®å‰æˆ‘ä»¬è¿”å›ä¸€ä¸ªç®€åŒ–å“åº”
                    return tool_response

                # æ­£å¸¸è¿”å›ç»“æ„åŒ–å“åº”
                return response['structured_response'].main_response

            return ""

        except Exception as e:
            if self.debug_mode:
                logger.info(f"âŒ APIè°ƒç”¨é”™è¯¯: {e}")

            # å¦‚æœå‡ºç°å·¥å…·è°ƒç”¨ç›¸å…³é”™è¯¯ï¼Œå›é€€åˆ°ç®€å•å“åº”
            if "tool_calls" in str(e) or "tool_call_id" in str(e):
                return "æˆ‘ç†è§£æ‚¨éœ€è¦å¸®åŠ©ï¼Œä½†åœ¨å½“å‰è®¾ç½®ä¸‹ï¼Œè¯·ç›´æ¥æå‡ºæ‚¨çš„é—®é¢˜æˆ–å›°æƒ‘ã€‚"

            raise e

    def _build_input_with_memory(self, input_text, memory_context):
        """æ„å»ºåŒ…å«å†…å­˜ä¸Šä¸‹æ–‡çš„è¾“å…¥"""
        if memory_context:
            return f"{memory_context}\n\nCurrent Question: {input_text}\n\nPlease respond based on the conversation context:"
        else:
            return input_text

    def _format_teacher_input(self, problem, history):
        """æ ¼å¼åŒ–æ•™å¸ˆè¾“å…¥"""
        context = f"Problem: {problem}\n\nDialogue History:\n"
        for role, content in history:
            context += f"{role}: {content}\n\n"
        context += 'System: '
        # æ ¹æ®è‹æ ¼æ‹‰åº•æ•™å­¦å¼€å…³è°ƒæ•´æç¤º
        if self.socratic_teaching_enabled:
            context += "Please use Socratic questioning to guide the student. Ask thought-provoking questions rather than giving direct answers."
        else:
            context += "Please provide appropriate guidance to help the student solve the problem."

        return context

    def get_memory_stats(self):
        """è·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if self.memory:
            return self.memory.get_stats()
        return {"enabled": False}

    def clear_memory(self):
        """æ¸…ç©ºå†…å­˜"""
        if self.memory:
            self.memory.clear()

    def _format_student_input(self, problem, history):
        """æ ¼å¼åŒ–å­¦ç”Ÿè¾“å…¥"""
        context = f"Original Problem: {problem}\n\nDialogue History:\n"
        for role, content in history:
            context += f"{role}: {content}\n\n"
        context += 'System: '
        # æ ¹æ®å¹¶è¡Œæ€è€ƒå¼€å…³è°ƒæ•´æç¤º
        if self.parallel_thinking_enabled:
            context += STUDENT_STANDARD_PARALLEL_THINKING_PROMPT.format(question=problem)

        context += "Please continue solving the problem or respond to the teacher's guidance:"
        return context

    def _should_end_dialogue(self, teacher_response):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»“æŸå¯¹è¯"""
        end_phrases = ["finished", "completed"]
        return any(phrase in teacher_response.lower() for phrase in end_phrases)

    def _has_correct_answer(self, student_answer, correct_answer):
        """åˆ¤æ–­å­¦ç”Ÿæ˜¯å¦å¾—å‡ºæ­£ç¡®ç­”æ¡ˆ"""
        # ä½¿ç”¨ç°æœ‰çš„ç­”æ¡ˆæå–å’Œæ¯”è¾ƒé€»è¾‘
        return is_equiv_MATH(correct_answer, student_answer)

    def _get_final_answer(self):
        """è·å–æœ€ç»ˆç­”æ¡ˆ"""
        student_final_answer = ""
        if self.dialogue_history:
            for role, content in self.dialogue_history:
                if role == "student":
                    student_final_answer = content[1]
            return student_final_answer
        return ""

    def agent_response_invoke(self, user_input, **kwargs):
        self.response = self.agent.invoke({
            "messages": [{"role": "user", "content": user_input}]
        }, config=self.agent_config, context=self.context, **kwargs)

    def agent_response_stream(self, user_input, **kwargs):
        for chunk in self.agent.stream({
            "messages": [{"role": "user", "content": user_input}]
        }, config=self.agent_config, context=self.context, stream_mode="values", **kwargs):
            latest_message = chunk["messages"][-1]
            if latest_message.content:
                print(f"Agent: {latest_message.content}")
            elif latest_message.tool_calls:
                print(f"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}")

    def agent_output(self, all_messages=False):
        if all_messages:
            for i in self.response['messages']:
                print(f"{type(i)}: {i.content}")
        else:
            print(f"{type(self.response['messages'][-1])}: {self.response['structured_response'].main_response}")

    def get_dialogue_summary(self):
        """è·å–å¯¹è¯æ‘˜è¦"""
        summary = f"Dialogue Summary ({self.current_turn} turns):\n"
        for i, (role, content) in enumerate(self.dialogue_history):
            summary += f"Turn {i + 1} ({role}): {content[:100]}...\n"
        return summary


class ExpertStudentAgent(SimpleAgent):
    """å­¦éœ¸Agent"""

    def __init__(self, debug_mode=False):
        super().__init__(agent_type="expert_student", debug_mode=debug_mode)

    def generate_solution_tree(self, problem):
        """ç”Ÿæˆè§£é¢˜æ ‘"""
        prompt = TREE_GENERATE_PROMPT.format(problem)

        response = self._invoke_agent(prompt)
        return self._parse_solution_tree(response, problem)

    def _parse_solution_tree(self, response, problem):
        """è§£æè§£é¢˜æ ‘å“åº”"""
        solution_tree = SolutionTree(problem)

        try:
            # ç®€å•çš„è§£æé€»è¾‘ - åœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„è§£æ
            if "<SolutionTree>" in response:
                # æå–è§£å†³æ–¹æ¡ˆè·¯å¾„
                paths_section = response.split("<SolutionPaths>")[1].split("</SolutionPaths>")[0]
                path_blocks = paths_section.split("</Path>")

                for block in path_blocks:
                    if "<Path" in block:
                        # æå–è·¯å¾„ä¿¡æ¯
                        method = self._extract_site_tag(block, "method")
                        complexity = self._extract_site_tag(block, "complexity")
                        innovation = self._extract_site_tag(block, "innovation")

                        # æå–æ­¥éª¤
                        steps = []
                        intermediate_answers = []
                        step_parts = block.split("<Step")
                        for step_part in step_parts[1:]:
                            if ">" in step_part and "</Step>" in step_part:
                                step_content = step_part.split(">", 1)[1].split("</Step>")[0]
                                steps.append(step_content)
                            if "<IntermediateAnswer>" in step_part and "</IntermediateAnswer>" in step_part:
                                intermediate_content = \
                                step_part.split("<IntermediateAnswer>", 1)[1].split("</IntermediateAnswer>")[0]
                                intermediate_answers.append(intermediate_content)

                        # æå–æœ€ç»ˆç­”æ¡ˆ
                        final_answer = self._extract_xml_tag(block, "FinalAnswer")

                        solution_tree.add_expert_path({
                            "method": method,
                            "complexity": complexity,
                            "innovation": innovation,
                            "steps": steps,
                            "intermediate_answers": intermediate_answers,
                            "final_answer": final_answer
                        })

        except Exception as e:
            print(f"âŒ Error parsing solution tree: {e}")
            # å¦‚æœè§£æå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„è§£å†³æ–¹æ¡ˆè·¯å¾„
            solution_tree.add_expert_path({
                "method": "algebraic",
                "complexity": "medium",
                "innovation": "medium",
                "steps": ["Apply standard algebraic approach", "Solve step by step"],
                "intermediate_answers": [],
                "final_answer": "[[Answer will be determined]]"
            })

        return solution_tree

    def _extract_xml_tag(self, text, tag_name):
        """æå–XMLæ ‡ç­¾å†…å®¹"""
        start_tag = f"<{tag_name}>"
        end_tag = f"</{tag_name}>"

        if start_tag in text and end_tag in text:
            return text.split(start_tag)[1].split(end_tag)[0].strip()
        return ""

    def _extract_site_tag(self, text, tag_name):
        """æå–XMLæ ‡ç­¾å†…å®¹"""
        start_tag = f'{tag_name}="'
        end_tag = f'"'

        if start_tag in text and end_tag in text:
            return text.split(start_tag)[1].split(end_tag)[0].strip()
        return ""


# æµ‹è¯•ä»£ç 
def test_dialogue_record():
    """æµ‹è¯•å¯¹è¯è®°å½•åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•å¯¹è¯è®°å½•åŠŸèƒ½...")

    record = DialogueRecord("What is 2+2?", "4")
    record.add_turn({
        "turn": 1,
        "student_response": "I think it's <Parallel>maybe 3</Parallel> or <Path>maybe 4</Path>",
        "teacher_response": "Think about basic addition",
        "teacher_intent": "guidance",
        "parallel_thinking_count": 1,
        "thinking_paths_count": 1,
        "answer_leakage": False
    })

    record.analyze_student_response("Another <Parallel>thought</Parallel>")
    record.check_answer_leakage("The answer is 4")

    assert record.parallel_thinking_count == 2
    assert record.thinking_paths_count == 1
    assert record.leaked_answer == True
    assert record.total_turns == 1

    print("âœ… å¯¹è¯è®°å½•æµ‹è¯•é€šè¿‡!")


def test_experiment_recorder():
    """æµ‹è¯•å®éªŒè®°å½•å™¨"""
    print("ğŸ§ª æµ‹è¯•å®éªŒè®°å½•å™¨...")

    recorder = ExperimentRecorder("test_experiment")

    record1 = DialogueRecord("Problem 1", "Answer 1")
    record1.correct = True
    record1.parallel_thinking_count = 2
    record1.thinking_paths_count = 3
    record1.total_turns = 2

    record2 = DialogueRecord("Problem 2", "Answer 2")
    record2.correct = False
    record2.parallel_thinking_count = 1
    record2.thinking_paths_count = 2
    record2.total_turns = 3

    recorder.add_record(record1)
    recorder.add_record(record2)

    stats = recorder.calculate_statistics()

    assert stats["total_problems"] == 2
    assert stats["accuracy"] == 0.5
    assert stats["avg_turns_per_problem"] == 2.5
    assert stats["avg_parallel_thinking"] == 1.5
    assert stats["avg_thinking_paths"] == 2.5

    print("âœ… å®éªŒè®°å½•å™¨æµ‹è¯•é€šè¿‡!")


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_dialogue_record()
    test_experiment_recorder()
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")


# å…¶ä»–ç±»ä¿æŒä¸å˜...
class CustomState(AgentState):
    user_preferences: dict


class CustomMiddleware(AgentMiddleware):
    state_schema = CustomState

    def before_model(self, state: CustomState, runtime):
        if state.user_preferences.get("style") == "technical":
            return {"system_prompt": "Provide detailed technical explanations with specifications."}
        else:
            return {"system_prompt": "Explain concepts in simple, easy-to-understand terms."}

    def before_tool_call(self, state: CustomState, runtime, tool_name):
        if state.user_preferences.get("verbosity") == "detailed":
            return "technical_search"
        else:
            return "simple_search"


class MiddlewareFunc(object):
    def __init__(self, basic_model, advanced_model):
        self.basic_model = basic_model
        self.advanced_model = advanced_model

    def middleware_handle_tool_errors(self) -> Callable:
        @wrap_tool_call
        def handle_tool_errors(request, handler):
            try:
                return handler(request)
            except Exception as e:
                return ToolMessage(
                    content=f"Tool error: Please check your input and try again. ({str(e)})",
                    tool_call_id=request.tool_call["id"]
                )

        return handle_tool_errors

    def middleware_dynamic_model_selection(self) -> Callable:
        @wrap_model_call
        def dynamic_model_middleware(request: ModelRequest, handler) -> ModelResponse:
            messages = request.state.get("messages", [])
            message_count = len(messages)
            print(f"Debug: Current message count = {message_count}")

            if message_count >= 2:
                user_content = ""
                for msg in reversed(messages):
                    if isinstance(msg, HumanMessage):
                        user_content = msg.content
                        break
                complex_keywords = ['solution', 'problem', 'explain', 'analyze', 'how to', 'why', 'compare']
                is_complex = any(keyword in user_content.lower() for keyword in complex_keywords)
                if is_complex and self.advanced_model:
                    model = self.advanced_model
                    model_name = self.advanced_model.model_name
                    print(f'ğŸ”€ Selected ADVANCED model for complex query: {model_name}')
                else:
                    model = self.basic_model
                    model_name = self.basic_model.model_name
                    print(f'ğŸ”€ Selected BASIC model: {model_name}')
            else:
                model = self.basic_model
                model_name = self.basic_model.model_name
                print(f'ğŸ”€ Selected BASIC model (first message): {model_name}')
            request.model = model
            return handler(request)

        return dynamic_model_middleware

    def middleware_user_role_prompt(self) -> Callable:
        @dynamic_prompt
        def user_role_prompt(request: ModelRequest) -> str:
            user_role = request.runtime.context.user_role
            if user_role == "expert":
                return EXPECT_PROMPT
            elif user_role == "beginner":
                return BEGINNER_PROMPT
            return BASE_PROMPT

        return user_role_prompt



