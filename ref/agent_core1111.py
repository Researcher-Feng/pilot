from typing import Dict, Any, Optional
from types import SimpleNamespace

from v8.utils.evaluator import logger, extract_answer
from v8.function.api_client import create_api_client, RawAPIClient, RawOllamaClient
from v8.utils.MARIO_EVAL.demo import is_equiv_MATH
from v8.function.memory import SmartSummaryMemory, SummaryConfig
from v8.function.solution_tree import SolutionTree
from v8.function.cognitive import StudentCognitiveState
from v8.function.middleware import CustomMiddleware, ModelConfig, MiddlewareFunc
from v8.function.contex_fun import Context
from v8.function.format_fun import ResponseFormat
from v8.function.record_eval import DialogueRecord
from v8.prompt.system import *
from v8.prompt.dialogue_tree_parallel import *
from v8.prompt.dialogue_socratic import *


class SimpleLocalAgent:
    """ç®€åŒ–ç‰ˆæœ¬åœ°Agentï¼Œæ”¯æŒæ ¸å¿ƒåŠŸèƒ½"""

    def __init__(self, agent_type="student", debug_mode=False):
        self.agent_type = agent_type
        self.model = None
        self.system_prompt = ""
        self.context = {}
        self.dialogue_history = []
        self.debug_mode = debug_mode
        self.last_full_prompt = ""  # è®°å½•æœ€åä¸€æ¬¡å®Œæ•´prompt

    def init_model(self, model_config):
        """åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹"""
        self.model = create_api_client(model_config)

    def set_system_prompt(self, prompt):
        """è®¾ç½®ç³»ç»Ÿæç¤ºè¯"""
        self.system_prompt = prompt

    def set_context(self, **kwargs):
        """è®¾ç½®ä¸Šä¸‹æ–‡"""
        self.context.update(kwargs)

    def invoke(self, input_dict, config=None, context=None):
        """è°ƒç”¨Agent"""
        user_input = self._extract_input(input_dict)
        full_prompt = user_input

        # æ„å»ºå®Œæ•´æç¤ºè¯
        # full_prompt = self._build_full_prompt(full_prompt)
        self.last_full_prompt = full_prompt  # ä¿å­˜å®Œæ•´prompt

        # Debugè¾“å‡º
        if self.debug_mode:
            logger.info("\n" + "=" * 80)
            logger.info(f"ğŸ” DEBUG - {self.agent_type.upper()} AGENT FULL PROMPT:")
            logger.info("=" * 80)
            logger.info(full_prompt)
            logger.info("=" * 80 + "\n")

        # è°ƒç”¨æ¨¡å‹ - ä½¿ç”¨æ¶ˆæ¯æ ¼å¼
        messages = [{"role": "user", "content": full_prompt}]
        if self.system_prompt:
            messages.insert(0, {"role": "system", "content": self.system_prompt})
        
        response = self.model.invoke(messages)

        # è®°å½•å¯¹è¯å†å²
        self.dialogue_history.append(("user", user_input))
        self.dialogue_history.append(("assistant", response['structured_response'].main_response))

        return response

    def get_last_prompt(self):
        """è·å–æœ€åä¸€æ¬¡çš„å®Œæ•´prompt"""
        return self.last_full_prompt

    def _extract_input(self, input_dict):
        """æå–è¾“å…¥æ–‡æœ¬"""
        messages = input_dict.get('messages', [])
        for msg in reversed(messages):
            if hasattr(msg, 'content'):
                return msg.content
            elif isinstance(msg, dict) and 'content' in msg:
                return msg['content']
        return ""

    def _build_full_prompt(self, user_input):
        """æ„å»ºå®Œæ•´æç¤ºè¯"""
        prompt_parts = []

        # ç³»ç»Ÿæç¤ºè¯
        if self.system_prompt:
            prompt_parts.append(f"user: {self.system_prompt}")  # dddd

        # ä¸Šä¸‹æ–‡ä¿¡æ¯
        if self.context:
            context_str = ", ".join([f"{k}: {v}" for k, v in self.context.items()])
            prompt_parts.append(f"Context: {context_str}")

        # å¯¹è¯å†å²
        if self.dialogue_history:
            d_list = []
            for role, content in self.dialogue_history[-6:]:  # æœ€è¿‘3è½®å¯¹è¯   # dddd
                if role == self.agent_type:
                    d_list.append(f"assistant\n{content}")
                else:
                    d_list.append(f"user\n{content}")
            history_str = "\n".join(d_list)
            prompt_parts.append(f"\n{history_str}")

        # å½“å‰è¾“å…¥
        prompt_parts.append(f"\nuser\n{user_input}")
        prompt_parts.append("<im_end>\n<im_start>assistant")

        return "\n\n".join(prompt_parts)
    

class SimpleAgent(object):
    """å¢å¼ºçš„Agentç±»ï¼Œæ”¯æŒå¤šæ¨¡å¼å’Œæœ¬åœ°è°ƒç”¨"""

    def __init__(self, agent_type: str = "student", debug_mode: bool = False):
        """åˆå§‹åŒ–agent

        Args:
            agent_type: "student" æˆ– "teacher"
        """
        self.agent_type = agent_type
        self.prompt_sys = None
        self.student_response = None
        self.teacher_response = None

        self.context_schema = None
        self.response_format = None
        self.checkpointer = None
        self.middleware_list = []

        self.agent_config = None
        self.response = None
        self.context = None

        self.middleware_er = None
        self.custom_middleware_er = CustomMiddleware
        self.model = None
        self.agent = None

        # å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé…ç½®
        self.dialogue_history = []
        self.max_turns = 5
        self.current_turn = 0
        self.correct_answer = None
        self.student_answer = None

        # åŠŸèƒ½å¼€å…³
        self.parallel_thinking_enabled = False
        self.socratic_teaching_enabled = False
        self.math_background_level = False
        self.debug_mode = debug_mode

        # Memory ä¸ Summary
        self.local_agent = None
        self.memory = None  # æ·»åŠ å†…å­˜ç®¡ç†
        self.summary_config = None
        self.summary_llm = None  # ä¸“é—¨çš„æ‘˜è¦LLM

        # è§£é¢˜æ ‘
        self.cognitive_state = None  # å­¦ç”Ÿè®¤çŸ¥çŠ¶æ€
        self.solution_tree = None    # å½“å‰è§£é¢˜æ ‘
        self.use_cognitive_state = False
        self.use_solution_tree = False

    def set_cognitive_state(self, cognitive_state: StudentCognitiveState):
        """è®¾ç½®è®¤çŸ¥çŠ¶æ€"""
        self.cognitive_state = cognitive_state
        self.use_cognitive_state = True

    def set_solution_tree(self, solution_tree: SolutionTree):
        """è®¾ç½®è§£é¢˜æ ‘"""
        self.solution_tree = solution_tree
        self.use_solution_tree = True

    def _build_full_prompt(self, user_input, memory_context, prompt_type='api'):
        """æ„å»ºå®Œæ•´æç¤ºè¯ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if self.agent_type == "student":
            if prompt_type == 'local':
                prompt_parts = ['<|im_start|>\n']

                # ç³»ç»Ÿæç¤ºè¯
                if self.prompt_sys and self.agent_type != 'student':
                    # å¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„ç³»ç»Ÿæç¤ºè¯
                    if isinstance(self.prompt_sys, str):
                        # å¦‚æœæ˜¯é¢„å®šä¹‰çš„æç¤ºè¯å˜é‡åï¼Œå°è¯•è·å–å…¶å€¼
                        if self.prompt_sys in globals():
                            system_prompt = globals()[self.prompt_sys]
                        else:
                            system_prompt = self.prompt_sys
                    else:
                        system_prompt = str(self.prompt_sys)
                    prompt_parts.append(f"{system_prompt}\n\n")

                # è®¤çŸ¥çŠ¶æ€ï¼ˆå¦‚æœæ˜¯å­¦ç”Ÿä¸”å¯ç”¨ï¼‰
                if (self.agent_type == "student" and self.use_cognitive_state and
                        self.cognitive_state and hasattr(self.cognitive_state, 'get_prompt_context')):
                    cognitive_context = self.cognitive_state.get_prompt_context()
                    prompt_parts.append(f"Student Profile: {cognitive_context}")

                # è§£é¢˜æ ‘ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.use_solution_tree and self.solution_tree:
                    try:
                        tree_context = self._build_solution_tree_context()
                        if tree_context:
                            prompt_parts.append(f"Solution Context: {tree_context}")
                    except Exception as e:
                        if self.debug_mode:
                            logger.info(f"âŒ è§£é¢˜æ ‘ä¸Šä¸‹æ–‡æ„å»ºå¤±è´¥: {e}")

                # å†…å­˜ä¸Šä¸‹æ–‡
                if memory_context:
                    prompt_parts.append(f"Conversation Context: {memory_context}")

                # å¯¹è¯å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
                if self.dialogue_history:
                    if self.dialogue_history[-1][1] == user_input:
                        dialogue_history = self.dialogue_history[:-1]
                    else:
                        dialogue_history = self.dialogue_history
                    d_list = []
                    for role, content in dialogue_history[-6:]:  # æœ€è¿‘3è½®å¯¹è¯   # dddd
                        if role == self.agent_type:
                            d_list.append(f"assistant\n{content}\n\n")
                        else:
                            d_list.append(f"user\n{content}\n\n")
                    history_str = "\n".join(d_list)
                    prompt_parts.append(f"\n{history_str}")

                # å½“å‰è¾“å…¥
                if user_input and len(user_input):
                    prompt_parts.append(f"\nuser\n{user_input}\n")
                prompt_parts.append("<|im_end|>\n<|im_start|>assistant")
                return "\n\n".join(prompt_parts)
            elif prompt_type == 'api':
                # Build messages list in OpenAI format: [{"role": "system/user/assistant", "content": "..."}, ...]
                messages = [{"role": "system", "content": self.prompt_sys}]
                system_content = None
                
                # 1. Dialogue history (previous user and assistant messages)
                if self.dialogue_history:
                    # å¦‚æœæœ€åä¸€æ¡å†å²è®°å½•æ˜¯å½“å‰è¾“å…¥ï¼Œåˆ™æ’é™¤å®ƒ
                    if self.dialogue_history and self.dialogue_history[-1][1] == user_input:
                        dialogue_history = self.dialogue_history[:-1]
                    else:
                        dialogue_history = self.dialogue_history
                    
                    # è½¬æ¢å¯¹è¯å†å²ä¸ºæ¶ˆæ¯æ ¼å¼ï¼ˆä¿ç•™æœ€è¿‘6è½®ï¼‰
                    for role, content in dialogue_history[-6:]:
                        # è½¬æ¢è§’è‰²ï¼šstudent -> assistant, teacher -> user (for student agent)
                        if role == self.agent_type:
                            messages.append({"role": "assistant", "content": content})
                        else:
                            messages.append({"role": "user", "content": content})
                
                # 2. Current user input
                if user_input and len(user_input):
                    messages.append({"role": "user", "content": user_input})

                # 3. Other prompts
                if self.use_cognitive_state or self.use_solution_tree or len(memory_context):
                    # è®¤çŸ¥çŠ¶æ€ï¼ˆå¦‚æœæ˜¯å­¦ç”Ÿä¸”å¯ç”¨ï¼‰
                    if (self.agent_type == "student" and self.use_cognitive_state and
                            self.cognitive_state and hasattr(self.cognitive_state, 'get_prompt_context')):
                        cognitive_context = self.cognitive_state.get_prompt_context()
                        if messages[0]['role'] == 'system':
                            messages[0]['content'] += f"\n\nStudent Profile: {cognitive_context}"

                    # è§£é¢˜æ ‘ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if self.use_solution_tree and self.solution_tree:
                        try:
                            tree_context = self._build_solution_tree_context()
                            if tree_context:
                                if messages[-1]['role'] == 'assistant':
                                    messages[-1]['content'] += f"\n\nSolution Context: {tree_context}"
                        except Exception as e:
                            if self.debug_mode:
                                logger.info(f"âŒ è§£é¢˜æ ‘ä¸Šä¸‹æ–‡æ„å»ºå¤±è´¥: {e}")
                
                return messages
        elif self.agent_type == "teacher":
            return user_input
        else:
            raise NotImplementedError

    def _build_solution_tree_context(self):
        """æ„å»ºè§£é¢˜æ ‘ä¸Šä¸‹æ–‡"""
        if not self.solution_tree:
            return ""

        context_parts = []

        try:
            if self.agent_type == "teacher":
                try:
                    # æ•™å¸ˆçœ‹åˆ°ä¸“å®¶è·¯å¾„å’Œå­¦ç”Ÿè·¯å¾„çš„æ¯”è¾ƒ
                    if hasattr(self.solution_tree, 'compare_with_expert'):
                        comparison = self.solution_tree.compare_with_expert()
                        if comparison and comparison.get("closest_expert_path"):
                            expert_method = comparison['closest_expert_path'].get('method', 'unknown')
                            similarity = comparison.get('similarity', 0)
                            context_parts.append(
                                f"Expert solution available using {expert_method} method")
                            context_parts.append(f"Similarity to student approach: {similarity:.2f}")
                except Exception as e:
                    if self.debug_mode:
                        print(f"âš ï¸ è§£é¢˜æ ‘æ¯”è¾ƒå¤±è´¥: {e}")
                    context_parts.append("Expert guidance available for this problem")

            elif self.agent_type == "student":
                # å­¦ç”Ÿçœ‹åˆ°è‡ªå·±çš„è¿›åº¦
                if (hasattr(self.solution_tree, 'current_student_path') and
                    self.solution_tree.current_student_path):
                    context_parts.append(
                        f"Your current solution path has {len(self.solution_tree.current_student_path)} steps")
                    context_parts.append("Continue your approach or try a different method if stuck")

                # æ·»åŠ å¯ç”¨çš„ä¸“å®¶è·¯å¾„ä¿¡æ¯
            if (hasattr(self.solution_tree, 'solution_paths') and
                    self.solution_tree.solution_paths):
                expert_paths = [p for p in self.solution_tree.solution_paths
                                if hasattr(p, 'get') and p.get("type") == "expert"]
                if expert_paths:
                    methods = set(p.get("method", "unknown") for p in expert_paths)
                    context_parts.append(f"Available expert methods: {', '.join(methods)}")

        except Exception as e:
            if self.debug_mode:
                logger.info(f"âŒ è§£é¢˜æ ‘ä¸Šä¸‹æ–‡æ„å»ºå¼‚å¸¸: {e}")
            # æä¾›åŸºæœ¬çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
            context_parts.append("Solution guidance is available for this problem")

        return "\n".join(context_parts) if context_parts else ""

    def _parse_solution_tree_student(self, response, problem):
        """è§£æå­¦ç”Ÿçš„è§£é¢˜æ ‘å“åº”"""
        solution_tree = SolutionTree(problem)

        try:
            # ç®€å•çš„è§£æé€»è¾‘ - åœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„è§£æ
            if "<SolutionTree>" in response and self.agent_type == "student":
                # æå–è§£å†³æ–¹æ¡ˆè·¯å¾„
                paths_section = response.split("<SolutionPaths>")[1].split("</SolutionPaths>")[0]
                path_blocks = paths_section.split("</Path>")

                for block in path_blocks:
                    if "<Path" in block:
                        # æå–è·¯å¾„ä¿¡æ¯
                        method = self._extract_site_tag(block, "method")
                        # æå–æ­¥éª¤
                        steps = []
                        intermediate_answers = []
                        step_parts = block.split("<Step")
                        for step_part in step_parts[1:]:
                            if ">" in step_part and "</Step>" in step_part:
                                step_content = step_part.split(">", 1)[1].split("</Step>")[0]
                                steps.append(step_content)
                            if "<IntermediateAnswer>" in step_part and "</IntermediateAnswer>" in step_part:
                                intermediate_content = step_part.split("<IntermediateAnswer>", 1)[1].split("</IntermediateAnswer>")[0]
                                intermediate_answers.append(intermediate_content)

                        # æå–æœ€ç»ˆç­”æ¡ˆ
                        final_answer = self._extract_xml_tag(block, "FinalAnswer")

                        solution_tree.add_expert_path({
                            "method": method,
                            "steps": steps,
                            "intermediate_answers": intermediate_answers,
                            "final_answer": final_answer
                        })

        except Exception as e:
            print(f"âŒ Error parsing solution tree: {e}")
            # å¦‚æœè§£æå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„è§£å†³æ–¹æ¡ˆè·¯å¾„
            solution_tree.add_expert_path({
                "method": "algebraic",
                "steps": ["Apply standard algebraic approach", "Solve step by step"],
                "final_answer": "[[Answer will be determined]]"
            })

        return solution_tree

    def _extract_xml_tag(self, text, tag_name):
        """æå–XMLæ ‡ç­¾å†…å®¹"""
        start_tag = f"<{tag_name}>"
        end_tag = f"</{tag_name}>"

        if start_tag in text and end_tag in text:
            return text.split(start_tag)[1].split(end_tag)[0].strip()
        return ""

    def _extract_site_tag(self, text, tag_name):
        """æå–XMLæ ‡ç­¾å†…å®¹"""
        start_tag = f'{tag_name}="'
        end_tag = f'"'

        if start_tag in text and end_tag in text:
            return text.split(start_tag)[1].split(end_tag)[0].strip()
        return ""

    def record_student_step(self, step_content, method_used=None):
        """è®°å½•å­¦ç”Ÿè§£é¢˜æ­¥éª¤"""
        if (self.agent_type == "student" and self.use_solution_tree and
                self.solution_tree and hasattr(self.solution_tree, 'add_student_step')):
            try:
                self.solution_tree.add_student_step(step_content, method_used)

                # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                if hasattr(self, 'debug_mode') and self.debug_mode:
                    detected_method = method_used if method_used else self.solution_tree._detect_student_method(
                        step_content)
                    print(f"ğŸ“ è®°å½•è§£é¢˜æ­¥éª¤: æ–¹æ³•={detected_method}, å†…å®¹é•¿åº¦={len(step_content)}")
            except Exception as e:
                if self.debug_mode:
                    print(f"âŒ è®°å½•è§£é¢˜æ­¥éª¤å¤±è´¥: {e}")


    def complete_student_solution(self, success, final_answer=None):
        """å®Œæˆå­¦ç”Ÿè§£é¢˜"""
        if (self.agent_type == "student" and self.use_solution_tree and
                self.solution_tree and hasattr(self.solution_tree, 'complete_student_path')):
            result = self.solution_tree.complete_student_path(success, final_answer)

            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            if hasattr(self, 'debug_mode') and self.debug_mode:
                print(f"ğŸŒ³ è§£é¢˜æ ‘å®Œæˆ: æˆåŠŸ={success}, ç­”æ¡ˆ={final_answer}")
                if hasattr(self.solution_tree, 'current_student_path'):
                    print(f"   è·¯å¾„æ­¥éª¤æ•°: {len(self.solution_tree.current_student_path)}")
                if hasattr(self.solution_tree, 'solution_paths'):
                    print(f"   æ€»è§£å†³æ–¹æ¡ˆè·¯å¾„: {len(self.solution_tree.solution_paths)}")

            return result
        return None

    def enable_conversation_summary(self, summary_config: SummaryConfig, summary_llm=None):
        """å¯ç”¨å¯¹è¯æ‘˜è¦åŠŸèƒ½"""
        self.summary_config = summary_config
        self.summary_llm = summary_llm

        self.memory = SmartSummaryMemory(
            llm=summary_llm,
            max_turns=summary_config.max_turns,
            max_token_limit=summary_config.max_token_limit,
            enabled=summary_config.enabled
        )

        if hasattr(self.memory, 'debug_mode'):
            self.memory.debug_mode = hasattr(self, 'debug_mode') and self.debug_mode

        status = "enabled" if summary_config.enabled else "disabled"
        logger.info(f"âœ… Conversation summary {status} for {self.agent_type} agent")

    def model_init(self, model_config: ModelConfig, model_name=None):
        """åˆå§‹åŒ–æ¨¡å‹ï¼Œæ”¯æŒAPIå’Œæœ¬åœ°è°ƒç”¨"""
        if model_config.model_type == "local":
            # ä½¿ç”¨ç®€åŒ–çš„æœ¬åœ°Agent
            self.local_agent = SimpleLocalAgent(self.agent_type, self.debug_mode)
            self.local_agent.init_model(model_config)
            print(f"âœ… åˆå§‹åŒ– {model_name} æœ¬åœ°æ¨¡å‹: {model_config.model_name}")
        else:
            # APIæ¨¡å‹é…ç½® - ä½¿ç”¨åŸå§‹APIå®¢æˆ·ç«¯
            self.model = create_api_client(model_config)
            print(f"âœ… åˆå§‹åŒ– {model_name} APIæ¨¡å‹: {model_config.model_name}")
            # ä¸å†éœ€è¦ä¸­é—´ä»¶ï¼Œå› ä¸ºä½¿ç”¨åŸå§‹APIè°ƒç”¨
            self.middleware_er = None

    def _add_debug_middleware(self):
        """ä¸ºAPIæ¨¡å‹æ·»åŠ debugä¸­é—´ä»¶ - å·²ç§»é™¤ï¼Œä½¿ç”¨åŸå§‹APIè°ƒç”¨"""
        pass

    def get_debug_info(self):
        """è·å–debugä¿¡æ¯"""
        if hasattr(self, 'local_agent') and self.local_agent:
            return {
                "agent_type": self.agent_type,
                "last_prompt": self.local_agent.get_last_prompt(),
                "dialogue_history": self.local_agent.dialogue_history
            }
        else:
            return {
                "agent_type": self.agent_type,
                "debug_mode": self.debug_mode
            }

    def set_correct_answer(self, correct_answer: str):
        """è®¾ç½®æ­£ç¡®ç­”æ¡ˆï¼ˆæ•™å¸ˆagentä½¿ç”¨ï¼‰"""
        self.correct_answer = correct_answer

    def agent_init(self, model_config, prompt_sys_name=None, teacher_response=None, student_response=None,
                   context_schema=Context, response_format=ResponseFormat, **kwargs):
        """åˆå§‹åŒ–agenté…ç½®"""
        self.context_schema = context_schema
        self.response_format = response_format

        # è®¾ç½®åŠŸèƒ½å¼€å…³
        self.max_turns = kwargs.get("max_turns", 5)
        self.parallel_thinking_enabled = kwargs.get("parallel_thinking", False)
        self.socratic_teaching_enabled = kwargs.get("socratic_teaching", False)

        self.solution_tree = kwargs.get("prompt_solution_tree", False)
        solution_tree_prompt = kwargs.get("prompt_solution_tree", None)

        if solution_tree_prompt:
            self.prompt_sys = solution_tree_prompt
            self.use_solution_tree = True
        else:
            self.prompt_sys = prompt_sys_name
            self.use_solution_tree = False
        self.teacher_response = teacher_response
        self.student_response = student_response

        # æ ¹æ®åŠŸèƒ½å¼€å…³è°ƒæ•´ç³»ç»Ÿæç¤ºè¯
        self._adjust_prompt_based_on_settings()

        # ä¸­é—´ä»¶ä¸å†éœ€è¦ï¼Œå› ä¸ºä½¿ç”¨åŸå§‹APIè°ƒç”¨
        # middlewareå‚æ•°ä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼Œä½†ä¸ä½¿ç”¨

        if model_config.model_type == "local":
            # æœ¬åœ°æ¨¡å‹åˆå§‹åŒ–
            self.local_agent.set_system_prompt(self.prompt_sys)
            if self.context:
                self.local_agent.set_context(**self.context.__dict__)
        else:
            # å¯¹äºAPIæ¨¡å‹ï¼Œä½¿ç”¨ç®€åŒ–çš„agentåˆ›å»ºæ–¹å¼ï¼ˆåŸå§‹APIè°ƒç”¨ï¼‰
            self.agent = self._create_simple_agent()
            if self.debug_mode:
                print("âœ… ä½¿ç”¨ç®€åŒ–Agentï¼ˆåŸå§‹APIè°ƒç”¨ï¼‰")

    def _create_simple_agent(self):
        """åˆ›å»ºä¸åŒ…å«å·¥å…·çš„ç®€å•å¯¹è¯agent - ä½¿ç”¨åŸå§‹APIè°ƒç”¨"""
        # åŒ…è£…æˆç±»ä¼¼agentçš„æ¥å£
        class SimpleAgentWrapper:
            def __init__(self, model, prompt_sys, debug_mode=False):
                self.model = model
                self.prompt_sys = prompt_sys
                self.debug_mode = debug_mode

            def invoke(self, input_dict, config=None, context=None):
                try:
                    api_messages = []
                    # æå–ç”¨æˆ·è¾“å…¥
                    messages = input_dict.get("messages", [])
                    for msg in messages:
                        if isinstance(msg, dict) and 'content' in msg:
                            api_messages.append({"role": msg['role'], "content": msg['content']})
                    
                    # è°ƒç”¨æ¨¡å‹
                    response = self.model.invoke(api_messages)
                    
                    # è¿”å›ä¸ç°æœ‰ä»£ç å…¼å®¹çš„æ ¼å¼
                    return response

                except Exception as e:
                    if self.debug_mode:
                        print(f"âŒ ç®€å•Agentè°ƒç”¨é”™è¯¯: {e}")
                    return {
                        'messages': [{'role': 'assistant', 'content': "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ã€‚è¯·é‡æ–°æé—®ã€‚"}],
                        'structured_response': SimpleNamespace(main_response="æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ã€‚è¯·é‡æ–°æé—®ã€‚")
                    }

            def stream(self, input_dict, config=None, context=None, stream_mode="values"):
                # ç®€å•çš„æµå¼å“åº”å®ç°
                result = self.invoke(input_dict, config, context)
                yield result

        return SimpleAgentWrapper(self.model, self.prompt_sys, self.debug_mode)

    def get_prompt_str(self, prompt_name):
        if prompt_name:
            if isinstance(prompt_name, str):
                # å¦‚æœæ˜¯é¢„å®šä¹‰çš„æç¤ºè¯å˜é‡åï¼Œå°è¯•è·å–å…¶å€¼
                if prompt_name in globals():
                    base_prompt = globals()[prompt_name]
                else:
                    base_prompt = prompt_name
            else:
                base_prompt = str(prompt_name)
            return base_prompt
        else:
            return None

    def _adjust_prompt_based_on_settings(self):
        """æ ¹æ®åŠŸèƒ½å¼€å…³è°ƒæ•´ç³»ç»Ÿæç¤ºè¯"""
        base_prompt = self.get_prompt_str(self.prompt_sys)
        self.student_response = self.get_prompt_str(self.student_response)
        self.teacher_response = self.get_prompt_str(self.teacher_response)

        if self.agent_type == "student":

            if self.parallel_thinking_enabled:
                base_prompt += "\n\n" + PARALLEL_THINKING_PROMPT

            self.prompt_sys = base_prompt

        elif self.agent_type == "teacher":

            if self.correct_answer:
                base_prompt += f"\n\nYou know the correct solution is: {self.correct_answer}. But DO NOT reveal this answer directly to the student. Guide the student to discover it themselves."

            if self.socratic_teaching_enabled:
                base_prompt += "\n\n" + SOCRATIC_TEACHING_PROMPT

            self.prompt_sys = base_prompt

        elif self.agent_type == "expert_student":

            self.prompt_sys = base_prompt

    def config_create(self, key_i, value_i):
        self.agent_config = {"configurable": {key_i: value_i}, "recursion_limit": 100}

    def context_set(self, **kwargs):
        self.context = self.context_schema(**kwargs)

    def chat_once(self, user_input, response_type='invoke', silence=False, **kwargs):
        if response_type == 'invoke':
            self.agent_response_invoke(user_input, **kwargs)
            if not silence:
                self.agent_output()
        else:
            self.agent_response_stream(user_input, **kwargs)

    def _extract_response_text(self, response):
        """ä»å“åº”å¯¹è±¡ä¸­æå–æ–‡æœ¬å†…å®¹"""
        if isinstance(response, str):
            return response

        if hasattr(response, 'get') and isinstance(response, dict):
            # å­—å…¸ç±»å‹çš„å“åº”
            if 'structured_response' in response and hasattr(response['structured_response'], 'main_response'):
                return response['structured_response'].main_response
            elif 'messages' in response and response['messages']:
                last_msg = response['messages'][-1]
                if hasattr(last_msg, 'content'):
                    return last_msg.content
                elif isinstance(last_msg, dict) and 'content' in last_msg:
                    return last_msg['content']
            # å°è¯•ç›´æ¥è·å–content
            elif 'content' in response:
                return response['content']

        if hasattr(response, 'structured_response') and hasattr(response.structured_response, 'main_response'):
            return response.structured_response.main_response
        elif hasattr(response, 'content'):
            return response.content

        # æœ€åå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        return str(response)

    def check_answer(self, student_response):
        self.student_answer = extract_answer(student_response)
        correctness = self._has_correct_answer(self.student_answer, self.correct_answer)
        if not correctness:
            self.student_answer = extract_answer(self.student_answer)
            correctness = self._has_correct_answer(self.student_answer, self.correct_answer)
        return correctness

    def multi_agent_chat_explicit(self, teacher_agent, problem: str, raw_problem: str, correct_answer: str,
                                  dialogue_record: DialogueRecord, **kwargs):
        """æ¨¡å¼1: æ˜¾å¼äº¤äº’ - æ•™å¸ˆå’Œå­¦ç”Ÿç›´æ¥å¯¹è¯"""
        if dialogue_record.debug_mode:
            logger.info(f"\nğŸ¯ å¼€å§‹è§£é¢˜: {problem}")
            logger.info("=" * 50)

        # è®¾ç½®æ•™å¸ˆçŸ¥é“çš„æ­£ç¡®ç­”æ¡ˆ
        teacher_agent.set_correct_answer(correct_answer)
        teacher_agent._adjust_prompt_based_on_settings()  # é‡æ–°è°ƒæ•´æç¤ºè¯

        # é‡ç½®å¯¹è¯å†å²
        self.dialogue_history = [('teacher', problem)]
        self.current_turn = 0

        # åˆå§‹åŒ–å†…å­˜ç®¡ç†
        global_memory = None
        teacher_memory = None

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ‘˜è¦åŠŸèƒ½
        if hasattr(self, 'memory') and self.memory and getattr(self.memory, 'enabled', False):
            global_memory = self.memory
            global_memory.debug_mode = dialogue_record.debug_mode
            global_memory.add_message('teacher', problem)
            if dialogue_record.debug_mode:
                logger.info("ğŸ—‚ å·²å°†åˆå§‹é—®é¢˜åŠ å…¥å­¦ç”Ÿæ‘˜è¦å†…å­˜")

        if hasattr(teacher_agent, 'memory') and teacher_agent.memory and getattr(teacher_agent.memory, 'enabled', False):
            teacher_memory = teacher_agent.memory
            teacher_memory.debug_mode = dialogue_record.debug_mode
            teacher_memory.add_message('teacher', problem)
            if dialogue_record.debug_mode:
                logger.info("ğŸ—‚ å·²å°†åˆå§‹é—®é¢˜åŠ å…¥æ•™å¸ˆæ‘˜è¦å†…å­˜")

        try:
            # å­¦ç”Ÿé¦–æ¬¡å°è¯•
            student_prompt_type = 'local' if (hasattr(self, 'local_agent') and self.local_agent) else 'api'

            # åœ¨æ„å»ºè¾“å…¥å‰æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦
            self._check_and_generate_summary(global_memory, teacher_memory, dialogue_record.debug_mode)

            student_input = self._format_student_input(raw_problem, self.dialogue_history, prompt_type=student_prompt_type)
            student_response_obj = self._invoke_agent(student_input)
            student_response = self._extract_response_text(student_response_obj)
            self.dialogue_history.append(("student", student_response))

            # å°†å­¦ç”Ÿå›å¤åŠ å…¥æ‘˜è¦å†…å­˜
            self._add_to_memory(global_memory, teacher_memory, 'student', student_response, dialogue_record.debug_mode)

            # è®°å½•å­¦ç”Ÿç¬¬ä¸€æ­¥è§£é¢˜æ­¥éª¤
            if self.use_solution_tree and self.solution_tree:
                self.record_student_step(student_response, method_used=self._detect_method_from_response(student_response))
                if dialogue_record.debug_mode:
                    logger.info("ğŸ“ å·²è®°å½•å­¦ç”Ÿç¬¬ä¸€æ¬¡è§£é¢˜æ­¥éª¤")

                # åˆ†æå­¦ç”Ÿå›å¤
                parallel_count, path_count = dialogue_record.analyze_student_response(student_response)

                # è®°å½•ç¬¬ä¸€è½®å¯¹è¯
                dialogue_record.add_turn({
                    "turn": 1,
                    "student_response": student_response,
                    "teacher_response": "",
                    "teacher_intent": "initial_response",
                    "parallel_thinking_count": parallel_count,
                    "thinking_paths_count": path_count,
                    "answer_leakage": False
                })

                if dialogue_record.debug_mode:
                    logger.info(f"ğŸ‘¨â€ğŸ“ å­¦ç”Ÿ [è½®æ¬¡1]: {student_response}")

                self.correct_answer = extract_answer(correct_answer)
                dialogue_record.first_correct = self.check_answer(student_response)

                for turn in range(self.max_turns):
                    # æ£€æŸ¥å­¦ç”Ÿæ˜¯å¦å¾—å‡ºæ­£ç¡®ç­”æ¡ˆ
                    if (turn == 0 and dialogue_record.first_correct) or (self.check_answer(student_response)):
                        if dialogue_record.debug_mode:
                            logger.info("ğŸ‰ å­¦ç”Ÿå¾—å‡ºæ­£ç¡®ç­”æ¡ˆ!")
                        dialogue_record.correct = True
                        dialogue_record.final_student_answer = self.student_answer

                        # æ›´æ–°è®¤çŸ¥çŠ¶æ€ï¼ˆæœ€åä¸€è½®ï¼ŒæˆåŠŸè§£å†³ï¼‰
                        if self.use_cognitive_state and self.cognitive_state:
                            try:
                                errors = dialogue_record._extract_errors()
                                method_used = self._detect_method_from_response(student_response)
                                self.cognitive_state.update_based_on_interaction(
                                    problem=problem,
                                    student_approach=student_response,
                                    errors=errors,
                                    method_used=method_used,
                                    success=True  # æˆåŠŸè§£å†³é—®é¢˜
                                )
                            except Exception as e:
                                if dialogue_record.debug_mode:
                                    logger.error(f"æ›´æ–°æœ€ç»ˆè®¤çŸ¥çŠ¶æ€æ—¶å‡ºé”™: {e}")

                        # å®Œæˆè§£é¢˜è·¯å¾„
                        if self.use_solution_tree and self.solution_tree:
                            self.complete_student_solution(success=True, final_answer=student_response)
                            if dialogue_record.debug_mode:
                                logger.info("âœ… å­¦ç”Ÿç­”å¯¹äº†ï¼Œè§£é¢˜è·¯å¾„å·²å®Œæˆ")

                        return self.student_answer, self.correct_answer, dialogue_record

                    current_turn = turn + 2  # ä»ç¬¬äºŒè½®å¼€å§‹

                    if dialogue_record.debug_mode:
                        logger.info(f"\nğŸ”„ ç¬¬ {current_turn} è½®å¯¹è¯:")
                        logger.info("-" * 30)

                    # æ•™å¸ˆå›åº”
                    # teacher_input = student_response
                    if self.parallel_thinking_enabled and turn == 0:
                        teacher_response = STUDENT_STANDARD_PARALLEL_THINKING_PROMPT.format(question=raw_problem)
                    else:
                        # ç¡®å®šæ•™å¸ˆagentä½¿ç”¨çš„æ¨¡å‹ç±»å‹
                        teacher_prompt_type = 'local' if (hasattr(teacher_agent, 'local_agent') and teacher_agent.local_agent) else 'api'
                        teacher_input = teacher_agent._format_teacher_input(self.dialogue_history, prompt_type=teacher_prompt_type)
                        teacher_response_obj = teacher_agent._invoke_agent(teacher_input)
                        teacher_response = self._extract_response_text(teacher_response_obj)

                    self.dialogue_history.append(("teacher", teacher_response))

                    # æ£€æŸ¥ç­”æ¡ˆæ³„éœ²
                    leakage_detected = dialogue_record.check_answer_leakage(teacher_response, self.correct_answer)

                    if dialogue_record.debug_mode:
                        logger.info(f"ğŸ‘¨â€ğŸ« æ•™å¸ˆ [è½®æ¬¡{current_turn}]: {teacher_response}")
                    if leakage_detected:
                        if dialogue_record.debug_mode:
                            logger.info("âš ï¸  æ£€æµ‹åˆ°ç­”æ¡ˆæ³„éœ²!")

                # åœ¨å­¦ç”Ÿå›åº”å‰æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦
                self._check_and_generate_summary(global_memory, teacher_memory, dialogue_record.debug_mode)

                # å­¦ç”Ÿå›åº”
                student_prompt_type = 'local' if (hasattr(self, 'local_agent') and self.local_agent) else 'api'
                student_input = self._format_student_input(raw_problem, self.dialogue_history,
                                                           prompt_type=student_prompt_type)
                student_response_obj = self._invoke_agent(student_input)
                student_response = self._extract_response_text(student_response_obj)
                self.dialogue_history.append(("student", student_response))
                self._add_to_memory(global_memory, teacher_memory, 'student', student_response, dialogue_record.debug_mode)

                # è®°å½•å­¦ç”Ÿè§£é¢˜æ­¥éª¤
                if self.use_solution_tree and self.solution_tree:
                    method_used = self._detect_method_from_response(student_response)
                    self.record_student_step(student_response, method_used=method_used)
                    if dialogue_record.debug_mode:
                        logger.info(f"ğŸ“ å·²è®°å½•å­¦ç”Ÿç¬¬{current_turn}æ­¥è§£é¢˜æ­¥éª¤ï¼Œæ–¹æ³•: {method_used}")

                # åˆ†æå­¦ç”Ÿå›å¤
                parallel_count, path_count = dialogue_record.analyze_student_response(student_response)

                # è®°å½•æœ¬è½®å¯¹è¯
                turn_data = {
                    "answer": self.correct_answer,
                    "answer_response": correct_answer,
                    "turn": current_turn,
                    "student_response": student_response,
                    "teacher_response": teacher_response,
                    "teacher_intent": self._analyze_teacher_intent(teacher_response),
                    "parallel_thinking_count": parallel_count,
                    "thinking_paths_count": path_count,
                    "answer_leakage": leakage_detected
                }
                dialogue_record.add_turn(turn_data)

                # åœ¨æ¯è½®å¯¹è¯åæ›´æ–°è®¤çŸ¥çŠ¶æ€ï¼ˆå½“å‰è½®ä¸æ˜¯æœ€åä¸€è½®ï¼‰
                if self.use_cognitive_state and self.cognitive_state:
                    # ä»run.pyå¯¼å…¥è¾…åŠ©å‡½æ•°
                    errors = dialogue_record._extract_errors()
                    method_used = self._detect_method_from_response(student_response)
                    try:
                        self.cognitive_state.update_based_on_interaction(
                            problem=problem,
                            student_approach=student_response,
                            errors=errors,
                            method_used=method_used,
                            success=False  # è¿˜åœ¨å¯¹è¯ä¸­ï¼Œæœªæœ€ç»ˆè§£å†³
                        )
                    except Exception as e:
                        if dialogue_record.debug_mode:
                            logger.error(f"æ›´æ–°è®¤çŸ¥çŠ¶æ€æ—¶å‡ºé”™: {e}")

                # åœ¨æ¯è½®å¯¹è¯åæ›´æ–°è®¤çŸ¥çŠ¶æ€ï¼ˆå½“å‰è½®ä¸æ˜¯æœ€åä¸€è½®ï¼‰
                if self.cognitive_state:
                    errors = dialogue_record._extract_errors()
                    method_used = self._detect_method_from_response(student_response)
                    self.cognitive_state.update_based_on_interaction(
                        problem=problem,
                        student_approach=student_response,
                        errors=errors,
                        method_used=method_used,
                        success=False  # è¿˜åœ¨å¯¹è¯ä¸­ï¼Œæœªæœ€ç»ˆè§£å†³
                    )
                if dialogue_record.debug_mode:
                    logger.info(f"ğŸ‘¨â€ğŸ“ å­¦ç”Ÿ [è½®æ¬¡{current_turn}]: {student_response}")

                self.current_turn = current_turn

            # è®¾ç½®æœ€ç»ˆç­”æ¡ˆ
            dialogue_record.final_student_answer = self.student_answer
            success = self._has_correct_answer(self.student_answer, self.correct_answer)

            # æ›´æ–°æœ€ç»ˆè®¤çŸ¥çŠ¶æ€ï¼ˆè¾¾åˆ°æœ€å¤§è½®æ¬¡ï¼‰
            if self.use_cognitive_state and self.cognitive_state:
                try:
                    errors = dialogue_record._extract_errors()
                    method_used = self._detect_method_from_response(student_response)
                    self.cognitive_state.update_based_on_interaction(
                        problem=problem,
                        student_approach=student_response,
                        errors=errors,
                        method_used=method_used,
                        success=success  # åŸºäºæœ€ç»ˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®
                    )
                except Exception as e:
                    if dialogue_record.debug_mode:
                        logger.error(f"æ›´æ–°æœ€ç»ˆè®¤çŸ¥çŠ¶æ€æ—¶å‡ºé”™: {e}")

            # å®Œæˆè§£é¢˜è·¯å¾„ï¼ˆè¾¾åˆ°æœ€å¤§è½®æ¬¡ï¼‰
            if self.use_solution_tree and self.solution_tree:
                self.complete_student_solution(success=success, final_answer=student_response)
                if dialogue_record.debug_mode:
                    status = "æˆåŠŸ" if success else "å¤±è´¥"
                    logger.info(f"ğŸ“ è¾¾åˆ°æœ€å¤§è½®æ¬¡ï¼Œè§£é¢˜è·¯å¾„{status}")

            return self._get_final_answer(), self.correct_answer, dialogue_record

        except Exception as e:
            if dialogue_record.debug_mode:
                logger.error(f"âŒ å¯¹è¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            # åœ¨å‡ºé”™æ—¶ä¹Ÿè¦å®Œæˆè§£é¢˜è·¯å¾„
            if self.use_solution_tree and self.solution_tree:
                self.complete_student_solution(success=False, final_answer="é”™è¯¯")
                if dialogue_record.debug_mode:
                    logger.info("ğŸ“ å¯¹è¯å‡ºé”™ï¼Œè§£é¢˜è·¯å¾„æ ‡è®°ä¸ºå¤±è´¥")

            # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„é”™è¯¯å“åº”
            dialogue_record.final_student_answer = "æŠ±æ­‰ï¼Œè§£é¢˜è¿‡ç¨‹ä¸­å‡ºç°äº†æŠ€æœ¯é—®é¢˜ã€‚"
            return "æŠ±æ­‰ï¼Œè§£é¢˜è¿‡ç¨‹ä¸­å‡ºç°äº†æŠ€æœ¯é—®é¢˜ã€‚", self.correct_answer, dialogue_record


    def _check_and_generate_summary(self, global_memory, teacher_memory, debug_mode):
        """æ£€æŸ¥å¹¶ç”Ÿæˆå¯¹è¯æ‘˜è¦"""
        try:
            # æ£€æŸ¥å­¦ç”Ÿå†…å­˜æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦
            if global_memory and getattr(global_memory, 'enabled', False):
                if global_memory._should_generate_summary():
                    global_memory._generate_summary()
                    if debug_mode:
                        logger.info("ğŸ“ å­¦ç”Ÿå†…å­˜å·²ç”Ÿæˆæ–°æ‘˜è¦")

            # æ£€æŸ¥æ•™å¸ˆå†…å­˜æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦
            if teacher_memory and getattr(teacher_memory, 'enabled', False):
                if teacher_memory._should_generate_summary():
                    teacher_memory._generate_summary()
                    if debug_mode:
                        logger.info("ğŸ“ æ•™å¸ˆå†…å­˜å·²ç”Ÿæˆæ–°æ‘˜è¦")

        except Exception as e:
            if debug_mode:
                logger.warning(f"ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")


    def _add_to_memory(self, global_memory, teacher_memory, role, content, debug_mode):
        """å°†æ¶ˆæ¯æ·»åŠ åˆ°å†…å­˜"""
        try:
            if global_memory and getattr(global_memory, 'enabled', False):
                global_memory.add_message(role, content)

            if teacher_memory and getattr(teacher_memory, 'enabled', False):
                teacher_memory.add_message(role, content)

        except Exception as e:
            if debug_mode:
                logger.warning(f"æ·»åŠ æ¶ˆæ¯åˆ°å†…å­˜æ—¶å‡ºé”™: {e}")

    def _analyze_teacher_intent(self, teacher_response: str) -> str:
        """åˆ†ææ•™å¸ˆå›å¤çš„æ„å›¾"""
        response_lower = teacher_response.lower()

        if any(word in response_lower for word in ["question", "ask", "what do you think"]):
            return "socratic_questioning"
        elif any(word in response_lower for word in ["hint", "suggest", "try"]):
            return "providing_hint"
        elif any(word in response_lower for word in ["correct", "right", "good"]):
            return "positive_feedback"
        elif any(word in response_lower for word in ["wrong", "incorrect", "mistake"]):
            return "correcting_error"
        elif any(word in response_lower for word in ["explain", "concept", "principle"]):
            return "explaining_concept"
        else:
            return "general_guidance"

    def _detect_method_from_response(self, response: str) -> str:
        """ä»å­¦ç”Ÿå“åº”ä¸­æ£€æµ‹ä½¿ç”¨çš„æ–¹æ³•"""
        if not response:
            return "unknown"

        response_lower = response.lower()

        # æ£€æµ‹æ–¹æ³•ç±»å‹
        if any(word in response_lower for word in ["equation", "solve for", "variable", "x =", "let x", "algebra"]):
            return "algebraic"
        elif any(word in response_lower for word in
                 ["diagram", "graph", "shape", "angle", "area", "triangle", "circle"]):
            return "geometric"
        elif any(word in response_lower for word in
                 ["calculate", "compute", "number", "digit", "sum", "total", "multiply"]):
            return "computational"
        elif any(word in response_lower for word in ["logic", "reason", "therefore", "because", "since", "if then"]):
            return "logical"
        elif any(word in response_lower for word in ["guess", "try", "maybe", "perhaps", "i think"]):
            return "trial_and_error"
        else:
            return "general"

    def multi_agent_chat_tool_based(self, problem: str, correct_answer: str,
                                    dialogue_record: DialogueRecord, **kwargs):
        """æ¨¡å¼2: å·¥å…·è°ƒç”¨ - å­¦ç”Ÿä½œä¸ºcontrollerè°ƒç”¨æ•™å¸ˆå·¥å…·"""
        print(f"\nğŸ¯ å¼€å§‹å·¥å…·è°ƒç”¨æ¨¡å¼è§£é¢˜: {problem}")
        print("=" * 50)

        # é…ç½®å­¦ç”Ÿagentä»¥åŒ…å«æ•™å¸ˆå·¥å…·ï¼ˆåŒ…å«æ­£ç¡®ç­”æ¡ˆï¼‰
        teacher_tool = self._create_teacher_tool(correct_answer)

        # æ›´æ–°ç³»ç»Ÿæç¤ºè¯ä»¥åŒ…å«å·¥å…·è¯´æ˜
        updated_prompt = self.prompt_sys + "\n\nYou can use the ask_teacher tool when you need guidance."

        # å­¦ç”Ÿè‡ªä¸»è§£é¢˜ï¼Œå¯åœ¨éœ€è¦æ—¶è°ƒç”¨æ•™å¸ˆå·¥å…·
        # æ³¨æ„ï¼šå·¥å…·è°ƒç”¨æ¨¡å¼åœ¨åŸå§‹APIä¸­éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œè¿™é‡Œç®€åŒ–ä¸ºç›´æ¥è°ƒç”¨
        final_response = self._invoke_agent(problem)

        # åˆ†æå­¦ç”Ÿå›å¤
        parallel_count, path_count = dialogue_record.analyze_student_response(final_response)

        # è®°å½•å·¥å…·è°ƒç”¨æ¨¡å¼çš„å¯¹è¯ï¼ˆç®€åŒ–ä¸ºå•è½®ï¼‰
        dialogue_record.add_turn({
            "turn": 1,
            "student_response": final_response,
            "teacher_response": "Tool-based interaction",
            "teacher_intent": "tool_guidance",
            "parallel_thinking_count": parallel_count,
            "thinking_paths_count": path_count,
            "answer_leakage": False
        })

        dialogue_record.final_student_answer = final_response

        print(f"ğŸ‘¨â€ğŸ“ å­¦ç”Ÿæœ€ç»ˆå›ç­”: {final_response}")

        return final_response

    def _create_teacher_tool(self, correct_answer: str):
        """åˆ›å»ºæ•™å¸ˆå·¥å…·ä¾›å­¦ç”Ÿè°ƒç”¨ï¼ˆåŒ…å«æ­£ç¡®ç­”æ¡ˆçŸ¥è¯†ï¼‰"""
        # ç®€å•çš„å·¥å…·å‡½æ•°ï¼Œä¸ä½¿ç”¨LangChainè£…é¥°å™¨
        def ask_teacher(question: str) -> str:
            """Ask the teacher for guidance on a specific question or problem.

            The teacher knows the correct answer but will not reveal it directly.
            Instead, the teacher will provide helpful guidance and hints.

            Use this tool when:
            - You're stuck on a math problem
            - You need clarification on concepts
            - You want to check your approach
            - You need step-by-step guidance
            """
            # åŸºäºæ­£ç¡®ç­”æ¡ˆæä¾›å¼•å¯¼æ€§æç¤º
            guidance_responses = [
                "Let me guide you through this step by step. What part are you finding difficult?",
                "Good attempt! Let's break this down. What's your current approach?",
                "I see where you might be confused. Let me ask you a question to help you think differently...",
                "Remember the key concept here is to identify the known values and what you're solving for.",
                "Try breaking the problem into smaller parts. What's the first step you would take?",
                "Consider what information you have and what you're trying to find. How can you connect them?",
                "That's a good start. Now think about what mathematical operations might be needed here."
            ]
            import random
            return random.choice(guidance_responses)

        return ask_teacher

    def _invoke_agent(self, input_text):
        """è°ƒç”¨agentå¹¶è¿”å›å“åº”
        
        Args:
            input_text: å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æ¶ˆæ¯åˆ—è¡¨ï¼ˆOpenAIæ ¼å¼ï¼‰
                       - å­—ç¬¦ä¸²ï¼šç”¨äºå­¦ç”Ÿagentæˆ–æœ¬åœ°æ¨¡å‹
                       - æ¶ˆæ¯åˆ—è¡¨ï¼šç”¨äºæ•™å¸ˆagentçš„APIè°ƒç”¨ï¼ˆå·²æ ¼å¼åŒ–ï¼‰
        """
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦å·²ç»æ˜¯æ¶ˆæ¯åˆ—è¡¨æ ¼å¼ï¼ˆAPIè°ƒç”¨ï¼‰
        if isinstance(input_text, list) and len(input_text) > 0 and isinstance(input_text[0], dict):
            # è¾“å…¥å·²ç»æ˜¯æ¶ˆæ¯åˆ—è¡¨ï¼Œç›´æ¥è°ƒç”¨API
            if hasattr(self, 'local_agent') and self.local_agent:
                # æœ¬åœ°æ¨¡å‹ä¸åº”è¯¥æ”¶åˆ°æ¶ˆæ¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
                user_content = ""
                for msg in reversed(input_text):
                    if msg.get("role") == "user":
                        user_content = msg.get("content", "")
                        break
                input_text = user_content
                response = self._invoke_local_with_memory(input_text)
            else:
                # APIè°ƒç”¨ï¼Œç›´æ¥ä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨
                try:
                    response = self.agent.invoke({
                        "messages": input_text
                    }, config=self.agent_config, context=self.context)
                    
                    if response and 'messages' in response:
                        response_text = response['structured_response'].main_response
                    else:
                        response_text = ""
                    
                    # è®°å½•åˆ°å†…å­˜
                    if self.memory and self.memory.enabled:
                        # æå–ç”¨æˆ·è¾“å…¥ï¼ˆæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰
                        for msg in reversed(input_text):
                            if msg.get("role") == "user":
                                self.memory.add_message("user", msg.get("content", ""))
                                break
                        self.memory.add_message("assistant", response_text)
                    
                    return response_text
                except Exception as e:
                    if self.debug_mode:
                        logger.info(f"âŒ APIè°ƒç”¨é”™è¯¯: {e}")
                    raise e
        else:
            # è¾“å…¥æ˜¯å­—ç¬¦ä¸²ï¼Œä½¿ç”¨æ ‡å‡†æµç¨‹
            if self.memory and self.memory.enabled:
                self.memory.add_message("user", input_text)

            if hasattr(self, 'local_agent') and self.local_agent:
                response = self._invoke_local_with_memory(input_text)
            else:
                response = self._invoke_api_with_memory(input_text)

            # è®°å½•åŠ©æ‰‹å“åº”åˆ°å†…å­˜
            if self.memory and self.memory.enabled and response:
                self.memory.add_message("assistant", response)

            return response

    def _invoke_local_with_memory(self, input_text):
        """æœ¬åœ°æ¨¡å‹è°ƒç”¨ï¼ˆæ”¯æŒå†…å­˜å’Œè§£é¢˜æ ‘ï¼‰"""
        # æ„å»ºåŒ…å«å†…å­˜ä¸Šä¸‹æ–‡å’Œè§£é¢˜æ ‘ä¸Šä¸‹æ–‡çš„è¾“å…¥
        memory_context = ""
        if self.memory and self.memory.enabled:
            memory_context = self.memory.get_context()

        # ä½¿ç”¨å¢å¼ºçš„å®Œæ•´æç¤ºè¯æ„å»ºæ–¹æ³•
        full_input = self._build_full_prompt(input_text, memory_context, 'local')

        response = self.local_agent.invoke({
            "messages": [{"role": "user", "content": full_input}]
        })
        return response['structured_response'].main_response

    def _invoke_api_with_memory(self, input_text):
        """APIæ¨¡å‹è°ƒç”¨ï¼ˆæ”¯æŒå†…å­˜å’Œè§£é¢˜æ ‘ï¼‰"""
        memory_context = ""
        if self.memory and self.memory.enabled:
            memory_context = self.memory.get_context()

        # ä½¿ç”¨å¢å¼ºçš„å®Œæ•´æç¤ºè¯æ„å»ºæ–¹æ³• - è¿”å›æ¶ˆæ¯åˆ—è¡¨ï¼ˆOpenAIæ ¼å¼ï¼‰
        messages = self._build_full_prompt(input_text, memory_context, 'api')

        try:
            # è°ƒç”¨agent - messageså·²ç»æ˜¯æ­£ç¡®æ ¼å¼
            response = self.agent.invoke({
                "messages": messages
            }, config=self.agent_config, context=self.context)

            if response and 'messages' in response:
                # æ­£å¸¸è¿”å›ç»“æ„åŒ–å“åº”
                return response['structured_response'].main_response

            return ""

        except Exception as e:
            if self.debug_mode:
                logger.info(f"âŒ APIè°ƒç”¨é”™è¯¯: {e}")

            # å¦‚æœå‡ºç°å·¥å…·è°ƒç”¨ç›¸å…³é”™è¯¯ï¼Œå›é€€åˆ°ç®€å•å“åº”
            if "tool_calls" in str(e) or "tool_call_id" in str(e):
                return "æˆ‘ç†è§£æ‚¨éœ€è¦å¸®åŠ©ï¼Œä½†åœ¨å½“å‰è®¾ç½®ä¸‹ï¼Œè¯·ç›´æ¥æå‡ºæ‚¨çš„é—®é¢˜æˆ–å›°æƒ‘ã€‚"

            raise e

    def _build_input_with_memory(self, input_text, memory_context):
        """æ„å»ºåŒ…å«å†…å­˜ä¸Šä¸‹æ–‡çš„è¾“å…¥"""
        if memory_context:
            return f"{memory_context}\n\nCurrent Question: {input_text}\n\nPlease respond based on the conversation context:"
        else:
            return input_text

    def _format_teacher_input(self, history, prompt_type='api'):
        """æ ¼å¼åŒ–æ•™å¸ˆè¾“å…¥ï¼Œæ”¯æŒæœ¬åœ°å’ŒAPIè°ƒç”¨
        
        Args:
            history: å¯¹è¯å†å²åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [(role, content), ...]
            prompt_type: 'api' æˆ– 'local'
        
        Returns:
            - å¯¹äº 'api': è¿”å›æ¶ˆæ¯åˆ—è¡¨ [{"role": "system/user/assistant", "content": "..."}, ...]
            - å¯¹äº 'local': è¿”å›å­—ç¬¦ä¸²æ ¼å¼çš„æç¤ºè¯
        """
        if prompt_type == 'api':
            # Build messages list in OpenAI format: [{"role": "system/user/assistant", "content": "..."}, ...]
            messages = []
            system_prompt = None
            # 1. System prompt (if exists)
            if self.prompt_sys:
                # å¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„ç³»ç»Ÿæç¤ºè¯
                if isinstance(self.prompt_sys, str):
                    # å¦‚æœæ˜¯é¢„å®šä¹‰çš„æç¤ºè¯å˜é‡åï¼Œå°è¯•è·å–å…¶å€¼
                    if self.prompt_sys in globals():
                        system_prompt = globals()[self.prompt_sys]
                    else:
                        system_prompt = self.prompt_sys
                else:
                    system_prompt = str(self.prompt_sys + "\n\n")
            
            # 2. Dialogue history (previous user and assistant messages)
            if history:
                # è½¬æ¢å¯¹è¯å†å²ä¸ºæ¶ˆæ¯æ ¼å¼ï¼ˆä¿ç•™æœ€è¿‘6è½®ï¼‰
                for role, content in history[-6:]:
                    # è½¬æ¢è§’è‰²ï¼šteacher -> assistant, student -> user (for teacher agent)
                    if role == self.agent_type:
                        messages.append({"role": "assistant", "content": content})
                    else:
                        messages.append({"role": "user", "content": content})
                
                if messages[-1]['role'] == 'user':
                    if self.socratic_teaching_enabled:
                        messages[-1]['content'] += f"\n\n{TEACHER_RESPONSE_TEACHING_SOCRATIC}"
                    else:
                        messages[-1]['content'] += f"\n\n{self.teacher_response}"
                
                if messages[0]['role'] == 'user':
                    if system_prompt:
                        messages[0]['content'] = system_prompt + messages[0]['content']
                elif messages[0]['role'] == 'assistant':
                    if system_prompt:
                        messages.insert(0, {"role": "user", "content": system_prompt})
            
            return messages
        
        elif prompt_type == 'local':
            # æœ¬åœ°æ¨¡å‹æ ¼å¼ï¼šè¿”å›å­—ç¬¦ä¸²
            context = ''
            d_list = []
            for role, content in history[-6:]:  # æœ€è¿‘3è½®å¯¹è¯
                if role == self.agent_type:
                    d_list.append(f"assistant\n{content}\n\n")
                else:
                    d_list.append(f"user\n{content}\n\n")
            history_str = "\n".join(d_list)
            context += f"\n{history_str}"
            
            # æ ¹æ®è‹æ ¼æ‹‰åº•æ•™å­¦å¼€å…³è°ƒæ•´æç¤º
            if self.socratic_teaching_enabled:
                context += TEACHER_RESPONSE_TEACHING_SOCRATIC
            else:
                context += TEACHER_RESPONSE_TEACHING

            return context
        else:
            raise ValueError(f"Unknown prompt_type: {prompt_type}. Must be 'api' or 'local'.")

    def _format_student_input(self, problem, history, prompt_type='api'):
        """æ ¼å¼åŒ–æ•™å¸ˆè¾“å…¥ï¼Œæ”¯æŒæœ¬åœ°å’ŒAPIè°ƒç”¨

        Args:
            history: å¯¹è¯å†å²åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [(role, content), ...]
            prompt_type: 'api' æˆ– 'local'

        Returns:
            - å¯¹äº 'api': è¿”å›æ¶ˆæ¯åˆ—è¡¨ [{"role": "system/user/assistant", "content": "..."}, ...]
            - å¯¹äº 'local': è¿”å›å­—ç¬¦ä¸²æ ¼å¼çš„æç¤ºè¯
        """
        if prompt_type == 'api':
            # Build messages list in OpenAI format: [{"role": "system/user/assistant", "content": "..."}, ...]
            messages = []
            system_prompt = None
            # 1. System prompt (if exists)
            if self.prompt_sys:
                # å¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„ç³»ç»Ÿæç¤ºè¯
                if isinstance(self.prompt_sys, str):
                    # å¦‚æœæ˜¯é¢„å®šä¹‰çš„æç¤ºè¯å˜é‡åï¼Œå°è¯•è·å–å…¶å€¼
                    if self.prompt_sys in globals():
                        system_prompt = globals()[self.prompt_sys]
                    else:
                        system_prompt = self.prompt_sys
                else:
                    system_prompt = str(self.prompt_sys + "\n\n")

            # 2. Dialogue history (previous user and assistant messages)
            if history:
                # è½¬æ¢å¯¹è¯å†å²ä¸ºæ¶ˆæ¯æ ¼å¼ï¼ˆä¿ç•™æœ€è¿‘6è½®ï¼‰
                for role, content in history[-6:]:
                    # è½¬æ¢è§’è‰²ï¼šteacher -> assistant, student -> user (for teacher agent)
                    if role == self.agent_type:
                        messages.append({"role": "assistant", "content": content})
                    else:
                        messages.append({"role": "user", "content": content})

                if messages[-1]['role'] == 'user':
                    messages[-1]['content'] += "\n\n" + self.student_response.format(question=problem)
                if self.parallel_thinking_enabled:
                    messages[-1]['content'] += STUDENT_STANDARD_PARALLEL_THINKING_PROMPT.format(question=problem)

                if messages[0]['role'] == 'user':
                    if system_prompt:
                        messages[0]['content'] = system_prompt + messages[0]['content']
                elif messages[0]['role'] == 'assistant':
                    if system_prompt:
                        messages.insert(0, {"role": "user", "content": system_prompt})

            return messages

        elif prompt_type == 'local':
            # æœ¬åœ°æ¨¡å‹æ ¼å¼ï¼šè¿”å›å­—ç¬¦ä¸²
            context = ''
            d_list = []
            for role, content in history[-6:]:  # æœ€è¿‘3è½®å¯¹è¯
                if role == self.agent_type:
                    d_list.append(f"assistant\n{content}\n\n")
                else:
                    d_list.append(f"user\n{content}\n\n")
            history_str = "\n".join(d_list)
            context += f"\n{history_str}"

            # æ ¹æ®è‹æ ¼æ‹‰åº•æ•™å­¦å¼€å…³è°ƒæ•´æç¤º
            if self.socratic_teaching_enabled:
                context += TEACHER_RESPONSE_TEACHING_SOCRATIC
            else:
                context += TEACHER_RESPONSE_TEACHING

            return context
        else:
            raise ValueError(f"Unknown prompt_type: {prompt_type}. Must be 'api' or 'local'.")

    def get_memory_stats(self):
        """è·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if self.memory:
            return self.memory.get_stats()
        return {"enabled": False}

    def clear_memory(self):
        """æ¸…ç©ºå†…å­˜"""
        if self.memory:
            self.memory.clear()

    def _should_end_dialogue(self, teacher_response):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»“æŸå¯¹è¯"""
        end_phrases = ["finished", "completed"]
        return any(phrase in teacher_response.lower() for phrase in end_phrases)

    def _has_correct_answer(self, student_answer, correct_answer):
        """åˆ¤æ–­å­¦ç”Ÿæ˜¯å¦å¾—å‡ºæ­£ç¡®ç­”æ¡ˆ"""
        # ä½¿ç”¨ç°æœ‰çš„ç­”æ¡ˆæå–å’Œæ¯”è¾ƒé€»è¾‘
        return is_equiv_MATH(correct_answer, student_answer)

    def _get_final_answer(self):
        """è·å–æœ€ç»ˆç­”æ¡ˆ"""
        student_final_answer = ""
        if self.dialogue_history:
            for role, content in self.dialogue_history:
                if role == "student":
                    student_final_answer = content[1]
            return student_final_answer
        return ""

    def agent_response_invoke(self, user_input, **kwargs):
        self.response = self.agent.invoke({
            "messages": [{"role": "user", "content": user_input}]
        }, config=self.agent_config, context=self.context, **kwargs)

    def agent_response_stream(self, user_input, **kwargs):
        for chunk in self.agent.stream({
            "messages": [{"role": "user", "content": user_input}]
        }, config=self.agent_config, context=self.context, stream_mode="values", **kwargs):
            latest_message = chunk["messages"][-1]
            if latest_message.content:
                print(f"Agent: {latest_message.content}")
            elif latest_message.tool_calls:
                print(f"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}")

    def agent_output(self, all_messages=False):
        if all_messages:
            for i in self.response['messages']:
                print(f"{type(i)}: {i.content}")
        else:
            print(f"{type(self.response['messages'][-1])}: {self.response['structured_response'].main_response}")

    def get_dialogue_summary(self):
        """è·å–å¯¹è¯æ‘˜è¦"""
        summary = f"Dialogue Summary ({self.current_turn} turns):\n"
        for i, (role, content) in enumerate(self.dialogue_history):
            summary += f"Turn {i + 1} ({role}): {content[:100]}...\n"
        return summary


# LocalModelWrapper is now replaced by RawOllamaClient in api_client.py


class ExpertStudentAgent(SimpleAgent):
    """å­¦éœ¸Agent"""

    def __init__(self, debug_mode=False):
        super().__init__(agent_type="expert_student", debug_mode=debug_mode)

    def generate_solution_tree(self, problem):
        """ç”Ÿæˆè§£é¢˜æ ‘"""
        prompt = TREE_GENERATE_PROMPT.format(problem)

        response = self._invoke_agent(prompt)
        return self._parse_solution_tree(response, problem)

    def _parse_solution_tree(self, response, problem):
        """è§£æè§£é¢˜æ ‘å“åº”"""
        solution_tree = SolutionTree(problem)

        try:
            # ç®€å•çš„è§£æé€»è¾‘ - åœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„è§£æ
            if "<SolutionTree>" in response:
                # æå–è§£å†³æ–¹æ¡ˆè·¯å¾„
                paths_section = response.split("<SolutionPaths>")[1].split("</SolutionPaths>")[0]
                path_blocks = paths_section.split("</Path>")

                for block in path_blocks:
                    if "<Path" in block:
                        # æå–è·¯å¾„ä¿¡æ¯
                        method = self._extract_site_tag(block, "method")
                        complexity = self._extract_site_tag(block, "complexity")
                        innovation = self._extract_site_tag(block, "innovation")

                        # æå–æ­¥éª¤
                        steps = []
                        intermediate_answers = []
                        step_parts = block.split("<Step")
                        for step_part in step_parts[1:]:
                            if ">" in step_part and "</Step>" in step_part:
                                step_content = step_part.split(">", 1)[1].split("</Step>")[0]
                                steps.append(step_content)
                            if "<IntermediateAnswer>" in step_part and "</IntermediateAnswer>" in step_part:
                                intermediate_content = \
                                step_part.split("<IntermediateAnswer>", 1)[1].split("</IntermediateAnswer>")[0]
                                intermediate_answers.append(intermediate_content)

                        # æå–æœ€ç»ˆç­”æ¡ˆ
                        final_answer = self._extract_xml_tag(block, "FinalAnswer")

                        solution_tree.add_expert_path({
                            "method": method,
                            "complexity": complexity,
                            "innovation": innovation,
                            "steps": steps,
                            "intermediate_answers": intermediate_answers,
                            "final_answer": final_answer
                        })

        except Exception as e:
            print(f"âŒ Error parsing solution tree: {e}")
            # å¦‚æœè§£æå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„è§£å†³æ–¹æ¡ˆè·¯å¾„
            solution_tree.add_expert_path({
                "method": "algebraic",
                "complexity": "medium",
                "innovation": "medium",
                "steps": ["Apply standard algebraic approach", "Solve step by step"],
                "intermediate_answers": [],
                "final_answer": "[[Answer will be determined]]"
            })

        return solution_tree

    def _extract_xml_tag(self, text, tag_name):
        """æå–XMLæ ‡ç­¾å†…å®¹"""
        start_tag = f"<{tag_name}>"
        end_tag = f"</{tag_name}>"

        if start_tag in text and end_tag in text:
            return text.split(start_tag)[1].split(end_tag)[0].strip()
        return ""

    def _extract_site_tag(self, text, tag_name):
        """æå–XMLæ ‡ç­¾å†…å®¹"""
        start_tag = f'{tag_name}="'
        end_tag = f'"'

        if start_tag in text and end_tag in text:
            return text.split(start_tag)[1].split(end_tag)[0].strip()
        return ""
    


